{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Baragouine/SentimentAnalysisReviewAmazone/blob/main/ML2_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Projet ML2: Création de modèle de prédiction de note et de l'utilité de commentaire sur des produits de boutique en ligne\n",
        "Télécharger le dataset et le modèle de google pour pouvoir exécuter le notebook:\n",
        " * Dataset: http://jmcauley.ucsd.edu/data/amazon/index_2014.html  \n",
        " * Modèle de google (word2vec: GoogleNews-vectors-negative300.bin): https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g\n",
        "\n",
        "Le dataset est très volumineux, nous vous conseillons donc de créer un sample à l'aide du notebook suivant: https://colab.research.google.com/drive/121vpmUad_6NTvcKytY64znGbckNeB443?usp=sharing\n"
      ],
      "metadata": {
        "id": "M-dgiLJPYgW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Pré-traitement des données"
      ],
      "metadata": {
        "id": "L6-zllLkd05u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A. Import"
      ],
      "metadata": {
        "id": "u1QScTRu0xR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "import gensim\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from gensim.parsing.porter import PorterStemmer\n",
        "import gensim.models.keyedvectors as word2vec\n",
        "\n",
        "# Ignorer les warnings\n",
        "import warnings\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "warnings.warn = warn\n",
        "\n",
        "from textblob import TextBlob\n",
        "from random import sample"
      ],
      "metadata": {
        "id": "pJocUlPo0v-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B. Chargement des données"
      ],
      "metadata": {
        "id": "iTC9w78i1DgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chargement des données\n",
        "df = pd.read_csv('./dataset/sample.csv',sep=';')\n",
        "\n",
        "# Suppression des na\n",
        "df = df.dropna()\n",
        "\n",
        "# Apercu du dataset\n",
        "df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "ct_zjTXf1GyN",
        "outputId": "6fc74c55-738e-4f11-ccc1-ba6392f58147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      title        asin   helpful  overall  \\\n",
              "0  Adult Ballet Tutu Yellow  0000032034  [11, 12]      5.0   \n",
              "1       Along Came a Spider  0006476155    [0, 0]      5.0   \n",
              "2       Along Came a Spider  0006476155    [0, 0]      4.0   \n",
              "\n",
              "                                          reviewText   reviewTime  \\\n",
              "0  Purchased and wore these tutus for the Portlan...  09 10, 2013   \n",
              "1  Great development of characters, you begin to ...  01 14, 2013   \n",
              "2  Always a true fan of James Patterson. He knows...   01 5, 2014   \n",
              "\n",
              "       reviewerID     reviewerName  \\\n",
              "0  A2MPIVS0KSY8O0            Sonja   \n",
              "1  A37QDGQ90TLQQX  Albert Wimberly   \n",
              "2  A3S8MFLN2HZ7RI       footsie517   \n",
              "\n",
              "                                             summary  unixReviewTime  \n",
              "0                                              Tutus      1378771200  \n",
              "1  Great character development / compelling case ...      1358121600  \n",
              "2                                         Love it!!!      1388880000  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>asin</th>\n",
              "      <th>helpful</th>\n",
              "      <th>overall</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>reviewTime</th>\n",
              "      <th>reviewerID</th>\n",
              "      <th>reviewerName</th>\n",
              "      <th>summary</th>\n",
              "      <th>unixReviewTime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Adult Ballet Tutu Yellow</td>\n",
              "      <td>0000032034</td>\n",
              "      <td>[11, 12]</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Purchased and wore these tutus for the Portlan...</td>\n",
              "      <td>09 10, 2013</td>\n",
              "      <td>A2MPIVS0KSY8O0</td>\n",
              "      <td>Sonja</td>\n",
              "      <td>Tutus</td>\n",
              "      <td>1378771200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Along Came a Spider</td>\n",
              "      <td>0006476155</td>\n",
              "      <td>[0, 0]</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Great development of characters, you begin to ...</td>\n",
              "      <td>01 14, 2013</td>\n",
              "      <td>A37QDGQ90TLQQX</td>\n",
              "      <td>Albert Wimberly</td>\n",
              "      <td>Great character development / compelling case ...</td>\n",
              "      <td>1358121600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Along Came a Spider</td>\n",
              "      <td>0006476155</td>\n",
              "      <td>[0, 0]</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Always a true fan of James Patterson. He knows...</td>\n",
              "      <td>01 5, 2014</td>\n",
              "      <td>A3S8MFLN2HZ7RI</td>\n",
              "      <td>footsie517</td>\n",
              "      <td>Love it!!!</td>\n",
              "      <td>1388880000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shape\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-24YP8JW1Jo0",
        "outputId": "54b30ed5-bce7-4085-9ff9-66bf55f71bfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(543646, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les titres et les commentaires sont en anglais."
      ],
      "metadata": {
        "id": "Kr7pzQrg80xe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C. Transformation de l'utilité en nombre\n",
        "L'utilité (colonne helpful) est une liste de deux éléments qui représente une fraction, le premier élément est le dividende et le deuxième le diviseur."
      ],
      "metadata": {
        "id": "LNd91V6O_p4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction qui convertie l'utilité en nombre\n",
        "def parse_helpful(s):\n",
        "  tmp = s.split(\",\")\n",
        "  tmp = [int(i) for i in tmp]\n",
        "  return 0 if tmp[1] == 0 else round(tmp[0] / tmp[1], 1)"
      ],
      "metadata": {
        "id": "NC9iYfXYAOL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['helpful'] = df['helpful'].str.replace(' ', '', regex=False).str.replace('[', '', regex=False).str.replace(']', '', regex=False)\n",
        "df['helpful'] = df['helpful'].apply(lambda line : parse_helpful(line))\n",
        "df['helpful'].head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFm_pzh2_v72",
        "outputId": "d7a79057-dc7c-4b69-dab2-032354d2c56a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.9\n",
              "1    0.0\n",
              "2    0.0\n",
              "Name: helpful, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### D. Statistiques"
      ],
      "metadata": {
        "id": "AyqaDBvb9CkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distributions des notes\n",
        "plt.hist(df['overall'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "J-Do3e369HYc",
        "outputId": "3df3d656-32c7-4ecc-db1f-99db62fde3bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Nombre de note égal à 2\n",
        "(df['overall'] == 2.0).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRZ5f7Fk9mOJ",
        "outputId": "4f7ab54d-bf32-4962-ce0c-068242ab3394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28525"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Différentes valeurs possibles pour l'utilité\n",
        "df['helpful'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZaj-4b9Fo-W",
        "outputId": "33ab3104-1538-40aa-b4a7-bbea2a535bf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.9, 0. , 1. , 0.5, 0.8, 0.1, 0.7, 0.6, 0.4, 0.2, 0.3, 1.5])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ramener à 1 les valeurs qui dépassent 1\n",
        "df['helpful'] = df['helpful'].apply(lambda n : int((n if n <= 1 else 1) * 10))\n",
        "df['helpful'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyV-FPULFiDq",
        "outputId": "c2a431b3-a302-4e3e-c494-33bae4ddc988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 9,  0, 10,  5,  8,  1,  7,  6,  4,  2,  3])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution de l'utilité\n",
        "plt.hist(df['helpful'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "WtvwYYPGH0sw",
        "outputId": "b76d335c-b77b-488d-d984-321ee7886f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fonction de preprocessing d'un text (passage d'un texte à une liste de token)"
      ],
      "metadata": {
        "id": "wtumZOMVS1L2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "porter_stemmer = PorterStemmer()\n",
        "def preprocess_text(sentence):\n",
        "    sentence = remove_stopwords(sentence)\n",
        "    tokenized_sentence = gensim.utils.simple_preprocess(sentence, deacc=True) # Tokenize the sentence and delete accent\n",
        "    tokenized_sentence = [porter_stemmer.stem(word) for word in tokenized_sentence] #stemmatization (garde seulement la racine des mots)\n",
        "    # tokenized_sentence = tokenized_sentence[:300]\n",
        "    return tokenized_sentence"
      ],
      "metadata": {
        "id": "H66sRau6S7lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Modèle qui ne prédit que la note"
      ],
      "metadata": {
        "id": "G3fxu2DcY1Bf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A. Préparation du corpus et des labels"
      ],
      "metadata": {
        "id": "KDS-23-zIS67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I. Sélection de 28525 lignes pour chaque note pour avoir un dataset équilibré"
      ],
      "metadata": {
        "id": "HPZD-14zR1Ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Renvoie n lignes du dataset avec une note égal à note\n",
        "def data_sample_by_note(data, n, note):\n",
        "  tmp_data = data[data[\"overall\"] == note]\n",
        "  idx = list(tmp_data.index)\n",
        "  idx = sample(range(len(idx)), min(n, len(idx)))\n",
        "  return tmp_data.iloc[idx]"
      ],
      "metadata": {
        "id": "C54paNL8LZLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nombre de lignes à prendre par catégorie\n",
        "nb_line_per_category = df['overall'].value_counts().min()\n",
        "\n",
        "# TODO: Ligne a supprimer dans la version finale, si nécessaire\n",
        "nb_line_per_category = 10000\n",
        "\n",
        "# Affichage\n",
        "nb_line_per_category"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b_9zjtYNJUe",
        "outputId": "3ccf0df1-79c1-4fe3-aa36-988ace69a27a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset utilisé pour la prédiction de la note\n",
        "note_df = pd.concat([data_sample_by_note(df, nb_line_per_category, i) for i in df['overall'].unique()])\n",
        "# Vérification de la taille\n",
        "note_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0VvbunyIUwv",
        "outputId": "fb4486ff-94ef-430f-89d4-5fe6d86d6e83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### II. Split du dataset"
      ],
      "metadata": {
        "id": "1vJLKLF7R35l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Affectation et transformation\n",
        "Y = note_df['overall']\n",
        "X = note_df[['title', 'reviewText']]"
      ],
      "metadata": {
        "id": "71M3E-X4SW9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution des note\n",
        "plt.hist(Y)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "kAJIM30ETvd5",
        "outputId": "9aa241d5-fe1c-4f0f-a516-251332f17745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,shuffle=True,random_state=1)"
      ],
      "metadata": {
        "id": "YL28C70WS_pZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_train_tokens = X_train['reviewText'].apply(lambda line : preprocess_text(line))\n",
        "reviews_test_tokens = X_test['reviewText'].apply(lambda line : preprocess_text(line))"
      ],
      "metadata": {
        "id": "UnNJxqbZV4wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title_train_tokens = X_train['title'].apply(lambda line : preprocess_text(line))\n",
        "title_test_tokens = X_test['title'].apply(lambda line : preprocess_text(line))"
      ],
      "metadata": {
        "id": "xAWtNDDDXdX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B. Utilisation que du commentaire"
      ],
      "metadata": {
        "id": "z_DRL8qQZtsk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I. Moyenne des embeddings"
      ],
      "metadata": {
        "id": "3PCTnRhgaWyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul de l'embedding d'une phrase\n",
        "def word2vec_generator(texts,model,vector_size):\n",
        "    dict_word2vec = {}\n",
        "    for index, word_list in enumerate(texts):\n",
        "        arr = np.array([0.0 for i in range(0, vector_size)])\n",
        "        nb_word=0\n",
        "        for word in word_list:\n",
        "            try:\n",
        "                arr += model[word]\n",
        "                nb_word=nb_word+1\n",
        "            except KeyError:\n",
        "                continue\n",
        "        if(len(word_list) == 0):\n",
        "            dict_word2vec[index] = arr\n",
        "        else:\n",
        "            try:\n",
        "                dict_word2vec[index] = arr / (nb_word if nb_word != 0 else 1)\n",
        "            except RuntimeWarning:\n",
        "                print(index)\n",
        "                print(arr)\n",
        "                print(nb_word)\n",
        "    df_word2vec = pd.DataFrame(dict_word2vec).T\n",
        "    return df_word2vec"
      ],
      "metadata": {
        "id": "I7kn0IUJYWIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_google_vec_neg_300 = word2vec.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
        "print(model_google_vec_neg_300.vector_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5Fn_PYeYxpF",
        "outputId": "50050817-0370-464b-b606-3b386c6698c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size=model_google_vec_neg_300.vector_size\n",
        "corpus_train_wv_google=word2vec_generator(reviews_train_tokens,model_google_vec_neg_300,vector_size)\n",
        "corpus_test_wv_google=word2vec_generator(reviews_test_tokens,model_google_vec_neg_300,vector_size)"
      ],
      "metadata": {
        "id": "PgkMVypwY77q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import mean\n",
        "\n",
        "# Notre accuracy personaliser accepte les distances de 1 entre les y\n",
        "def customized_accuracy(y_true, y_pred, d = 1):\n",
        "  return mean([(1 if abs(y_true[i] - y_pred[i]) <= d else 0) for i in range(len(y_true))])"
      ],
      "metadata": {
        "id": "8dNuarQAbes9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error,accuracy_score,confusion_matrix\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "algos = {\n",
        "  'RF' : RandomForestRegressor(n_estimators=50,random_state=1,n_jobs=-1),\n",
        "  'KNN' : KNeighborsRegressor(n_neighbors=5,n_jobs=-1,metric='cosine'),\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(20,10),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "def run_models (X_train,Y_train,X_test,Y_test,algos):\n",
        "    for algo_name in algos:\n",
        "        model=algos[algo_name]\n",
        "        model.fit(X_train,Y_train)\n",
        "        prediction=model.predict(X_test)\n",
        "        prediction[prediction<1]=1\n",
        "        prediction[prediction>5]=5\n",
        "        MAE=mean_absolute_error(Y_test,prediction)\n",
        "        ACC=accuracy_score(Y_test,np.round(prediction))\n",
        "        CACC=customized_accuracy(Y_test,np.round(prediction))\n",
        "        \n",
        "        print('################## {0} #############'.format(algo_name))\n",
        "        print('MAE = {0:.3f}, Accuracy = {1:.3f}, Customized Accuracy = {2:.3f}'.format(MAE,ACC, CACC))\n",
        "        display(confusion_matrix(Y_test,np.round(prediction)))\n",
        "        print()"
      ],
      "metadata": {
        "id": "9FBFdFr1ZP9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_models(corpus_train_wv_google, y_train, corpus_test_wv_google, y_test.to_numpy(), algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "zuQ--q0yYySj",
        "outputId": "5e6f99b9-f4b7-41e9-d0af-90ef61bb6896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## RF #############\n",
            "MAE = 1.070, Accuracy = 0.246, Customized Accuracy = 0.717\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   6,  728, 1176,   69,    0],\n",
              "       [   2,  480, 1372,  129,    0],\n",
              "       [   1,  269, 1451,  230,    0],\n",
              "       [   0,  141, 1354,  496,   10],\n",
              "       [   0,  104, 1207,  752,   23]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## KNN #############\n",
            "MAE = 1.127, Accuracy = 0.249, Customized Accuracy = 0.695\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[ 123,  756,  891,  200,    9],\n",
              "       [  54,  639,  984,  300,    6],\n",
              "       [  26,  472, 1086,  353,   14],\n",
              "       [   8,  335, 1085,  533,   40],\n",
              "       [   2,  286,  977,  713,  108]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## MLP #############\n",
            "MAE = 0.976, Accuracy = 0.325, Customized Accuracy = 0.790\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[ 360, 1019,  305,  272,   23],\n",
              "       [ 145,  947,  426,  432,   33],\n",
              "       [  57,  614,  494,  743,   43],\n",
              "       [  27,  351,  424, 1045,  154],\n",
              "       [  34,  259,  260, 1133,  400]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Courbe d'apprentissage du meilleur modèle"
      ],
      "metadata": {
        "id": "Jl6PURTtyLaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "# utiliser la fonction learning_curve\n",
        "train_sizes, train_scores, test_scores = learning_curve(\\\n",
        "  algos[\"MLP\"], pd.concat([corpus_train_wv_google, corpus_test_wv_google]),\\\n",
        "  pd.concat([y_train, y_test]))\n",
        "\n",
        "# tracer les courbes d'apprentissage\n",
        "plt.figure()\n",
        "plt.title(\"Courbe d'apprentissage\")\n",
        "plt.xlabel(\"Taille de l'échantillon d'entraînement\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.plot(train_sizes, train_scores.mean(axis=1), 'o-', color=\"r\", label=\"Score d'entraînement\")\n",
        "plt.plot(train_sizes, test_scores.mean(axis=1), 'o-', color=\"g\", label=\"Score de validation croisée\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "lZvuh44BybV7",
        "outputId": "aa078ff4-7bfb-4567-970a-658817b14a6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous voyons qu'il n'y a pas besoin de plus de données pour l'entraînement."
      ],
      "metadata": {
        "id": "TCJxlDqo4dad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Amélioration du meilleur modèle"
      ],
      "metadata": {
        "id": "eq6VjRx5ab4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plus de neurones\n",
        "algos = {\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(40, 20),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models(corpus_train_wv_google, y_train, corpus_test_wv_google, y_test.to_numpy(), algos)"
      ],
      "metadata": {
        "id": "DFBf4Yzvage5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "234cf68b-535a-4eb4-9100-b480fd9e4881"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## MLP #############\n",
            "MAE = 1.007, Accuracy = 0.329, Customized Accuracy = 0.768\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[442, 801, 397, 253,  86],\n",
              "       [217, 715, 590, 367,  94],\n",
              "       [ 91, 477, 651, 581, 151],\n",
              "       [ 46, 251, 523, 815, 366],\n",
              "       [ 55, 161, 366, 842, 662]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On perd en MAE, on gagne un peu en Accuracy mais on perd en Customized Accuracy: Ça n'améliore pas les performances."
      ],
      "metadata": {
        "id": "EE4roHrudJsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Moins de neurones\n",
        "algos = {\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(10, 5),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models(corpus_train_wv_google, y_train, corpus_test_wv_google, y_test.to_numpy(), algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "CssJ4EMSamSh",
        "outputId": "f3ec8afb-0cbf-413d-ec5e-370c23c59d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## MLP #############\n",
            "MAE = 0.941, Accuracy = 0.323, Customized Accuracy = 0.805\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[ 400, 1015,  399,  153,   12],\n",
              "       [ 158,  930,  640,  242,   13],\n",
              "       [  55,  590,  796,  492,   18],\n",
              "       [  28,  297,  747,  833,   96],\n",
              "       [  24,  199,  512, 1078,  273]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On gagne au niveau du MAE, on perd en Accuracy et on gagne en Customized Accuracy: Ça améliore un peu les performances mais c'est négligeable."
      ],
      "metadata": {
        "id": "GgoHJgGGWFry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### II. Prise en compte de la longueur, de la spécificité et de la subjectivité du texte"
      ],
      "metadata": {
        "id": "9l_1U8uH3O6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Renvoie la polarité d'un texte\n",
        "def polarity_txt(text):\n",
        "    return TextBlob(text).sentiment[0] \n",
        "\n",
        "# Renvoie la subjectivité d'un texte\n",
        "def subj_txt(text):\n",
        "    return  TextBlob(text).sentiment[1]"
      ],
      "metadata": {
        "id": "f7LrFApnr1dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul de la longueur, la spécificité et la subjectivité\n",
        "df_meta_train=pd.DataFrame([])\n",
        "df_meta_train['length_text'] = [len(reviews_train_tokens.iloc[i]) for i in range(len(reviews_train_tokens))]\n",
        "df_meta_train['polarity'] = [polarity_txt(X_train[\"reviewText\"].iloc[i]) for i in range(len(X_train))]\n",
        "df_meta_train['subjectivity'] = [subj_txt(X_train[\"reviewText\"].iloc[i]) for i in range(len(X_train))]\n",
        "\n",
        "df_meta_test=pd.DataFrame([])\n",
        "df_meta_test['length_text'] = [len(reviews_test_tokens.iloc[i]) for i in range(len(reviews_test_tokens))]\n",
        "df_meta_test['polarity'] = [polarity_txt(X_test[\"reviewText\"].iloc[i]) for i in range(len(X_test))]\n",
        "df_meta_test['subjectivity'] = [subj_txt(X_test[\"reviewText\"].iloc[i]) for i in range(len(X_test))]"
      ],
      "metadata": {
        "id": "27uMa9F8sDLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape des données du train transformer\n",
        "corpus_train_wv_google.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SvbwIxA2tCH",
        "outputId": "2a6c9f09-e1f7-4fc0-9bbb-dbbaaf139337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40000, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Concaténation\n",
        "corpus_train_wv_google_meta=pd.concat([corpus_train_wv_google,df_meta_train],axis=1)\n",
        "corpus_test_wv_google_meta=pd.concat([corpus_test_wv_google,df_meta_test],axis=1)"
      ],
      "metadata": {
        "id": "QKWgZiw82Hvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape des données du train après concaténation\n",
        "corpus_train_wv_google_meta.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ruqk93ZF2nPF",
        "outputId": "f4946634-5a60-42dc-8c47-778a44f2c3ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40000, 303)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_train_wv_google_meta.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "RKoIQDPi8iVa",
        "outputId": "118eb18c-a41c-4599-a9a5-fe03a4fbc84c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3         4         5         6  \\\n",
              "0  0.029349  0.039355 -0.043146  0.137334 -0.050119  0.011490  0.037109   \n",
              "1  0.005580  0.085300 -0.023612  0.059867 -0.084558  0.055219  0.045292   \n",
              "2  0.016178  0.032145 -0.021306  0.052707  0.063334  0.089003  0.116973   \n",
              "\n",
              "          7         8         9        10        11        12        13  \\\n",
              "0 -0.058620  0.036689  0.074262 -0.052490 -0.120954 -0.065456 -0.007420   \n",
              "1 -0.013961  0.073800  0.073568 -0.064131 -0.087977 -0.001510  0.007506   \n",
              "2 -0.108175 -0.008477  0.036092  0.030233 -0.057129 -0.012912 -0.036621   \n",
              "\n",
              "         14        15        16        17        18        19        20  \\\n",
              "0 -0.107020  0.025611  0.082581  0.117641 -0.080366 -0.057848  0.007472   \n",
              "1 -0.118765  0.070056  0.081668  0.098182  0.016869 -0.128859 -0.024112   \n",
              "2 -0.168918  0.137004  0.072500  0.156752  0.005157 -0.032732 -0.072367   \n",
              "\n",
              "         21        22        23        24        25        26        27  \\\n",
              "0  0.040187  0.015901  0.026445  0.038720  0.048357 -0.055760  0.038609   \n",
              "1  0.031227  0.036170  0.034805  0.022005  0.028271 -0.061917  0.089236   \n",
              "2  0.013156  0.070360  0.033040  0.072171 -0.026530 -0.108005 -0.045164   \n",
              "\n",
              "         28        29        30        31        32        33        34  \\\n",
              "0  0.034306 -0.152562 -0.105931  0.095141 -0.011897 -0.013230 -0.065061   \n",
              "1  0.052048 -0.070236 -0.072764  0.036908 -0.039874 -0.017802 -0.065566   \n",
              "2 -0.040053 -0.062771 -0.045529  0.029690  0.041395 -0.037693  0.070801   \n",
              "\n",
              "         35        36        37        38        39        40        41  \\\n",
              "0 -0.130545  0.016349  0.089068  0.017705 -0.019113  0.050886 -0.137695   \n",
              "1 -0.058662  0.045484  0.060569  0.025748  0.038731  0.050980 -0.052367   \n",
              "2 -0.049967  0.081733  0.040507  0.071876  0.079305  0.080473 -0.023105   \n",
              "\n",
              "         42        43        44        45        46        47        48  \\\n",
              "0  0.223364  0.012050  0.026917  0.019598 -0.057053  0.055734 -0.056854   \n",
              "1  0.114041 -0.010232 -0.028403 -0.056405 -0.051394 -0.009512  0.070647   \n",
              "2  0.105306  0.063653 -0.114855 -0.010607 -0.070160 -0.068264 -0.009111   \n",
              "\n",
              "         49        50        51        52        53        54        55  \\\n",
              "0  0.022518 -0.058526  0.042130  0.006001  0.010803  0.009626 -0.032889   \n",
              "1 -0.041611 -0.045579 -0.011794 -0.022420 -0.055827 -0.003942  0.041191   \n",
              "2  0.037232 -0.004612  0.061157  0.038808 -0.024482  0.019803 -0.011163   \n",
              "\n",
              "         56        57        58        59        60        61        62  \\\n",
              "0 -0.019396 -0.083147  0.046321 -0.046790 -0.085031  0.119106 -0.171291   \n",
              "1 -0.021208 -0.079629 -0.036742 -0.022661 -0.138956  0.013124 -0.094502   \n",
              "2 -0.060235 -0.065362  0.084656 -0.125081  0.001655  0.090416 -0.073310   \n",
              "\n",
              "         63        64       65        66        67        68        69  \\\n",
              "0 -0.068813  0.117058 -0.02260 -0.038156  0.048811 -0.078201  0.080414   \n",
              "1 -0.027921 -0.038336 -0.01165 -0.038306  0.027518 -0.062284  0.097517   \n",
              "2  0.002609  0.008274 -0.04796 -0.049181  0.043165 -0.062676  0.011047   \n",
              "\n",
              "         70        71        72        73        74        75        76  \\\n",
              "0  0.110713  0.029674  0.084643  0.007516 -0.212769  0.045794  0.056074   \n",
              "1  0.071055  0.019842  0.093523 -0.024006 -0.124910 -0.081706  0.115077   \n",
              "2  0.082601  0.003052  0.113376  0.021973 -0.134576  0.004964  0.085649   \n",
              "\n",
              "         77        78        79        80        81        82        83  \\\n",
              "0  0.072498  0.071429  0.045323  0.006836  0.010429  0.001552  0.008327   \n",
              "1  0.074918  0.014500  0.057425  0.036139  0.020951  0.037078 -0.024989   \n",
              "2  0.083908 -0.016900  0.025180  0.014476 -0.015774  0.081594 -0.017470   \n",
              "\n",
              "         84        85        86        87        88        89        90  \\\n",
              "0 -0.044635  0.028966  0.043653  0.115702  0.058755 -0.003577 -0.022288   \n",
              "1 -0.018503  0.050570 -0.029624  0.062621  0.000421  0.022855  0.025969   \n",
              "2 -0.116699  0.006321 -0.035468  0.115857  0.026408  0.031477  0.058539   \n",
              "\n",
              "         91        92        93        94        95        96        97  \\\n",
              "0 -0.006223 -0.064606 -0.084874 -0.045925 -0.010803  0.144930  0.027706   \n",
              "1 -0.038253 -0.012213 -0.031447  0.004621 -0.032744 -0.007686  0.011745   \n",
              "2  0.053507 -0.065267 -0.044169 -0.013536 -0.029460  0.063104  0.020223   \n",
              "\n",
              "         98        99       100       101       102       103       104  \\\n",
              "0  0.036451 -0.136647 -0.066066  0.026742  0.036355 -0.021649 -0.071716   \n",
              "1  0.081771  0.020228 -0.044685 -0.069836  0.041860 -0.045683 -0.063999   \n",
              "2  0.061117 -0.051639 -0.121392 -0.038249  0.071242 -0.024224 -0.027161   \n",
              "\n",
              "        105       106       107       108       109       110       111  \\\n",
              "0 -0.020543  0.048941 -0.049635  0.007207 -0.107270 -0.095982 -0.026106   \n",
              "1 -0.038992 -0.002189 -0.011811  0.044066 -0.044411 -0.047256  0.002104   \n",
              "2 -0.023473 -0.006612 -0.025869  0.074678 -0.058160  0.002631 -0.057838   \n",
              "\n",
              "        112       113       114       115       116       117       118  \\\n",
              "0  0.036152 -0.081857  0.124246 -0.044870  0.009783 -0.002145 -0.028431   \n",
              "1 -0.007041 -0.011091  0.129405  0.018276 -0.043437 -0.050995  0.021507   \n",
              "2  0.068186  0.005851  0.054091 -0.056695 -0.021244 -0.109402 -0.001017   \n",
              "\n",
              "        119       120       121       122       123       124       125  \\\n",
              "0 -0.000589 -0.108172 -0.059658 -0.093497  0.034520  0.025427 -0.050507   \n",
              "1  0.030798 -0.032876 -0.018366 -0.084074  0.048709  0.018249 -0.020988   \n",
              "2  0.028402 -0.072873  0.033407 -0.041880  0.083550 -0.066332 -0.010379   \n",
              "\n",
              "        126       127       128       129       130       131       132  \\\n",
              "0 -0.073363 -0.030029  0.006771  0.006856 -0.106724 -0.012695  0.020935   \n",
              "1 -0.066153 -0.051706  0.044230  0.009216 -0.024518 -0.026715 -0.053068   \n",
              "2 -0.018158 -0.035507 -0.020643  0.038194 -0.026259 -0.119439 -0.069383   \n",
              "\n",
              "        133       134       135       136       137       138       139  \\\n",
              "0  0.178149 -0.012427 -0.010827 -0.006696  0.043917  0.002947  0.122637   \n",
              "1  0.047969 -0.022892 -0.018158  0.028669  0.019290  0.031894  0.023224   \n",
              "2  0.009928 -0.034403 -0.020060 -0.034234  0.097619  0.040541  0.036620   \n",
              "\n",
              "        140       141       142       143       144       145       146  \\\n",
              "0  0.098938 -0.079065  0.032924 -0.004040 -0.079830  0.036145 -0.063729   \n",
              "1  0.090406 -0.056694 -0.025045  0.019466  0.026692 -0.018428 -0.035688   \n",
              "2 -0.002007 -0.006273  0.042508 -0.036251 -0.042101  0.049873 -0.114882   \n",
              "\n",
              "        147       148       149       150       151       152       153  \\\n",
              "0 -0.036996 -0.019060 -0.004517  0.161743  0.030021 -0.092826  0.113676   \n",
              "1  0.010103 -0.035806 -0.020723  0.036973 -0.038289 -0.082985  0.088022   \n",
              "2 -0.051134 -0.062039 -0.025370  0.063938  0.036553 -0.043240  0.084825   \n",
              "\n",
              "        154       155       156       157       158       159       160  \\\n",
              "0 -0.090441 -0.054308 -0.043638 -0.074184 -0.077776 -0.064273 -0.104289   \n",
              "1 -0.075249 -0.032980 -0.062438 -0.050168 -0.032538  0.008664 -0.014264   \n",
              "2 -0.044798 -0.007982 -0.021142 -0.037876 -0.042996 -0.048151 -0.037842   \n",
              "\n",
              "        161       162       163       164       165       166       167  \\\n",
              "0  0.091649  0.018354  0.064697  0.025271 -0.161298 -0.001225 -0.029922   \n",
              "1  0.014338  0.011957 -0.034922 -0.013335 -0.047260  0.021864 -0.013537   \n",
              "2  0.102960 -0.030192  0.122423  0.056207 -0.155138  0.060086 -0.094957   \n",
              "\n",
              "        168       169       170       171       172       173       174  \\\n",
              "0 -0.003283 -0.043265 -0.137870  0.009999  0.023056  0.051784  0.010817   \n",
              "1 -0.040186  0.003110 -0.085474 -0.014884 -0.037386 -0.004133 -0.032421   \n",
              "2  0.008613  0.034614 -0.023926  0.044067 -0.051721 -0.049127 -0.106676   \n",
              "\n",
              "        175       176       177       178       179       180       181  \\\n",
              "0  0.058472  0.121730 -0.068704 -0.083221  0.079237 -0.071141 -0.091073   \n",
              "1  0.031046  0.108533 -0.189647 -0.065071 -0.008955 -0.127483 -0.074811   \n",
              "2  0.038439  0.126329 -0.045193  0.038323  0.029165 -0.081943 -0.040249   \n",
              "\n",
              "        182       183       184       185       186       187       188  \\\n",
              "0  0.047538  0.063494 -0.018337 -0.025423 -0.020890 -0.072911  0.053816   \n",
              "1  0.016072 -0.015883 -0.043636 -0.031738 -0.094789  0.020677  0.075420   \n",
              "2  0.023356 -0.035556 -0.011146 -0.024127  0.034336  0.014733 -0.037503   \n",
              "\n",
              "        189       190       191       192       193       194       195  \\\n",
              "0 -0.024026  0.019584  0.008074  0.011818 -0.053589  0.000462  0.176505   \n",
              "1  0.039272  0.011520 -0.014802 -0.013358 -0.026666  0.002438  0.070307   \n",
              "2  0.079902 -0.007161  0.015959  0.023739 -0.019365 -0.036099 -0.036492   \n",
              "\n",
              "        196       197       198       199       200       201       202  \\\n",
              "0 -0.008375 -0.136230 -0.138829 -0.090470  0.018563  0.064261 -0.087738   \n",
              "1 -0.001578 -0.068348 -0.031263 -0.049007  0.049735  0.068944 -0.060033   \n",
              "2  0.013604  0.036818 -0.060306 -0.010193 -0.064548  0.039951 -0.097358   \n",
              "\n",
              "        203       204       205       206       207       208       209  \\\n",
              "0 -0.071213  0.036503 -0.151509 -0.030448 -0.031996 -0.064571  0.040584   \n",
              "1 -0.062541  0.079628 -0.038818 -0.046268  0.001947 -0.017050 -0.052253   \n",
              "2 -0.046848 -0.040405 -0.014883 -0.093262  0.016825  0.006015 -0.006538   \n",
              "\n",
              "        210       211       212       213       214       215       216  \\\n",
              "0 -0.081959  0.092773 -0.139744  0.042459 -0.032533 -0.027989  0.098371   \n",
              "1 -0.029678  0.118643 -0.110048  0.008860 -0.050145  0.000312  0.003922   \n",
              "2  0.067824 -0.042813 -0.106947 -0.000834 -0.022359  0.021817  0.088433   \n",
              "\n",
              "        217       218       219       220       221       222       223  \\\n",
              "0  0.043300 -0.135919 -0.003034 -0.030036  0.065979  0.013876 -0.017308   \n",
              "1  0.030736 -0.087338  0.007633 -0.047783  0.033778 -0.028245  0.038771   \n",
              "2  0.088169 -0.063761 -0.025002  0.001268 -0.074894  0.034695  0.035434   \n",
              "\n",
              "        224       225       226       227       228       229       230  \\\n",
              "0  0.077567 -0.013354  0.001551  0.046534 -0.043215  0.047590 -0.033900   \n",
              "1  0.008593 -0.007277  0.022737  0.042860  0.026604 -0.034759  0.009034   \n",
              "2  0.083103 -0.079054  0.005493  0.086297  0.002045 -0.011959  0.053433   \n",
              "\n",
              "        231       232       233       234       235       236       237  \\\n",
              "0 -0.018250  0.006884 -0.033901  0.062081  0.028909 -0.053926 -0.154968   \n",
              "1 -0.037268  0.010572 -0.016896  0.042140  0.007601 -0.008338 -0.056065   \n",
              "2 -0.061761  0.021627  0.016259  0.094652 -0.042352 -0.056396 -0.068651   \n",
              "\n",
              "        238       239       240       241       242       243       244  \\\n",
              "0  0.101880  0.072985  0.126535 -0.017020 -0.046378 -0.105586 -0.020673   \n",
              "1  0.034869  0.014307 -0.009829 -0.039078  0.009282 -0.096402  0.043873   \n",
              "2  0.053593  0.038059 -0.001107 -0.004272 -0.027066 -0.054498  0.010640   \n",
              "\n",
              "        245       246       247       248       249       250       251  \\\n",
              "0  0.043405  0.009373  0.068233 -0.097857 -0.207807 -0.008571  0.104527   \n",
              "1 -0.016066 -0.041573  0.060135 -0.001188 -0.094655 -0.011820 -0.003895   \n",
              "2  0.057631  0.004868  0.038642  0.015625 -0.102471 -0.035177  0.098728   \n",
              "\n",
              "        252       253       254       255       256       257       258  \\\n",
              "0  0.096941  0.115147 -0.008942 -0.110573 -0.040229 -0.051553  0.031355   \n",
              "1  0.079302  0.086066  0.034077 -0.044135 -0.016489 -0.024242 -0.079945   \n",
              "2  0.009087  0.038574  0.080838 -0.014540 -0.026733  0.013217 -0.112739   \n",
              "\n",
              "        259       260       261       262       263       264       265  \\\n",
              "0 -0.105504  0.010230 -0.020512 -0.078979  0.013674 -0.064750  0.162955   \n",
              "1 -0.015627  0.008575 -0.000057 -0.038578  0.003882  0.092860  0.070185   \n",
              "2 -0.050128 -0.004123 -0.004612 -0.067491  0.011224 -0.000841  0.143175   \n",
              "\n",
              "        266       267       268       269       270       271       272  \\\n",
              "0 -0.077624 -0.150068 -0.085999 -0.046899 -0.023124  0.054796  0.170694   \n",
              "1  0.008929  0.003402 -0.067874 -0.014913 -0.010251  0.092048  0.048665   \n",
              "2  0.010417 -0.015523 -0.070543 -0.007595  0.055779  0.053182  0.076362   \n",
              "\n",
              "        273       274       275       276       277       278       279  \\\n",
              "0  0.074289  0.063477 -0.080540 -0.018136 -0.030698  0.003611  0.047468   \n",
              "1  0.004263  0.082684 -0.070974 -0.085914 -0.121464 -0.108557  0.056362   \n",
              "2  0.029573  0.070814  0.003160 -0.110260 -0.047377 -0.164252  0.011909   \n",
              "\n",
              "        280       281       282       283       284       285       286  \\\n",
              "0  0.082973 -0.017027  0.099034  0.100255  0.036614 -0.064976 -0.057118   \n",
              "1  0.003471 -0.004600  0.027936  0.081766 -0.001350  0.025344 -0.049986   \n",
              "2 -0.011544 -0.013593  0.032905  0.106418  0.099162  0.093560 -0.094699   \n",
              "\n",
              "        287       288       289       290       291       292       293  \\\n",
              "0 -0.088784 -0.036063  0.114485 -0.024549  0.183432 -0.109658  0.030273   \n",
              "1 -0.025076  0.005720  0.006427 -0.040072  0.103039 -0.082554  0.022182   \n",
              "2  0.010837  0.008592  0.138273 -0.102301  0.092102 -0.125210 -0.039361   \n",
              "\n",
              "        294       295       296       297       298       299  length_text  \\\n",
              "0 -0.043125 -0.049957 -0.017133 -0.090628 -0.043672  0.064272           19   \n",
              "1 -0.003769  0.015608 -0.013759 -0.031591 -0.034226 -0.004203           68   \n",
              "2 -0.045234 -0.004466 -0.005018 -0.056888 -0.016968 -0.004015           10   \n",
              "\n",
              "   polarity  subjectivity  \n",
              "0  0.242857      0.500000  \n",
              "1  0.097059      0.466667  \n",
              "2  0.600000      0.650000  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "      <th>128</th>\n",
              "      <th>129</th>\n",
              "      <th>130</th>\n",
              "      <th>131</th>\n",
              "      <th>132</th>\n",
              "      <th>133</th>\n",
              "      <th>134</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "      <th>141</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>144</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "      <th>151</th>\n",
              "      <th>152</th>\n",
              "      <th>153</th>\n",
              "      <th>154</th>\n",
              "      <th>155</th>\n",
              "      <th>156</th>\n",
              "      <th>157</th>\n",
              "      <th>158</th>\n",
              "      <th>159</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "      <th>200</th>\n",
              "      <th>201</th>\n",
              "      <th>202</th>\n",
              "      <th>203</th>\n",
              "      <th>204</th>\n",
              "      <th>205</th>\n",
              "      <th>206</th>\n",
              "      <th>207</th>\n",
              "      <th>208</th>\n",
              "      <th>209</th>\n",
              "      <th>210</th>\n",
              "      <th>211</th>\n",
              "      <th>212</th>\n",
              "      <th>213</th>\n",
              "      <th>214</th>\n",
              "      <th>215</th>\n",
              "      <th>216</th>\n",
              "      <th>217</th>\n",
              "      <th>218</th>\n",
              "      <th>219</th>\n",
              "      <th>220</th>\n",
              "      <th>221</th>\n",
              "      <th>222</th>\n",
              "      <th>223</th>\n",
              "      <th>224</th>\n",
              "      <th>225</th>\n",
              "      <th>226</th>\n",
              "      <th>227</th>\n",
              "      <th>228</th>\n",
              "      <th>229</th>\n",
              "      <th>230</th>\n",
              "      <th>231</th>\n",
              "      <th>232</th>\n",
              "      <th>233</th>\n",
              "      <th>234</th>\n",
              "      <th>235</th>\n",
              "      <th>236</th>\n",
              "      <th>237</th>\n",
              "      <th>238</th>\n",
              "      <th>239</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "      <th>244</th>\n",
              "      <th>245</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "      <th>259</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "      <th>length_text</th>\n",
              "      <th>polarity</th>\n",
              "      <th>subjectivity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.029349</td>\n",
              "      <td>0.039355</td>\n",
              "      <td>-0.043146</td>\n",
              "      <td>0.137334</td>\n",
              "      <td>-0.050119</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.037109</td>\n",
              "      <td>-0.058620</td>\n",
              "      <td>0.036689</td>\n",
              "      <td>0.074262</td>\n",
              "      <td>-0.052490</td>\n",
              "      <td>-0.120954</td>\n",
              "      <td>-0.065456</td>\n",
              "      <td>-0.007420</td>\n",
              "      <td>-0.107020</td>\n",
              "      <td>0.025611</td>\n",
              "      <td>0.082581</td>\n",
              "      <td>0.117641</td>\n",
              "      <td>-0.080366</td>\n",
              "      <td>-0.057848</td>\n",
              "      <td>0.007472</td>\n",
              "      <td>0.040187</td>\n",
              "      <td>0.015901</td>\n",
              "      <td>0.026445</td>\n",
              "      <td>0.038720</td>\n",
              "      <td>0.048357</td>\n",
              "      <td>-0.055760</td>\n",
              "      <td>0.038609</td>\n",
              "      <td>0.034306</td>\n",
              "      <td>-0.152562</td>\n",
              "      <td>-0.105931</td>\n",
              "      <td>0.095141</td>\n",
              "      <td>-0.011897</td>\n",
              "      <td>-0.013230</td>\n",
              "      <td>-0.065061</td>\n",
              "      <td>-0.130545</td>\n",
              "      <td>0.016349</td>\n",
              "      <td>0.089068</td>\n",
              "      <td>0.017705</td>\n",
              "      <td>-0.019113</td>\n",
              "      <td>0.050886</td>\n",
              "      <td>-0.137695</td>\n",
              "      <td>0.223364</td>\n",
              "      <td>0.012050</td>\n",
              "      <td>0.026917</td>\n",
              "      <td>0.019598</td>\n",
              "      <td>-0.057053</td>\n",
              "      <td>0.055734</td>\n",
              "      <td>-0.056854</td>\n",
              "      <td>0.022518</td>\n",
              "      <td>-0.058526</td>\n",
              "      <td>0.042130</td>\n",
              "      <td>0.006001</td>\n",
              "      <td>0.010803</td>\n",
              "      <td>0.009626</td>\n",
              "      <td>-0.032889</td>\n",
              "      <td>-0.019396</td>\n",
              "      <td>-0.083147</td>\n",
              "      <td>0.046321</td>\n",
              "      <td>-0.046790</td>\n",
              "      <td>-0.085031</td>\n",
              "      <td>0.119106</td>\n",
              "      <td>-0.171291</td>\n",
              "      <td>-0.068813</td>\n",
              "      <td>0.117058</td>\n",
              "      <td>-0.02260</td>\n",
              "      <td>-0.038156</td>\n",
              "      <td>0.048811</td>\n",
              "      <td>-0.078201</td>\n",
              "      <td>0.080414</td>\n",
              "      <td>0.110713</td>\n",
              "      <td>0.029674</td>\n",
              "      <td>0.084643</td>\n",
              "      <td>0.007516</td>\n",
              "      <td>-0.212769</td>\n",
              "      <td>0.045794</td>\n",
              "      <td>0.056074</td>\n",
              "      <td>0.072498</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.045323</td>\n",
              "      <td>0.006836</td>\n",
              "      <td>0.010429</td>\n",
              "      <td>0.001552</td>\n",
              "      <td>0.008327</td>\n",
              "      <td>-0.044635</td>\n",
              "      <td>0.028966</td>\n",
              "      <td>0.043653</td>\n",
              "      <td>0.115702</td>\n",
              "      <td>0.058755</td>\n",
              "      <td>-0.003577</td>\n",
              "      <td>-0.022288</td>\n",
              "      <td>-0.006223</td>\n",
              "      <td>-0.064606</td>\n",
              "      <td>-0.084874</td>\n",
              "      <td>-0.045925</td>\n",
              "      <td>-0.010803</td>\n",
              "      <td>0.144930</td>\n",
              "      <td>0.027706</td>\n",
              "      <td>0.036451</td>\n",
              "      <td>-0.136647</td>\n",
              "      <td>-0.066066</td>\n",
              "      <td>0.026742</td>\n",
              "      <td>0.036355</td>\n",
              "      <td>-0.021649</td>\n",
              "      <td>-0.071716</td>\n",
              "      <td>-0.020543</td>\n",
              "      <td>0.048941</td>\n",
              "      <td>-0.049635</td>\n",
              "      <td>0.007207</td>\n",
              "      <td>-0.107270</td>\n",
              "      <td>-0.095982</td>\n",
              "      <td>-0.026106</td>\n",
              "      <td>0.036152</td>\n",
              "      <td>-0.081857</td>\n",
              "      <td>0.124246</td>\n",
              "      <td>-0.044870</td>\n",
              "      <td>0.009783</td>\n",
              "      <td>-0.002145</td>\n",
              "      <td>-0.028431</td>\n",
              "      <td>-0.000589</td>\n",
              "      <td>-0.108172</td>\n",
              "      <td>-0.059658</td>\n",
              "      <td>-0.093497</td>\n",
              "      <td>0.034520</td>\n",
              "      <td>0.025427</td>\n",
              "      <td>-0.050507</td>\n",
              "      <td>-0.073363</td>\n",
              "      <td>-0.030029</td>\n",
              "      <td>0.006771</td>\n",
              "      <td>0.006856</td>\n",
              "      <td>-0.106724</td>\n",
              "      <td>-0.012695</td>\n",
              "      <td>0.020935</td>\n",
              "      <td>0.178149</td>\n",
              "      <td>-0.012427</td>\n",
              "      <td>-0.010827</td>\n",
              "      <td>-0.006696</td>\n",
              "      <td>0.043917</td>\n",
              "      <td>0.002947</td>\n",
              "      <td>0.122637</td>\n",
              "      <td>0.098938</td>\n",
              "      <td>-0.079065</td>\n",
              "      <td>0.032924</td>\n",
              "      <td>-0.004040</td>\n",
              "      <td>-0.079830</td>\n",
              "      <td>0.036145</td>\n",
              "      <td>-0.063729</td>\n",
              "      <td>-0.036996</td>\n",
              "      <td>-0.019060</td>\n",
              "      <td>-0.004517</td>\n",
              "      <td>0.161743</td>\n",
              "      <td>0.030021</td>\n",
              "      <td>-0.092826</td>\n",
              "      <td>0.113676</td>\n",
              "      <td>-0.090441</td>\n",
              "      <td>-0.054308</td>\n",
              "      <td>-0.043638</td>\n",
              "      <td>-0.074184</td>\n",
              "      <td>-0.077776</td>\n",
              "      <td>-0.064273</td>\n",
              "      <td>-0.104289</td>\n",
              "      <td>0.091649</td>\n",
              "      <td>0.018354</td>\n",
              "      <td>0.064697</td>\n",
              "      <td>0.025271</td>\n",
              "      <td>-0.161298</td>\n",
              "      <td>-0.001225</td>\n",
              "      <td>-0.029922</td>\n",
              "      <td>-0.003283</td>\n",
              "      <td>-0.043265</td>\n",
              "      <td>-0.137870</td>\n",
              "      <td>0.009999</td>\n",
              "      <td>0.023056</td>\n",
              "      <td>0.051784</td>\n",
              "      <td>0.010817</td>\n",
              "      <td>0.058472</td>\n",
              "      <td>0.121730</td>\n",
              "      <td>-0.068704</td>\n",
              "      <td>-0.083221</td>\n",
              "      <td>0.079237</td>\n",
              "      <td>-0.071141</td>\n",
              "      <td>-0.091073</td>\n",
              "      <td>0.047538</td>\n",
              "      <td>0.063494</td>\n",
              "      <td>-0.018337</td>\n",
              "      <td>-0.025423</td>\n",
              "      <td>-0.020890</td>\n",
              "      <td>-0.072911</td>\n",
              "      <td>0.053816</td>\n",
              "      <td>-0.024026</td>\n",
              "      <td>0.019584</td>\n",
              "      <td>0.008074</td>\n",
              "      <td>0.011818</td>\n",
              "      <td>-0.053589</td>\n",
              "      <td>0.000462</td>\n",
              "      <td>0.176505</td>\n",
              "      <td>-0.008375</td>\n",
              "      <td>-0.136230</td>\n",
              "      <td>-0.138829</td>\n",
              "      <td>-0.090470</td>\n",
              "      <td>0.018563</td>\n",
              "      <td>0.064261</td>\n",
              "      <td>-0.087738</td>\n",
              "      <td>-0.071213</td>\n",
              "      <td>0.036503</td>\n",
              "      <td>-0.151509</td>\n",
              "      <td>-0.030448</td>\n",
              "      <td>-0.031996</td>\n",
              "      <td>-0.064571</td>\n",
              "      <td>0.040584</td>\n",
              "      <td>-0.081959</td>\n",
              "      <td>0.092773</td>\n",
              "      <td>-0.139744</td>\n",
              "      <td>0.042459</td>\n",
              "      <td>-0.032533</td>\n",
              "      <td>-0.027989</td>\n",
              "      <td>0.098371</td>\n",
              "      <td>0.043300</td>\n",
              "      <td>-0.135919</td>\n",
              "      <td>-0.003034</td>\n",
              "      <td>-0.030036</td>\n",
              "      <td>0.065979</td>\n",
              "      <td>0.013876</td>\n",
              "      <td>-0.017308</td>\n",
              "      <td>0.077567</td>\n",
              "      <td>-0.013354</td>\n",
              "      <td>0.001551</td>\n",
              "      <td>0.046534</td>\n",
              "      <td>-0.043215</td>\n",
              "      <td>0.047590</td>\n",
              "      <td>-0.033900</td>\n",
              "      <td>-0.018250</td>\n",
              "      <td>0.006884</td>\n",
              "      <td>-0.033901</td>\n",
              "      <td>0.062081</td>\n",
              "      <td>0.028909</td>\n",
              "      <td>-0.053926</td>\n",
              "      <td>-0.154968</td>\n",
              "      <td>0.101880</td>\n",
              "      <td>0.072985</td>\n",
              "      <td>0.126535</td>\n",
              "      <td>-0.017020</td>\n",
              "      <td>-0.046378</td>\n",
              "      <td>-0.105586</td>\n",
              "      <td>-0.020673</td>\n",
              "      <td>0.043405</td>\n",
              "      <td>0.009373</td>\n",
              "      <td>0.068233</td>\n",
              "      <td>-0.097857</td>\n",
              "      <td>-0.207807</td>\n",
              "      <td>-0.008571</td>\n",
              "      <td>0.104527</td>\n",
              "      <td>0.096941</td>\n",
              "      <td>0.115147</td>\n",
              "      <td>-0.008942</td>\n",
              "      <td>-0.110573</td>\n",
              "      <td>-0.040229</td>\n",
              "      <td>-0.051553</td>\n",
              "      <td>0.031355</td>\n",
              "      <td>-0.105504</td>\n",
              "      <td>0.010230</td>\n",
              "      <td>-0.020512</td>\n",
              "      <td>-0.078979</td>\n",
              "      <td>0.013674</td>\n",
              "      <td>-0.064750</td>\n",
              "      <td>0.162955</td>\n",
              "      <td>-0.077624</td>\n",
              "      <td>-0.150068</td>\n",
              "      <td>-0.085999</td>\n",
              "      <td>-0.046899</td>\n",
              "      <td>-0.023124</td>\n",
              "      <td>0.054796</td>\n",
              "      <td>0.170694</td>\n",
              "      <td>0.074289</td>\n",
              "      <td>0.063477</td>\n",
              "      <td>-0.080540</td>\n",
              "      <td>-0.018136</td>\n",
              "      <td>-0.030698</td>\n",
              "      <td>0.003611</td>\n",
              "      <td>0.047468</td>\n",
              "      <td>0.082973</td>\n",
              "      <td>-0.017027</td>\n",
              "      <td>0.099034</td>\n",
              "      <td>0.100255</td>\n",
              "      <td>0.036614</td>\n",
              "      <td>-0.064976</td>\n",
              "      <td>-0.057118</td>\n",
              "      <td>-0.088784</td>\n",
              "      <td>-0.036063</td>\n",
              "      <td>0.114485</td>\n",
              "      <td>-0.024549</td>\n",
              "      <td>0.183432</td>\n",
              "      <td>-0.109658</td>\n",
              "      <td>0.030273</td>\n",
              "      <td>-0.043125</td>\n",
              "      <td>-0.049957</td>\n",
              "      <td>-0.017133</td>\n",
              "      <td>-0.090628</td>\n",
              "      <td>-0.043672</td>\n",
              "      <td>0.064272</td>\n",
              "      <td>19</td>\n",
              "      <td>0.242857</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.005580</td>\n",
              "      <td>0.085300</td>\n",
              "      <td>-0.023612</td>\n",
              "      <td>0.059867</td>\n",
              "      <td>-0.084558</td>\n",
              "      <td>0.055219</td>\n",
              "      <td>0.045292</td>\n",
              "      <td>-0.013961</td>\n",
              "      <td>0.073800</td>\n",
              "      <td>0.073568</td>\n",
              "      <td>-0.064131</td>\n",
              "      <td>-0.087977</td>\n",
              "      <td>-0.001510</td>\n",
              "      <td>0.007506</td>\n",
              "      <td>-0.118765</td>\n",
              "      <td>0.070056</td>\n",
              "      <td>0.081668</td>\n",
              "      <td>0.098182</td>\n",
              "      <td>0.016869</td>\n",
              "      <td>-0.128859</td>\n",
              "      <td>-0.024112</td>\n",
              "      <td>0.031227</td>\n",
              "      <td>0.036170</td>\n",
              "      <td>0.034805</td>\n",
              "      <td>0.022005</td>\n",
              "      <td>0.028271</td>\n",
              "      <td>-0.061917</td>\n",
              "      <td>0.089236</td>\n",
              "      <td>0.052048</td>\n",
              "      <td>-0.070236</td>\n",
              "      <td>-0.072764</td>\n",
              "      <td>0.036908</td>\n",
              "      <td>-0.039874</td>\n",
              "      <td>-0.017802</td>\n",
              "      <td>-0.065566</td>\n",
              "      <td>-0.058662</td>\n",
              "      <td>0.045484</td>\n",
              "      <td>0.060569</td>\n",
              "      <td>0.025748</td>\n",
              "      <td>0.038731</td>\n",
              "      <td>0.050980</td>\n",
              "      <td>-0.052367</td>\n",
              "      <td>0.114041</td>\n",
              "      <td>-0.010232</td>\n",
              "      <td>-0.028403</td>\n",
              "      <td>-0.056405</td>\n",
              "      <td>-0.051394</td>\n",
              "      <td>-0.009512</td>\n",
              "      <td>0.070647</td>\n",
              "      <td>-0.041611</td>\n",
              "      <td>-0.045579</td>\n",
              "      <td>-0.011794</td>\n",
              "      <td>-0.022420</td>\n",
              "      <td>-0.055827</td>\n",
              "      <td>-0.003942</td>\n",
              "      <td>0.041191</td>\n",
              "      <td>-0.021208</td>\n",
              "      <td>-0.079629</td>\n",
              "      <td>-0.036742</td>\n",
              "      <td>-0.022661</td>\n",
              "      <td>-0.138956</td>\n",
              "      <td>0.013124</td>\n",
              "      <td>-0.094502</td>\n",
              "      <td>-0.027921</td>\n",
              "      <td>-0.038336</td>\n",
              "      <td>-0.01165</td>\n",
              "      <td>-0.038306</td>\n",
              "      <td>0.027518</td>\n",
              "      <td>-0.062284</td>\n",
              "      <td>0.097517</td>\n",
              "      <td>0.071055</td>\n",
              "      <td>0.019842</td>\n",
              "      <td>0.093523</td>\n",
              "      <td>-0.024006</td>\n",
              "      <td>-0.124910</td>\n",
              "      <td>-0.081706</td>\n",
              "      <td>0.115077</td>\n",
              "      <td>0.074918</td>\n",
              "      <td>0.014500</td>\n",
              "      <td>0.057425</td>\n",
              "      <td>0.036139</td>\n",
              "      <td>0.020951</td>\n",
              "      <td>0.037078</td>\n",
              "      <td>-0.024989</td>\n",
              "      <td>-0.018503</td>\n",
              "      <td>0.050570</td>\n",
              "      <td>-0.029624</td>\n",
              "      <td>0.062621</td>\n",
              "      <td>0.000421</td>\n",
              "      <td>0.022855</td>\n",
              "      <td>0.025969</td>\n",
              "      <td>-0.038253</td>\n",
              "      <td>-0.012213</td>\n",
              "      <td>-0.031447</td>\n",
              "      <td>0.004621</td>\n",
              "      <td>-0.032744</td>\n",
              "      <td>-0.007686</td>\n",
              "      <td>0.011745</td>\n",
              "      <td>0.081771</td>\n",
              "      <td>0.020228</td>\n",
              "      <td>-0.044685</td>\n",
              "      <td>-0.069836</td>\n",
              "      <td>0.041860</td>\n",
              "      <td>-0.045683</td>\n",
              "      <td>-0.063999</td>\n",
              "      <td>-0.038992</td>\n",
              "      <td>-0.002189</td>\n",
              "      <td>-0.011811</td>\n",
              "      <td>0.044066</td>\n",
              "      <td>-0.044411</td>\n",
              "      <td>-0.047256</td>\n",
              "      <td>0.002104</td>\n",
              "      <td>-0.007041</td>\n",
              "      <td>-0.011091</td>\n",
              "      <td>0.129405</td>\n",
              "      <td>0.018276</td>\n",
              "      <td>-0.043437</td>\n",
              "      <td>-0.050995</td>\n",
              "      <td>0.021507</td>\n",
              "      <td>0.030798</td>\n",
              "      <td>-0.032876</td>\n",
              "      <td>-0.018366</td>\n",
              "      <td>-0.084074</td>\n",
              "      <td>0.048709</td>\n",
              "      <td>0.018249</td>\n",
              "      <td>-0.020988</td>\n",
              "      <td>-0.066153</td>\n",
              "      <td>-0.051706</td>\n",
              "      <td>0.044230</td>\n",
              "      <td>0.009216</td>\n",
              "      <td>-0.024518</td>\n",
              "      <td>-0.026715</td>\n",
              "      <td>-0.053068</td>\n",
              "      <td>0.047969</td>\n",
              "      <td>-0.022892</td>\n",
              "      <td>-0.018158</td>\n",
              "      <td>0.028669</td>\n",
              "      <td>0.019290</td>\n",
              "      <td>0.031894</td>\n",
              "      <td>0.023224</td>\n",
              "      <td>0.090406</td>\n",
              "      <td>-0.056694</td>\n",
              "      <td>-0.025045</td>\n",
              "      <td>0.019466</td>\n",
              "      <td>0.026692</td>\n",
              "      <td>-0.018428</td>\n",
              "      <td>-0.035688</td>\n",
              "      <td>0.010103</td>\n",
              "      <td>-0.035806</td>\n",
              "      <td>-0.020723</td>\n",
              "      <td>0.036973</td>\n",
              "      <td>-0.038289</td>\n",
              "      <td>-0.082985</td>\n",
              "      <td>0.088022</td>\n",
              "      <td>-0.075249</td>\n",
              "      <td>-0.032980</td>\n",
              "      <td>-0.062438</td>\n",
              "      <td>-0.050168</td>\n",
              "      <td>-0.032538</td>\n",
              "      <td>0.008664</td>\n",
              "      <td>-0.014264</td>\n",
              "      <td>0.014338</td>\n",
              "      <td>0.011957</td>\n",
              "      <td>-0.034922</td>\n",
              "      <td>-0.013335</td>\n",
              "      <td>-0.047260</td>\n",
              "      <td>0.021864</td>\n",
              "      <td>-0.013537</td>\n",
              "      <td>-0.040186</td>\n",
              "      <td>0.003110</td>\n",
              "      <td>-0.085474</td>\n",
              "      <td>-0.014884</td>\n",
              "      <td>-0.037386</td>\n",
              "      <td>-0.004133</td>\n",
              "      <td>-0.032421</td>\n",
              "      <td>0.031046</td>\n",
              "      <td>0.108533</td>\n",
              "      <td>-0.189647</td>\n",
              "      <td>-0.065071</td>\n",
              "      <td>-0.008955</td>\n",
              "      <td>-0.127483</td>\n",
              "      <td>-0.074811</td>\n",
              "      <td>0.016072</td>\n",
              "      <td>-0.015883</td>\n",
              "      <td>-0.043636</td>\n",
              "      <td>-0.031738</td>\n",
              "      <td>-0.094789</td>\n",
              "      <td>0.020677</td>\n",
              "      <td>0.075420</td>\n",
              "      <td>0.039272</td>\n",
              "      <td>0.011520</td>\n",
              "      <td>-0.014802</td>\n",
              "      <td>-0.013358</td>\n",
              "      <td>-0.026666</td>\n",
              "      <td>0.002438</td>\n",
              "      <td>0.070307</td>\n",
              "      <td>-0.001578</td>\n",
              "      <td>-0.068348</td>\n",
              "      <td>-0.031263</td>\n",
              "      <td>-0.049007</td>\n",
              "      <td>0.049735</td>\n",
              "      <td>0.068944</td>\n",
              "      <td>-0.060033</td>\n",
              "      <td>-0.062541</td>\n",
              "      <td>0.079628</td>\n",
              "      <td>-0.038818</td>\n",
              "      <td>-0.046268</td>\n",
              "      <td>0.001947</td>\n",
              "      <td>-0.017050</td>\n",
              "      <td>-0.052253</td>\n",
              "      <td>-0.029678</td>\n",
              "      <td>0.118643</td>\n",
              "      <td>-0.110048</td>\n",
              "      <td>0.008860</td>\n",
              "      <td>-0.050145</td>\n",
              "      <td>0.000312</td>\n",
              "      <td>0.003922</td>\n",
              "      <td>0.030736</td>\n",
              "      <td>-0.087338</td>\n",
              "      <td>0.007633</td>\n",
              "      <td>-0.047783</td>\n",
              "      <td>0.033778</td>\n",
              "      <td>-0.028245</td>\n",
              "      <td>0.038771</td>\n",
              "      <td>0.008593</td>\n",
              "      <td>-0.007277</td>\n",
              "      <td>0.022737</td>\n",
              "      <td>0.042860</td>\n",
              "      <td>0.026604</td>\n",
              "      <td>-0.034759</td>\n",
              "      <td>0.009034</td>\n",
              "      <td>-0.037268</td>\n",
              "      <td>0.010572</td>\n",
              "      <td>-0.016896</td>\n",
              "      <td>0.042140</td>\n",
              "      <td>0.007601</td>\n",
              "      <td>-0.008338</td>\n",
              "      <td>-0.056065</td>\n",
              "      <td>0.034869</td>\n",
              "      <td>0.014307</td>\n",
              "      <td>-0.009829</td>\n",
              "      <td>-0.039078</td>\n",
              "      <td>0.009282</td>\n",
              "      <td>-0.096402</td>\n",
              "      <td>0.043873</td>\n",
              "      <td>-0.016066</td>\n",
              "      <td>-0.041573</td>\n",
              "      <td>0.060135</td>\n",
              "      <td>-0.001188</td>\n",
              "      <td>-0.094655</td>\n",
              "      <td>-0.011820</td>\n",
              "      <td>-0.003895</td>\n",
              "      <td>0.079302</td>\n",
              "      <td>0.086066</td>\n",
              "      <td>0.034077</td>\n",
              "      <td>-0.044135</td>\n",
              "      <td>-0.016489</td>\n",
              "      <td>-0.024242</td>\n",
              "      <td>-0.079945</td>\n",
              "      <td>-0.015627</td>\n",
              "      <td>0.008575</td>\n",
              "      <td>-0.000057</td>\n",
              "      <td>-0.038578</td>\n",
              "      <td>0.003882</td>\n",
              "      <td>0.092860</td>\n",
              "      <td>0.070185</td>\n",
              "      <td>0.008929</td>\n",
              "      <td>0.003402</td>\n",
              "      <td>-0.067874</td>\n",
              "      <td>-0.014913</td>\n",
              "      <td>-0.010251</td>\n",
              "      <td>0.092048</td>\n",
              "      <td>0.048665</td>\n",
              "      <td>0.004263</td>\n",
              "      <td>0.082684</td>\n",
              "      <td>-0.070974</td>\n",
              "      <td>-0.085914</td>\n",
              "      <td>-0.121464</td>\n",
              "      <td>-0.108557</td>\n",
              "      <td>0.056362</td>\n",
              "      <td>0.003471</td>\n",
              "      <td>-0.004600</td>\n",
              "      <td>0.027936</td>\n",
              "      <td>0.081766</td>\n",
              "      <td>-0.001350</td>\n",
              "      <td>0.025344</td>\n",
              "      <td>-0.049986</td>\n",
              "      <td>-0.025076</td>\n",
              "      <td>0.005720</td>\n",
              "      <td>0.006427</td>\n",
              "      <td>-0.040072</td>\n",
              "      <td>0.103039</td>\n",
              "      <td>-0.082554</td>\n",
              "      <td>0.022182</td>\n",
              "      <td>-0.003769</td>\n",
              "      <td>0.015608</td>\n",
              "      <td>-0.013759</td>\n",
              "      <td>-0.031591</td>\n",
              "      <td>-0.034226</td>\n",
              "      <td>-0.004203</td>\n",
              "      <td>68</td>\n",
              "      <td>0.097059</td>\n",
              "      <td>0.466667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.016178</td>\n",
              "      <td>0.032145</td>\n",
              "      <td>-0.021306</td>\n",
              "      <td>0.052707</td>\n",
              "      <td>0.063334</td>\n",
              "      <td>0.089003</td>\n",
              "      <td>0.116973</td>\n",
              "      <td>-0.108175</td>\n",
              "      <td>-0.008477</td>\n",
              "      <td>0.036092</td>\n",
              "      <td>0.030233</td>\n",
              "      <td>-0.057129</td>\n",
              "      <td>-0.012912</td>\n",
              "      <td>-0.036621</td>\n",
              "      <td>-0.168918</td>\n",
              "      <td>0.137004</td>\n",
              "      <td>0.072500</td>\n",
              "      <td>0.156752</td>\n",
              "      <td>0.005157</td>\n",
              "      <td>-0.032732</td>\n",
              "      <td>-0.072367</td>\n",
              "      <td>0.013156</td>\n",
              "      <td>0.070360</td>\n",
              "      <td>0.033040</td>\n",
              "      <td>0.072171</td>\n",
              "      <td>-0.026530</td>\n",
              "      <td>-0.108005</td>\n",
              "      <td>-0.045164</td>\n",
              "      <td>-0.040053</td>\n",
              "      <td>-0.062771</td>\n",
              "      <td>-0.045529</td>\n",
              "      <td>0.029690</td>\n",
              "      <td>0.041395</td>\n",
              "      <td>-0.037693</td>\n",
              "      <td>0.070801</td>\n",
              "      <td>-0.049967</td>\n",
              "      <td>0.081733</td>\n",
              "      <td>0.040507</td>\n",
              "      <td>0.071876</td>\n",
              "      <td>0.079305</td>\n",
              "      <td>0.080473</td>\n",
              "      <td>-0.023105</td>\n",
              "      <td>0.105306</td>\n",
              "      <td>0.063653</td>\n",
              "      <td>-0.114855</td>\n",
              "      <td>-0.010607</td>\n",
              "      <td>-0.070160</td>\n",
              "      <td>-0.068264</td>\n",
              "      <td>-0.009111</td>\n",
              "      <td>0.037232</td>\n",
              "      <td>-0.004612</td>\n",
              "      <td>0.061157</td>\n",
              "      <td>0.038808</td>\n",
              "      <td>-0.024482</td>\n",
              "      <td>0.019803</td>\n",
              "      <td>-0.011163</td>\n",
              "      <td>-0.060235</td>\n",
              "      <td>-0.065362</td>\n",
              "      <td>0.084656</td>\n",
              "      <td>-0.125081</td>\n",
              "      <td>0.001655</td>\n",
              "      <td>0.090416</td>\n",
              "      <td>-0.073310</td>\n",
              "      <td>0.002609</td>\n",
              "      <td>0.008274</td>\n",
              "      <td>-0.04796</td>\n",
              "      <td>-0.049181</td>\n",
              "      <td>0.043165</td>\n",
              "      <td>-0.062676</td>\n",
              "      <td>0.011047</td>\n",
              "      <td>0.082601</td>\n",
              "      <td>0.003052</td>\n",
              "      <td>0.113376</td>\n",
              "      <td>0.021973</td>\n",
              "      <td>-0.134576</td>\n",
              "      <td>0.004964</td>\n",
              "      <td>0.085649</td>\n",
              "      <td>0.083908</td>\n",
              "      <td>-0.016900</td>\n",
              "      <td>0.025180</td>\n",
              "      <td>0.014476</td>\n",
              "      <td>-0.015774</td>\n",
              "      <td>0.081594</td>\n",
              "      <td>-0.017470</td>\n",
              "      <td>-0.116699</td>\n",
              "      <td>0.006321</td>\n",
              "      <td>-0.035468</td>\n",
              "      <td>0.115857</td>\n",
              "      <td>0.026408</td>\n",
              "      <td>0.031477</td>\n",
              "      <td>0.058539</td>\n",
              "      <td>0.053507</td>\n",
              "      <td>-0.065267</td>\n",
              "      <td>-0.044169</td>\n",
              "      <td>-0.013536</td>\n",
              "      <td>-0.029460</td>\n",
              "      <td>0.063104</td>\n",
              "      <td>0.020223</td>\n",
              "      <td>0.061117</td>\n",
              "      <td>-0.051639</td>\n",
              "      <td>-0.121392</td>\n",
              "      <td>-0.038249</td>\n",
              "      <td>0.071242</td>\n",
              "      <td>-0.024224</td>\n",
              "      <td>-0.027161</td>\n",
              "      <td>-0.023473</td>\n",
              "      <td>-0.006612</td>\n",
              "      <td>-0.025869</td>\n",
              "      <td>0.074678</td>\n",
              "      <td>-0.058160</td>\n",
              "      <td>0.002631</td>\n",
              "      <td>-0.057838</td>\n",
              "      <td>0.068186</td>\n",
              "      <td>0.005851</td>\n",
              "      <td>0.054091</td>\n",
              "      <td>-0.056695</td>\n",
              "      <td>-0.021244</td>\n",
              "      <td>-0.109402</td>\n",
              "      <td>-0.001017</td>\n",
              "      <td>0.028402</td>\n",
              "      <td>-0.072873</td>\n",
              "      <td>0.033407</td>\n",
              "      <td>-0.041880</td>\n",
              "      <td>0.083550</td>\n",
              "      <td>-0.066332</td>\n",
              "      <td>-0.010379</td>\n",
              "      <td>-0.018158</td>\n",
              "      <td>-0.035507</td>\n",
              "      <td>-0.020643</td>\n",
              "      <td>0.038194</td>\n",
              "      <td>-0.026259</td>\n",
              "      <td>-0.119439</td>\n",
              "      <td>-0.069383</td>\n",
              "      <td>0.009928</td>\n",
              "      <td>-0.034403</td>\n",
              "      <td>-0.020060</td>\n",
              "      <td>-0.034234</td>\n",
              "      <td>0.097619</td>\n",
              "      <td>0.040541</td>\n",
              "      <td>0.036620</td>\n",
              "      <td>-0.002007</td>\n",
              "      <td>-0.006273</td>\n",
              "      <td>0.042508</td>\n",
              "      <td>-0.036251</td>\n",
              "      <td>-0.042101</td>\n",
              "      <td>0.049873</td>\n",
              "      <td>-0.114882</td>\n",
              "      <td>-0.051134</td>\n",
              "      <td>-0.062039</td>\n",
              "      <td>-0.025370</td>\n",
              "      <td>0.063938</td>\n",
              "      <td>0.036553</td>\n",
              "      <td>-0.043240</td>\n",
              "      <td>0.084825</td>\n",
              "      <td>-0.044798</td>\n",
              "      <td>-0.007982</td>\n",
              "      <td>-0.021142</td>\n",
              "      <td>-0.037876</td>\n",
              "      <td>-0.042996</td>\n",
              "      <td>-0.048151</td>\n",
              "      <td>-0.037842</td>\n",
              "      <td>0.102960</td>\n",
              "      <td>-0.030192</td>\n",
              "      <td>0.122423</td>\n",
              "      <td>0.056207</td>\n",
              "      <td>-0.155138</td>\n",
              "      <td>0.060086</td>\n",
              "      <td>-0.094957</td>\n",
              "      <td>0.008613</td>\n",
              "      <td>0.034614</td>\n",
              "      <td>-0.023926</td>\n",
              "      <td>0.044067</td>\n",
              "      <td>-0.051721</td>\n",
              "      <td>-0.049127</td>\n",
              "      <td>-0.106676</td>\n",
              "      <td>0.038439</td>\n",
              "      <td>0.126329</td>\n",
              "      <td>-0.045193</td>\n",
              "      <td>0.038323</td>\n",
              "      <td>0.029165</td>\n",
              "      <td>-0.081943</td>\n",
              "      <td>-0.040249</td>\n",
              "      <td>0.023356</td>\n",
              "      <td>-0.035556</td>\n",
              "      <td>-0.011146</td>\n",
              "      <td>-0.024127</td>\n",
              "      <td>0.034336</td>\n",
              "      <td>0.014733</td>\n",
              "      <td>-0.037503</td>\n",
              "      <td>0.079902</td>\n",
              "      <td>-0.007161</td>\n",
              "      <td>0.015959</td>\n",
              "      <td>0.023739</td>\n",
              "      <td>-0.019365</td>\n",
              "      <td>-0.036099</td>\n",
              "      <td>-0.036492</td>\n",
              "      <td>0.013604</td>\n",
              "      <td>0.036818</td>\n",
              "      <td>-0.060306</td>\n",
              "      <td>-0.010193</td>\n",
              "      <td>-0.064548</td>\n",
              "      <td>0.039951</td>\n",
              "      <td>-0.097358</td>\n",
              "      <td>-0.046848</td>\n",
              "      <td>-0.040405</td>\n",
              "      <td>-0.014883</td>\n",
              "      <td>-0.093262</td>\n",
              "      <td>0.016825</td>\n",
              "      <td>0.006015</td>\n",
              "      <td>-0.006538</td>\n",
              "      <td>0.067824</td>\n",
              "      <td>-0.042813</td>\n",
              "      <td>-0.106947</td>\n",
              "      <td>-0.000834</td>\n",
              "      <td>-0.022359</td>\n",
              "      <td>0.021817</td>\n",
              "      <td>0.088433</td>\n",
              "      <td>0.088169</td>\n",
              "      <td>-0.063761</td>\n",
              "      <td>-0.025002</td>\n",
              "      <td>0.001268</td>\n",
              "      <td>-0.074894</td>\n",
              "      <td>0.034695</td>\n",
              "      <td>0.035434</td>\n",
              "      <td>0.083103</td>\n",
              "      <td>-0.079054</td>\n",
              "      <td>0.005493</td>\n",
              "      <td>0.086297</td>\n",
              "      <td>0.002045</td>\n",
              "      <td>-0.011959</td>\n",
              "      <td>0.053433</td>\n",
              "      <td>-0.061761</td>\n",
              "      <td>0.021627</td>\n",
              "      <td>0.016259</td>\n",
              "      <td>0.094652</td>\n",
              "      <td>-0.042352</td>\n",
              "      <td>-0.056396</td>\n",
              "      <td>-0.068651</td>\n",
              "      <td>0.053593</td>\n",
              "      <td>0.038059</td>\n",
              "      <td>-0.001107</td>\n",
              "      <td>-0.004272</td>\n",
              "      <td>-0.027066</td>\n",
              "      <td>-0.054498</td>\n",
              "      <td>0.010640</td>\n",
              "      <td>0.057631</td>\n",
              "      <td>0.004868</td>\n",
              "      <td>0.038642</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>-0.102471</td>\n",
              "      <td>-0.035177</td>\n",
              "      <td>0.098728</td>\n",
              "      <td>0.009087</td>\n",
              "      <td>0.038574</td>\n",
              "      <td>0.080838</td>\n",
              "      <td>-0.014540</td>\n",
              "      <td>-0.026733</td>\n",
              "      <td>0.013217</td>\n",
              "      <td>-0.112739</td>\n",
              "      <td>-0.050128</td>\n",
              "      <td>-0.004123</td>\n",
              "      <td>-0.004612</td>\n",
              "      <td>-0.067491</td>\n",
              "      <td>0.011224</td>\n",
              "      <td>-0.000841</td>\n",
              "      <td>0.143175</td>\n",
              "      <td>0.010417</td>\n",
              "      <td>-0.015523</td>\n",
              "      <td>-0.070543</td>\n",
              "      <td>-0.007595</td>\n",
              "      <td>0.055779</td>\n",
              "      <td>0.053182</td>\n",
              "      <td>0.076362</td>\n",
              "      <td>0.029573</td>\n",
              "      <td>0.070814</td>\n",
              "      <td>0.003160</td>\n",
              "      <td>-0.110260</td>\n",
              "      <td>-0.047377</td>\n",
              "      <td>-0.164252</td>\n",
              "      <td>0.011909</td>\n",
              "      <td>-0.011544</td>\n",
              "      <td>-0.013593</td>\n",
              "      <td>0.032905</td>\n",
              "      <td>0.106418</td>\n",
              "      <td>0.099162</td>\n",
              "      <td>0.093560</td>\n",
              "      <td>-0.094699</td>\n",
              "      <td>0.010837</td>\n",
              "      <td>0.008592</td>\n",
              "      <td>0.138273</td>\n",
              "      <td>-0.102301</td>\n",
              "      <td>0.092102</td>\n",
              "      <td>-0.125210</td>\n",
              "      <td>-0.039361</td>\n",
              "      <td>-0.045234</td>\n",
              "      <td>-0.004466</td>\n",
              "      <td>-0.005018</td>\n",
              "      <td>-0.056888</td>\n",
              "      <td>-0.016968</td>\n",
              "      <td>-0.004015</td>\n",
              "      <td>10</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.650000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entraînement\n",
        "algos = {\n",
        "  'RF' : RandomForestRegressor(n_estimators=50,random_state=1,n_jobs=-1),\n",
        "  'KNN' : KNeighborsRegressor(n_neighbors=5,n_jobs=-1,metric='cosine'),\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(20,10),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models(corpus_train_wv_google_meta,y_train,corpus_test_wv_google_meta,y_test.to_numpy(),algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "Bx9Q3mWW2qj1",
        "outputId": "26771620-2b08-4d83-90b3-6a733aba948d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## RF #############\n",
            "MAE = 0.977, Accuracy = 0.285, Customized Accuracy = 0.786\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[ 101, 1017,  736,  125,    0],\n",
              "       [  25,  738, 1043,  177,    0],\n",
              "       [  11,  416, 1152,  368,    4],\n",
              "       [   2,  174, 1043,  758,   24],\n",
              "       [   2,  112,  792, 1078,  102]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## KNN #############\n",
            "MAE = 1.012, Accuracy = 0.295, Customized Accuracy = 0.759\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[265, 789, 688, 216,  21],\n",
              "       [102, 644, 853, 354,  30],\n",
              "       [ 49, 445, 860, 534,  63],\n",
              "       [ 16, 212, 807, 818, 148],\n",
              "       [ 10, 160, 594, 963, 359]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## MLP #############\n",
            "MAE = 0.890, Accuracy = 0.348, Customized Accuracy = 0.823\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[ 431,  985,  439,  110,   14],\n",
              "       [ 139,  906,  732,  196,   10],\n",
              "       [  55,  549,  940,  383,   24],\n",
              "       [  12,  229,  817,  819,  124],\n",
              "       [   8,  144,  529, 1021,  384]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tous les modèles ont nettement été améliorer."
      ],
      "metadata": {
        "id": "mDmX2apq_9Yo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### III. Prise en compte de l'ordre des mots (LSTM)"
      ],
      "metadata": {
        "id": "H2YgxvmfaoJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequences\n",
        "def to_sequence(index, text):\n",
        "    indexes = [index[word] for word in text if word in index]\n",
        "    return indexes"
      ],
      "metadata": {
        "id": "AOemBBhNTlUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = {word: idx for idx, word in enumerate(model_google_vec_neg_300.index_to_key)}\n",
        "X_train_sequences = [to_sequence(word2idx, x) for x in reviews_train_tokens]\n",
        "X_test_sequences = [to_sequence(word2idx, x) for x in  reviews_test_tokens]\n",
        "\n",
        "print(reviews_train_tokens.values[0])\n",
        "print(X_train_sequences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dcsv6aWJCa9o",
        "outputId": "51d5a234-9493-4456-82a5-50066f1f6c90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['good', 'crib', 'live', 'room', 'temp', 'portabl', 'sleep', 'solut', 'rock', 'nice', 'stabl', 'chang', 'tabl', 'useless', 'sound', 'rough', 'otherwis', 'cute', 'us']\n",
            "[127, 27331, 588, 835, 38986, 3629, 2453, 1535, 158051, 13788, 1867, 4876, 10659, 164]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import pad_sequences"
      ],
      "metadata": {
        "id": "N12iMWheHLxZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bbfd889-cafa-4958-d36c-b4e57472b8d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-01-23 15:05:56.055491: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-01-23 15:05:57.306869: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2023-01-23 15:05:57.306930: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2023-01-23 15:05:57.306934: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGHT=50\n",
        "N_FEATURES = len(model_google_vec_neg_300.index_to_key)\n",
        "X_train_sequences = pad_sequences(X_train_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)\n",
        "X_test_sequences = pad_sequences(X_test_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)\n",
        "print(X_train_sequences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny4YcG43HizV",
        "outputId": "8e2dc4ed-bfdc-42ea-ee00-23269a51b8f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000000 3000000 3000000 3000000 3000000 3000000 3000000 3000000 3000000\n",
            " 3000000 3000000 3000000 3000000 3000000 3000000 3000000 3000000 3000000\n",
            " 3000000 3000000 3000000 3000000 3000000 3000000 3000000 3000000 3000000\n",
            " 3000000 3000000 3000000 3000000 3000000 3000000 3000000 3000000 3000000\n",
            "     127   27331     588     835   38986    3629    2453    1535  158051\n",
            "   13788    1867    4876   10659     164]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDINGS_LEN = model_google_vec_neg_300.vector_size\n",
        "embeddings_index = np.zeros((len(model_google_vec_neg_300.index_to_key)+1, EMBEDDINGS_LEN))\n",
        "embeddings_index.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLmRe3N7Ian6",
        "outputId": "864987ea-b897-4d16-faf4-8f0430205e1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000001, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word, idx in word2idx.items():\n",
        "    try:\n",
        "        embedding = model_google_vec_neg_300[word]\n",
        "        embeddings_index[idx] = embedding\n",
        "    except:\n",
        "        pass\n",
        "      \n",
        "print(\"EMBEDDINGS_LEN=\", EMBEDDINGS_LEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tieema5TIx5V",
        "outputId": "e439fda9-4f39-4d8e-ef7d-ac5742d831d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EMBEDDINGS_LEN= 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense,LSTM\n",
        "\n",
        "\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(len(model_google_vec_neg_300.key_to_index)+1,\n",
        "                    EMBEDDINGS_LEN,  # Embedding size\n",
        "                    weights=[embeddings_index],\n",
        "                    trainable=False))\n",
        "\n",
        "model_lstm.add(LSTM(300, dropout=0.2))\n",
        "\n",
        "model_lstm.add(Dense(1))\n",
        "model_lstm.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "model_lstm.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMMbMLjPJSHe",
        "outputId": "ac8fe9e5-b734-48ed-aecd-9898dc792b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-01-23 15:26:28.525356: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 3600001200 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, None, 300)         900000300 \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 300)               721200    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 301       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 900,721,801\n",
            "Trainable params: 721,501\n",
            "Non-trainable params: 900,000,300\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_lstm.fit(X_train_sequences, y_train, epochs=10, batch_size=128, verbose=1,validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVOMUHtHJZH9",
        "outputId": "72d038ca-ddd1-43a7-bd9c-d367fb7e161f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "282/282 [==============================] - 41s 142ms/step - loss: 1.0649 - mean_absolute_error: 1.0649 - val_loss: 0.9888 - val_mean_absolute_error: 0.9888\n",
            "Epoch 2/10\n",
            "282/282 [==============================] - 39s 138ms/step - loss: 0.9595 - mean_absolute_error: 0.9595 - val_loss: 0.9354 - val_mean_absolute_error: 0.9354\n",
            "Epoch 3/10\n",
            "282/282 [==============================] - 39s 138ms/step - loss: 0.9198 - mean_absolute_error: 0.9198 - val_loss: 0.8968 - val_mean_absolute_error: 0.8968\n",
            "Epoch 4/10\n",
            "282/282 [==============================] - 39s 138ms/step - loss: 0.8930 - mean_absolute_error: 0.8930 - val_loss: 0.8825 - val_mean_absolute_error: 0.8825\n",
            "Epoch 5/10\n",
            "282/282 [==============================] - 39s 137ms/step - loss: 0.8660 - mean_absolute_error: 0.8660 - val_loss: 0.9285 - val_mean_absolute_error: 0.9285\n",
            "Epoch 6/10\n",
            "282/282 [==============================] - 39s 138ms/step - loss: 0.8469 - mean_absolute_error: 0.8469 - val_loss: 0.8541 - val_mean_absolute_error: 0.8541\n",
            "Epoch 7/10\n",
            "282/282 [==============================] - 39s 138ms/step - loss: 0.8320 - mean_absolute_error: 0.8320 - val_loss: 0.8559 - val_mean_absolute_error: 0.8559\n",
            "Epoch 8/10\n",
            "282/282 [==============================] - 39s 137ms/step - loss: 0.8152 - mean_absolute_error: 0.8152 - val_loss: 0.8419 - val_mean_absolute_error: 0.8419\n",
            "Epoch 9/10\n",
            "282/282 [==============================] - 39s 137ms/step - loss: 0.7918 - mean_absolute_error: 0.7918 - val_loss: 0.8337 - val_mean_absolute_error: 0.8337\n",
            "Epoch 10/10\n",
            "282/282 [==============================] - 39s 138ms/step - loss: 0.7731 - mean_absolute_error: 0.7731 - val_loss: 0.8418 - val_mean_absolute_error: 0.8418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores_lstm = model_lstm.evaluate(X_test_sequences, y_test, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imbJPv-_JpKp",
        "outputId": "50c47606-2c21-4ece-f095-7ba182b68a20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 8s 24ms/step - loss: 0.8675 - mean_absolute_error: 0.8675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction=model_lstm.predict(X_test_sequences)\n",
        "prediction[prediction<1]=1\n",
        "prediction[prediction>5]=5\n",
        "ACC=accuracy_score(y_test,np.round(prediction))\n",
        "MAE=mean_absolute_error(y_test,prediction)\n",
        "CACC=customized_accuracy(y_test.to_numpy(), np.round(prediction))\n",
        "print('For LSTM MAE = {0:.3f}, Accuracy = {1:.3f}, Customized Accuracy = {2:.3f}'.format(MAE,ACC,CACC))\n",
        "display(confusion_matrix(y_test,np.round(prediction)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "MBQqqNLSJ5xv",
        "outputId": "04a0fa49-7401-4ea0-e657-776b5672e231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 8s 24ms/step\n",
            "For LSTM MAE = 0.862, Accuracy = 0.389, Customized Accuracy = 0.828\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[876, 701, 275, 115,  12],\n",
              "       [379, 885, 540, 170,   9],\n",
              "       [123, 629, 794, 377,  28],\n",
              "       [ 27, 273, 691, 835, 175],\n",
              "       [ 35, 207, 451, 889, 504]])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Meilleur modèle que le précédent."
      ],
      "metadata": {
        "id": "w0GSMH_P98kI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### IV. Utilisation de la sortie du LSTM dans d'autres modèles seupervisés"
      ],
      "metadata": {
        "id": "LSy2FOerfTXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "Model_rf = Model(inputs=model_lstm.inputs, outputs=model_lstm.layers[1].output)\n",
        "#Model_rf.save('../models/model_lstm_features.h5')\n",
        "corpus_train_lstm = Model_rf.predict(X_train_sequences)\n",
        "corpus_test_lstm = Model_rf.predict(X_test_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j30gtdsHfcmZ",
        "outputId": "f94bb09e-a3d0-4ab3-ad8a-adb56922c50a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1250/1250 [==============================] - 30s 24ms/step\n",
            "313/313 [==============================] - 8s 24ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "algos = {\n",
        "  'RF' : RandomForestRegressor(n_estimators=50,random_state=1,n_jobs=-1),\n",
        "  'KNN' : KNeighborsRegressor(n_neighbors=5,n_jobs=-1,metric='cosine'),\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(20,10),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models(corpus_train_lstm,y_train,corpus_test_lstm,y_test.to_numpy(),algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "j8pJYXFLfjpg",
        "outputId": "37a79032-eee4-4a18-c509-f5fe72a2a012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## RF #############\n",
            "MAE = 0.869, Accuracy = 0.362, Customized Accuracy = 0.837\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[ 478,  952,  399,  134,   16],\n",
              "       [ 173,  869,  713,  220,    8],\n",
              "       [  38,  497,  910,  483,   23],\n",
              "       [  10,  173,  716,  954,  148],\n",
              "       [   9,  137,  465, 1061,  414]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## KNN #############\n",
            "MAE = 0.901, Accuracy = 0.369, Customized Accuracy = 0.811\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[620, 785, 366, 173,  35],\n",
              "       [253, 784, 613, 294,  39],\n",
              "       [ 79, 486, 766, 537,  83],\n",
              "       [ 25, 205, 584, 896, 291],\n",
              "       [ 24, 159, 404, 870, 629]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## MLP #############\n",
            "MAE = 0.899, Accuracy = 0.380, Customized Accuracy = 0.809\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[797, 604, 371, 170,  37],\n",
              "       [352, 707, 599, 299,  26],\n",
              "       [129, 458, 767, 531,  66],\n",
              "       [ 49, 189, 591, 915, 257],\n",
              "       [ 63, 155, 351, 907, 610]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il y a amélioration du Random Forest, du KNN mais le MLP n'a gagné qu'en Accuracy, les deux autres métriques ont baissé."
      ],
      "metadata": {
        "id": "_qdpljcQESk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C. Utilisation du commentaire et du titre du produit"
      ],
      "metadata": {
        "id": "T6PpeqQ9aLfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I. Moyenne des embeddings"
      ],
      "metadata": {
        "id": "pR_eGwJSazgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size=model_google_vec_neg_300.vector_size\n",
        "\n",
        "reviews_train_wv_google=word2vec_generator(reviews_train_tokens,model_google_vec_neg_300,vector_size)\n",
        "reviews_test_wv_google=word2vec_generator(reviews_test_tokens,model_google_vec_neg_300,vector_size)\n",
        "\n",
        "title_train_wv_google=word2vec_generator(title_train_tokens,model_google_vec_neg_300,vector_size)\n",
        "title_test_wv_google=word2vec_generator(title_test_tokens,model_google_vec_neg_300,vector_size)\n",
        "\n",
        "# renommage des colonnes de titles...\n",
        "title_train_wv_google.columns = [\"t\" + str(c) for c in title_train_wv_google.columns]\n",
        "title_test_wv_google.columns = [\"t\" + str(c) for c in title_test_wv_google.columns]"
      ],
      "metadata": {
        "id": "0drcmc-4NEIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rev_a_title_train_wv_google = pd.concat([reviews_train_wv_google, title_train_wv_google], axis=1)\n",
        "rev_a_title_test_wv_google = pd.concat([reviews_test_wv_google, title_test_wv_google], axis=1)\n",
        "\n",
        "rev_a_title_train_wv_google.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "6lceKavgObvH",
        "outputId": "54a5df53-dd8a-4070-d495-0773c17314de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3         4         5         6  \\\n",
              "0  0.029349  0.039355 -0.043146  0.137334 -0.050119  0.011490  0.037109   \n",
              "1  0.005580  0.085300 -0.023612  0.059867 -0.084558  0.055219  0.045292   \n",
              "2  0.016178  0.032145 -0.021306  0.052707  0.063334  0.089003  0.116973   \n",
              "\n",
              "          7         8         9        10        11        12        13  \\\n",
              "0 -0.058620  0.036689  0.074262 -0.052490 -0.120954 -0.065456 -0.007420   \n",
              "1 -0.013961  0.073800  0.073568 -0.064131 -0.087977 -0.001510  0.007506   \n",
              "2 -0.108175 -0.008477  0.036092  0.030233 -0.057129 -0.012912 -0.036621   \n",
              "\n",
              "         14        15        16        17        18        19        20  \\\n",
              "0 -0.107020  0.025611  0.082581  0.117641 -0.080366 -0.057848  0.007472   \n",
              "1 -0.118765  0.070056  0.081668  0.098182  0.016869 -0.128859 -0.024112   \n",
              "2 -0.168918  0.137004  0.072500  0.156752  0.005157 -0.032732 -0.072367   \n",
              "\n",
              "         21        22        23        24        25        26        27  \\\n",
              "0  0.040187  0.015901  0.026445  0.038720  0.048357 -0.055760  0.038609   \n",
              "1  0.031227  0.036170  0.034805  0.022005  0.028271 -0.061917  0.089236   \n",
              "2  0.013156  0.070360  0.033040  0.072171 -0.026530 -0.108005 -0.045164   \n",
              "\n",
              "         28        29        30        31        32        33        34  \\\n",
              "0  0.034306 -0.152562 -0.105931  0.095141 -0.011897 -0.013230 -0.065061   \n",
              "1  0.052048 -0.070236 -0.072764  0.036908 -0.039874 -0.017802 -0.065566   \n",
              "2 -0.040053 -0.062771 -0.045529  0.029690  0.041395 -0.037693  0.070801   \n",
              "\n",
              "         35        36        37        38        39        40        41  \\\n",
              "0 -0.130545  0.016349  0.089068  0.017705 -0.019113  0.050886 -0.137695   \n",
              "1 -0.058662  0.045484  0.060569  0.025748  0.038731  0.050980 -0.052367   \n",
              "2 -0.049967  0.081733  0.040507  0.071876  0.079305  0.080473 -0.023105   \n",
              "\n",
              "         42        43        44        45        46        47        48  \\\n",
              "0  0.223364  0.012050  0.026917  0.019598 -0.057053  0.055734 -0.056854   \n",
              "1  0.114041 -0.010232 -0.028403 -0.056405 -0.051394 -0.009512  0.070647   \n",
              "2  0.105306  0.063653 -0.114855 -0.010607 -0.070160 -0.068264 -0.009111   \n",
              "\n",
              "         49        50        51        52        53        54        55  \\\n",
              "0  0.022518 -0.058526  0.042130  0.006001  0.010803  0.009626 -0.032889   \n",
              "1 -0.041611 -0.045579 -0.011794 -0.022420 -0.055827 -0.003942  0.041191   \n",
              "2  0.037232 -0.004612  0.061157  0.038808 -0.024482  0.019803 -0.011163   \n",
              "\n",
              "         56        57        58        59        60        61        62  \\\n",
              "0 -0.019396 -0.083147  0.046321 -0.046790 -0.085031  0.119106 -0.171291   \n",
              "1 -0.021208 -0.079629 -0.036742 -0.022661 -0.138956  0.013124 -0.094502   \n",
              "2 -0.060235 -0.065362  0.084656 -0.125081  0.001655  0.090416 -0.073310   \n",
              "\n",
              "         63        64       65        66        67        68        69  \\\n",
              "0 -0.068813  0.117058 -0.02260 -0.038156  0.048811 -0.078201  0.080414   \n",
              "1 -0.027921 -0.038336 -0.01165 -0.038306  0.027518 -0.062284  0.097517   \n",
              "2  0.002609  0.008274 -0.04796 -0.049181  0.043165 -0.062676  0.011047   \n",
              "\n",
              "         70        71        72        73        74        75        76  \\\n",
              "0  0.110713  0.029674  0.084643  0.007516 -0.212769  0.045794  0.056074   \n",
              "1  0.071055  0.019842  0.093523 -0.024006 -0.124910 -0.081706  0.115077   \n",
              "2  0.082601  0.003052  0.113376  0.021973 -0.134576  0.004964  0.085649   \n",
              "\n",
              "         77        78        79        80        81        82        83  \\\n",
              "0  0.072498  0.071429  0.045323  0.006836  0.010429  0.001552  0.008327   \n",
              "1  0.074918  0.014500  0.057425  0.036139  0.020951  0.037078 -0.024989   \n",
              "2  0.083908 -0.016900  0.025180  0.014476 -0.015774  0.081594 -0.017470   \n",
              "\n",
              "         84        85        86        87        88        89        90  \\\n",
              "0 -0.044635  0.028966  0.043653  0.115702  0.058755 -0.003577 -0.022288   \n",
              "1 -0.018503  0.050570 -0.029624  0.062621  0.000421  0.022855  0.025969   \n",
              "2 -0.116699  0.006321 -0.035468  0.115857  0.026408  0.031477  0.058539   \n",
              "\n",
              "         91        92        93        94        95        96        97  \\\n",
              "0 -0.006223 -0.064606 -0.084874 -0.045925 -0.010803  0.144930  0.027706   \n",
              "1 -0.038253 -0.012213 -0.031447  0.004621 -0.032744 -0.007686  0.011745   \n",
              "2  0.053507 -0.065267 -0.044169 -0.013536 -0.029460  0.063104  0.020223   \n",
              "\n",
              "         98        99       100       101       102       103       104  \\\n",
              "0  0.036451 -0.136647 -0.066066  0.026742  0.036355 -0.021649 -0.071716   \n",
              "1  0.081771  0.020228 -0.044685 -0.069836  0.041860 -0.045683 -0.063999   \n",
              "2  0.061117 -0.051639 -0.121392 -0.038249  0.071242 -0.024224 -0.027161   \n",
              "\n",
              "        105       106       107       108       109       110       111  \\\n",
              "0 -0.020543  0.048941 -0.049635  0.007207 -0.107270 -0.095982 -0.026106   \n",
              "1 -0.038992 -0.002189 -0.011811  0.044066 -0.044411 -0.047256  0.002104   \n",
              "2 -0.023473 -0.006612 -0.025869  0.074678 -0.058160  0.002631 -0.057838   \n",
              "\n",
              "        112       113       114       115       116       117       118  \\\n",
              "0  0.036152 -0.081857  0.124246 -0.044870  0.009783 -0.002145 -0.028431   \n",
              "1 -0.007041 -0.011091  0.129405  0.018276 -0.043437 -0.050995  0.021507   \n",
              "2  0.068186  0.005851  0.054091 -0.056695 -0.021244 -0.109402 -0.001017   \n",
              "\n",
              "        119       120       121       122       123       124       125  \\\n",
              "0 -0.000589 -0.108172 -0.059658 -0.093497  0.034520  0.025427 -0.050507   \n",
              "1  0.030798 -0.032876 -0.018366 -0.084074  0.048709  0.018249 -0.020988   \n",
              "2  0.028402 -0.072873  0.033407 -0.041880  0.083550 -0.066332 -0.010379   \n",
              "\n",
              "        126       127       128       129       130       131       132  \\\n",
              "0 -0.073363 -0.030029  0.006771  0.006856 -0.106724 -0.012695  0.020935   \n",
              "1 -0.066153 -0.051706  0.044230  0.009216 -0.024518 -0.026715 -0.053068   \n",
              "2 -0.018158 -0.035507 -0.020643  0.038194 -0.026259 -0.119439 -0.069383   \n",
              "\n",
              "        133       134       135       136       137       138       139  \\\n",
              "0  0.178149 -0.012427 -0.010827 -0.006696  0.043917  0.002947  0.122637   \n",
              "1  0.047969 -0.022892 -0.018158  0.028669  0.019290  0.031894  0.023224   \n",
              "2  0.009928 -0.034403 -0.020060 -0.034234  0.097619  0.040541  0.036620   \n",
              "\n",
              "        140       141       142       143       144       145       146  \\\n",
              "0  0.098938 -0.079065  0.032924 -0.004040 -0.079830  0.036145 -0.063729   \n",
              "1  0.090406 -0.056694 -0.025045  0.019466  0.026692 -0.018428 -0.035688   \n",
              "2 -0.002007 -0.006273  0.042508 -0.036251 -0.042101  0.049873 -0.114882   \n",
              "\n",
              "        147       148       149       150       151       152       153  \\\n",
              "0 -0.036996 -0.019060 -0.004517  0.161743  0.030021 -0.092826  0.113676   \n",
              "1  0.010103 -0.035806 -0.020723  0.036973 -0.038289 -0.082985  0.088022   \n",
              "2 -0.051134 -0.062039 -0.025370  0.063938  0.036553 -0.043240  0.084825   \n",
              "\n",
              "        154       155       156       157       158       159       160  \\\n",
              "0 -0.090441 -0.054308 -0.043638 -0.074184 -0.077776 -0.064273 -0.104289   \n",
              "1 -0.075249 -0.032980 -0.062438 -0.050168 -0.032538  0.008664 -0.014264   \n",
              "2 -0.044798 -0.007982 -0.021142 -0.037876 -0.042996 -0.048151 -0.037842   \n",
              "\n",
              "        161       162       163       164       165       166       167  \\\n",
              "0  0.091649  0.018354  0.064697  0.025271 -0.161298 -0.001225 -0.029922   \n",
              "1  0.014338  0.011957 -0.034922 -0.013335 -0.047260  0.021864 -0.013537   \n",
              "2  0.102960 -0.030192  0.122423  0.056207 -0.155138  0.060086 -0.094957   \n",
              "\n",
              "        168       169       170       171       172       173       174  \\\n",
              "0 -0.003283 -0.043265 -0.137870  0.009999  0.023056  0.051784  0.010817   \n",
              "1 -0.040186  0.003110 -0.085474 -0.014884 -0.037386 -0.004133 -0.032421   \n",
              "2  0.008613  0.034614 -0.023926  0.044067 -0.051721 -0.049127 -0.106676   \n",
              "\n",
              "        175       176       177       178       179       180       181  \\\n",
              "0  0.058472  0.121730 -0.068704 -0.083221  0.079237 -0.071141 -0.091073   \n",
              "1  0.031046  0.108533 -0.189647 -0.065071 -0.008955 -0.127483 -0.074811   \n",
              "2  0.038439  0.126329 -0.045193  0.038323  0.029165 -0.081943 -0.040249   \n",
              "\n",
              "        182       183       184       185       186       187       188  \\\n",
              "0  0.047538  0.063494 -0.018337 -0.025423 -0.020890 -0.072911  0.053816   \n",
              "1  0.016072 -0.015883 -0.043636 -0.031738 -0.094789  0.020677  0.075420   \n",
              "2  0.023356 -0.035556 -0.011146 -0.024127  0.034336  0.014733 -0.037503   \n",
              "\n",
              "        189       190       191       192       193       194       195  \\\n",
              "0 -0.024026  0.019584  0.008074  0.011818 -0.053589  0.000462  0.176505   \n",
              "1  0.039272  0.011520 -0.014802 -0.013358 -0.026666  0.002438  0.070307   \n",
              "2  0.079902 -0.007161  0.015959  0.023739 -0.019365 -0.036099 -0.036492   \n",
              "\n",
              "        196       197       198       199       200       201       202  \\\n",
              "0 -0.008375 -0.136230 -0.138829 -0.090470  0.018563  0.064261 -0.087738   \n",
              "1 -0.001578 -0.068348 -0.031263 -0.049007  0.049735  0.068944 -0.060033   \n",
              "2  0.013604  0.036818 -0.060306 -0.010193 -0.064548  0.039951 -0.097358   \n",
              "\n",
              "        203       204       205       206       207       208       209  \\\n",
              "0 -0.071213  0.036503 -0.151509 -0.030448 -0.031996 -0.064571  0.040584   \n",
              "1 -0.062541  0.079628 -0.038818 -0.046268  0.001947 -0.017050 -0.052253   \n",
              "2 -0.046848 -0.040405 -0.014883 -0.093262  0.016825  0.006015 -0.006538   \n",
              "\n",
              "        210       211       212       213       214       215       216  \\\n",
              "0 -0.081959  0.092773 -0.139744  0.042459 -0.032533 -0.027989  0.098371   \n",
              "1 -0.029678  0.118643 -0.110048  0.008860 -0.050145  0.000312  0.003922   \n",
              "2  0.067824 -0.042813 -0.106947 -0.000834 -0.022359  0.021817  0.088433   \n",
              "\n",
              "        217       218       219       220       221       222       223  \\\n",
              "0  0.043300 -0.135919 -0.003034 -0.030036  0.065979  0.013876 -0.017308   \n",
              "1  0.030736 -0.087338  0.007633 -0.047783  0.033778 -0.028245  0.038771   \n",
              "2  0.088169 -0.063761 -0.025002  0.001268 -0.074894  0.034695  0.035434   \n",
              "\n",
              "        224       225       226       227       228       229       230  \\\n",
              "0  0.077567 -0.013354  0.001551  0.046534 -0.043215  0.047590 -0.033900   \n",
              "1  0.008593 -0.007277  0.022737  0.042860  0.026604 -0.034759  0.009034   \n",
              "2  0.083103 -0.079054  0.005493  0.086297  0.002045 -0.011959  0.053433   \n",
              "\n",
              "        231       232       233       234       235       236       237  \\\n",
              "0 -0.018250  0.006884 -0.033901  0.062081  0.028909 -0.053926 -0.154968   \n",
              "1 -0.037268  0.010572 -0.016896  0.042140  0.007601 -0.008338 -0.056065   \n",
              "2 -0.061761  0.021627  0.016259  0.094652 -0.042352 -0.056396 -0.068651   \n",
              "\n",
              "        238       239       240       241       242       243       244  \\\n",
              "0  0.101880  0.072985  0.126535 -0.017020 -0.046378 -0.105586 -0.020673   \n",
              "1  0.034869  0.014307 -0.009829 -0.039078  0.009282 -0.096402  0.043873   \n",
              "2  0.053593  0.038059 -0.001107 -0.004272 -0.027066 -0.054498  0.010640   \n",
              "\n",
              "        245       246       247       248       249       250       251  \\\n",
              "0  0.043405  0.009373  0.068233 -0.097857 -0.207807 -0.008571  0.104527   \n",
              "1 -0.016066 -0.041573  0.060135 -0.001188 -0.094655 -0.011820 -0.003895   \n",
              "2  0.057631  0.004868  0.038642  0.015625 -0.102471 -0.035177  0.098728   \n",
              "\n",
              "        252       253       254       255       256       257       258  \\\n",
              "0  0.096941  0.115147 -0.008942 -0.110573 -0.040229 -0.051553  0.031355   \n",
              "1  0.079302  0.086066  0.034077 -0.044135 -0.016489 -0.024242 -0.079945   \n",
              "2  0.009087  0.038574  0.080838 -0.014540 -0.026733  0.013217 -0.112739   \n",
              "\n",
              "        259       260       261       262       263       264       265  \\\n",
              "0 -0.105504  0.010230 -0.020512 -0.078979  0.013674 -0.064750  0.162955   \n",
              "1 -0.015627  0.008575 -0.000057 -0.038578  0.003882  0.092860  0.070185   \n",
              "2 -0.050128 -0.004123 -0.004612 -0.067491  0.011224 -0.000841  0.143175   \n",
              "\n",
              "        266       267       268       269       270       271       272  \\\n",
              "0 -0.077624 -0.150068 -0.085999 -0.046899 -0.023124  0.054796  0.170694   \n",
              "1  0.008929  0.003402 -0.067874 -0.014913 -0.010251  0.092048  0.048665   \n",
              "2  0.010417 -0.015523 -0.070543 -0.007595  0.055779  0.053182  0.076362   \n",
              "\n",
              "        273       274       275       276       277       278       279  \\\n",
              "0  0.074289  0.063477 -0.080540 -0.018136 -0.030698  0.003611  0.047468   \n",
              "1  0.004263  0.082684 -0.070974 -0.085914 -0.121464 -0.108557  0.056362   \n",
              "2  0.029573  0.070814  0.003160 -0.110260 -0.047377 -0.164252  0.011909   \n",
              "\n",
              "        280       281       282       283       284       285       286  \\\n",
              "0  0.082973 -0.017027  0.099034  0.100255  0.036614 -0.064976 -0.057118   \n",
              "1  0.003471 -0.004600  0.027936  0.081766 -0.001350  0.025344 -0.049986   \n",
              "2 -0.011544 -0.013593  0.032905  0.106418  0.099162  0.093560 -0.094699   \n",
              "\n",
              "        287       288       289       290       291       292       293  \\\n",
              "0 -0.088784 -0.036063  0.114485 -0.024549  0.183432 -0.109658  0.030273   \n",
              "1 -0.025076  0.005720  0.006427 -0.040072  0.103039 -0.082554  0.022182   \n",
              "2  0.010837  0.008592  0.138273 -0.102301  0.092102 -0.125210 -0.039361   \n",
              "\n",
              "        294       295       296       297       298       299        t0  \\\n",
              "0 -0.043125 -0.049957 -0.017133 -0.090628 -0.043672  0.064272  0.023642   \n",
              "1 -0.003769  0.015608 -0.013759 -0.031591 -0.034226 -0.004203  0.072074   \n",
              "2 -0.045234 -0.004466 -0.005018 -0.056888 -0.016968 -0.004015  0.049540   \n",
              "\n",
              "         t1        t2        t3        t4        t5        t6        t7  \\\n",
              "0 -0.004700  0.041460  0.105759  0.027174 -0.027481  0.002319 -0.184525   \n",
              "1  0.040558 -0.059041  0.076077 -0.085263  0.053602  0.013017 -0.031647   \n",
              "2  0.030131  0.048677  0.112376 -0.118184  0.075155 -0.032440 -0.157471   \n",
              "\n",
              "         t8        t9       t10       t11       t12       t13       t14  \\\n",
              "0  0.031586  0.075348 -0.089478 -0.097244 -0.005665 -0.010010 -0.093909   \n",
              "1  0.110952  0.122038  0.018046 -0.103100 -0.042667 -0.065543 -0.096259   \n",
              "2  0.084554  0.061971 -0.008842 -0.102376 -0.016032  0.018026 -0.093058   \n",
              "\n",
              "        t15       t16       t17       t18       t19       t20       t21  \\\n",
              "0  0.007469 -0.008614  0.030029  0.015177 -0.044548 -0.016830  0.114384   \n",
              "1  0.084649 -0.019592  0.133745 -0.010486 -0.140469  0.026974  0.075194   \n",
              "2  0.114014  0.000977  0.097900 -0.077749 -0.202301  0.066462  0.010315   \n",
              "\n",
              "        t22       t23       t24       t25       t26       t27       t28  \\\n",
              "0 -0.080585 -0.016862  0.037582 -0.053860 -0.069687  0.083313 -0.042421   \n",
              "1 -0.016741  0.027879 -0.047309 -0.034644 -0.145486  0.197649  0.067005   \n",
              "2  0.037567 -0.062113 -0.016734  0.035411 -0.215027  0.166280  0.173828   \n",
              "\n",
              "        t29       t30       t31       t32       t33       t34       t35  \\\n",
              "0  0.002563 -0.013336 -0.007278 -0.004440 -0.074299 -0.005066 -0.020451   \n",
              "1 -0.110731 -0.054667 -0.026079 -0.095256  0.090323 -0.092984 -0.095876   \n",
              "2 -0.040609  0.025513 -0.039469 -0.029968 -0.024277 -0.201660 -0.081136   \n",
              "\n",
              "        t36       t37       t38       t39       t40       t41       t42  \\\n",
              "0 -0.059204 -0.062714  0.009560  0.078644  0.026581 -0.133003  0.090698   \n",
              "1  0.015055  0.088971 -0.027774  0.020678  0.092035  0.041648  0.160799   \n",
              "2  0.205729  0.055949 -0.103602  0.048035  0.013550  0.039266  0.135132   \n",
              "\n",
              "        t43       t44       t45       t46       t47       t48       t49  \\\n",
              "0 -0.045013  0.053688  0.012964 -0.024307 -0.016724 -0.083771  0.045853   \n",
              "1  0.062739 -0.045525 -0.020688 -0.037571 -0.005925  0.103875 -0.088866   \n",
              "2 -0.061584 -0.073730 -0.134725 -0.194756 -0.100952 -0.008748 -0.010335   \n",
              "\n",
              "        t50       t51       t52       t53       t54       t55       t56  \\\n",
              "0  0.017776  0.040825  0.028770 -0.047867  0.018163  0.010151 -0.028946   \n",
              "1 -0.019413 -0.054698  0.002770 -0.009474 -0.008789  0.008799 -0.117137   \n",
              "2 -0.079671  0.058400  0.106242 -0.007507  0.139437 -0.105672  0.088623   \n",
              "\n",
              "        t57       t58       t59       t60       t61       t62       t63  \\\n",
              "0 -0.023495 -0.023163 -0.028366 -0.114807  0.051544 -0.007233 -0.071938   \n",
              "1 -0.147083 -0.035261  0.004141 -0.160217  0.046421 -0.166667 -0.012882   \n",
              "2 -0.014404 -0.024495  0.004211 -0.132365  0.033783 -0.049057 -0.082535   \n",
              "\n",
              "        t64       t65       t66       t67       t68       t69       t70  \\\n",
              "0  0.088280  0.049053 -0.040497  0.064194  0.067669  0.138885  0.060989   \n",
              "1 -0.030312 -0.048543 -0.066866  0.066101 -0.088707  0.146139  0.064121   \n",
              "2 -0.037354  0.100342 -0.074188 -0.025838  0.146139  0.140951  0.188639   \n",
              "\n",
              "        t71       t72       t73       t74       t75       t76       t77  \\\n",
              "0  0.051712  0.103764  0.040390 -0.122742 -0.037155 -0.001972  0.063379   \n",
              "1  0.055908  0.029626  0.014513 -0.140981 -0.078273  0.176829  0.071353   \n",
              "2  0.015462  0.010213 -0.049723 -0.151118 -0.121053  0.079712 -0.009766   \n",
              "\n",
              "        t78       t79       t80       t81       t82       t83       t84  \\\n",
              "0  0.097115  0.082230  0.079483 -0.069189  0.039116 -0.018723 -0.022295   \n",
              "1  0.033986 -0.027561  0.065012  0.030216  0.013129 -0.042867  0.037198   \n",
              "2 -0.039144  0.002360  0.112203 -0.028544 -0.174967  0.044576 -0.036621   \n",
              "\n",
              "        t85       t86       t87       t88       t89       t90       t91  \\\n",
              "0 -0.085098 -0.025173  0.111206  0.005409  0.042976 -0.012733  0.034204   \n",
              "1  0.072281 -0.031041  0.153687  0.034637 -0.035149 -0.017297 -0.057248   \n",
              "2  0.075928  0.050334  0.225586  0.010173 -0.019674 -0.036377 -0.045858   \n",
              "\n",
              "        t92       t93       t94       t95       t96       t97       t98  \\\n",
              "0 -0.066124 -0.106903 -0.101349 -0.002483  0.054401 -0.014191  0.063248   \n",
              "1 -0.065253  0.033555  0.001017  0.006477  0.004241 -0.019572  0.163083   \n",
              "2 -0.078003 -0.144470  0.002563 -0.043396 -0.047526 -0.072795  0.128764   \n",
              "\n",
              "        t99      t100      t101      t102      t103      t104      t105  \\\n",
              "0 -0.031708 -0.049109 -0.057486  0.060177 -0.006256 -0.023739 -0.055176   \n",
              "1  0.047638  0.028456  0.036516  0.090596 -0.024985  0.015789  0.031962   \n",
              "2  0.018717 -0.142253 -0.053650  0.069539 -0.037516 -0.055806 -0.016073   \n",
              "\n",
              "       t106      t107      t108      t109      t110      t111      t112  \\\n",
              "0 -0.048543  0.019409 -0.000183 -0.060661  0.004597  0.035736  0.012192   \n",
              "1  0.009528  0.027685  0.141507 -0.084938 -0.076904 -0.036393 -0.051358   \n",
              "2  0.165812  0.059326 -0.067932 -0.014455 -0.161458 -0.130625  0.146383   \n",
              "\n",
              "       t113      t114      t115      t116      t117      t118      t119  \\\n",
              "0 -0.032845  0.085388 -0.046345 -0.019450 -0.035706  0.016754  0.057755   \n",
              "1 -0.027690  0.072727  0.063784 -0.131812 -0.100488  0.061903 -0.030070   \n",
              "2 -0.041812  0.042140  0.005208  0.030622 -0.086589 -0.014771  0.020426   \n",
              "\n",
              "       t120      t121      t122      t123      t124      t125      t126  \\\n",
              "0 -0.098450 -0.018036 -0.022797  0.022186  0.032612  0.025421 -0.035217   \n",
              "1  0.018216 -0.053216 -0.031942  0.001055  0.056776 -0.113120 -0.071985   \n",
              "2  0.109375 -0.023234 -0.054057  0.033997  0.064173  0.056193 -0.034139   \n",
              "\n",
              "       t127      t128      t129      t130      t131      t132      t133  \\\n",
              "0 -0.030624  0.108231  0.066277 -0.103188 -0.026997 -0.147491  0.000961   \n",
              "1 -0.015476  0.114777  0.080234 -0.001495  0.047900 -0.004254 -0.002325   \n",
              "2  0.051158 -0.026057  0.071899 -0.019063 -0.080526 -0.101440  0.091370   \n",
              "\n",
              "       t134      t135      t136      t137      t138      t139      t140  \\\n",
              "0 -0.031845 -0.033691  0.071167  0.050507 -0.081879  0.144764  0.058868   \n",
              "1  0.007401  0.047967  0.049201  0.001354 -0.045737 -0.022102  0.169608   \n",
              "2 -0.068603  0.192301  0.017049 -0.013547  0.117269  0.069351  0.060455   \n",
              "\n",
              "       t141      t142      t143      t144      t145      t146      t147  \\\n",
              "0 -0.106548  0.035995 -0.035400 -0.017593 -0.016199  0.002197 -0.060684   \n",
              "1 -0.049750 -0.123857  0.026048  0.003852 -0.061346  0.066515  0.001324   \n",
              "2 -0.226278  0.011121 -0.089945 -0.049449 -0.045812  0.001841 -0.010661   \n",
              "\n",
              "       t148      t149      t150      t151      t152      t153      t154  \\\n",
              "0 -0.082272 -0.071381  0.042133  0.035461 -0.060865  0.055679 -0.009415   \n",
              "1 -0.094447 -0.071023  0.109818 -0.101891 -0.065055  0.018982 -0.087540   \n",
              "2  0.032572 -0.044271  0.104187 -0.042969  0.023397  0.051015 -0.037394   \n",
              "\n",
              "       t155      t156      t157      t158      t159      t160      t161  \\\n",
              "0 -0.028581 -0.104553 -0.082756 -0.066826 -0.052322 -0.017059 -0.005196   \n",
              "1 -0.075412 -0.080638  0.016439 -0.062612  0.029961 -0.036604 -0.047197   \n",
              "2 -0.049784 -0.174072 -0.022603 -0.075033 -0.042643  0.083221  0.051804   \n",
              "\n",
              "       t162      t163      t164      t165      t166      t167      t168  \\\n",
              "0  0.032303  0.047432  0.025467 -0.045532  0.003082 -0.071457  0.083565   \n",
              "1  0.011225 -0.037971 -0.029541 -0.013615  0.025279  0.004001 -0.163066   \n",
              "2  0.191488  0.083567 -0.050090 -0.137573  0.139303  0.016479 -0.057536   \n",
              "\n",
              "       t169      t170      t171      t172      t173      t174      t175  \\\n",
              "0 -0.048666 -0.035172 -0.048920 -0.047852 -0.078445 -0.051291 -0.057014   \n",
              "1 -0.010736 -0.177494 -0.076650  0.048767 -0.049095 -0.071594  0.028727   \n",
              "2 -0.018000 -0.081299 -0.048096 -0.064535  0.148913 -0.156893  0.019011   \n",
              "\n",
              "       t176      t177      t178      t179      t180      t181      t182  \\\n",
              "0  0.113098 -0.071762 -0.040543  0.025101 -0.082382 -0.020996 -0.039651   \n",
              "1  0.077076 -0.230523 -0.016568 -0.063239 -0.107525 -0.075107 -0.042996   \n",
              "2  0.171056 -0.147888 -0.071533 -0.052816 -0.074910 -0.126526  0.032046   \n",
              "\n",
              "       t183      t184      t185      t186      t187      t188      t189  \\\n",
              "0  0.081390 -0.031908 -0.030106  0.076168  0.021408  0.110954  0.008377   \n",
              "1 -0.085531  0.019552 -0.006953 -0.095984 -0.011824  0.011069  0.071967   \n",
              "2 -0.020020  0.049642 -0.102132 -0.006785  0.110982 -0.052734  0.140055   \n",
              "\n",
              "       t190      t191      t192      t193      t194      t195      t196  \\\n",
              "0 -0.019379 -0.041382  0.105824  0.037804 -0.063751  0.114182 -0.053772   \n",
              "1 -0.028858  0.059741 -0.050117 -0.049461 -0.019352  0.101257  0.059842   \n",
              "2 -0.071777 -0.022461 -0.106486  0.139598  0.009562  0.083903 -0.136271   \n",
              "\n",
              "       t197      t198      t199      t200      t201      t202      t203  \\\n",
              "0  0.013447 -0.096786 -0.102066  0.046633  0.029480 -0.029068 -0.016441   \n",
              "1 -0.037360  0.009777 -0.011286  0.039090  0.058506  0.012838  0.042984   \n",
              "2 -0.045227 -0.084066 -0.185547 -0.071838 -0.076701 -0.132988 -0.138468   \n",
              "\n",
              "       t204      t205      t206      t207      t208      t209      t210  \\\n",
              "0  0.052124 -0.068390  0.010468  0.065750  0.053902 -0.043030 -0.076973   \n",
              "1  0.124607 -0.092646 -0.010668 -0.075703 -0.053804 -0.044622 -0.051129   \n",
              "2  0.033694 -0.171956  0.023407  0.004903 -0.031570  0.109174 -0.023600   \n",
              "\n",
              "       t211      t212      t213      t214      t215      t216      t217  \\\n",
              "0  0.046310 -0.089283  0.053295 -0.090179 -0.024185  0.044952 -0.026138   \n",
              "1  0.099270 -0.100833  0.008849 -0.119866  0.032648  0.006958  0.025728   \n",
              "2  0.150757 -0.019979 -0.019511 -0.033244  0.002116 -0.046936 -0.125366   \n",
              "\n",
              "       t218      t219      t220      t221      t222      t223      t224  \\\n",
              "0 -0.108727 -0.003693 -0.024208  0.080994  0.096851  0.065338  0.093872   \n",
              "1 -0.105520 -0.012360 -0.081112  0.010891  0.072445  0.064259 -0.055200   \n",
              "2 -0.165771 -0.057454 -0.132843  0.138407  0.047607  0.002848  0.055717   \n",
              "\n",
              "       t225      t226      t227      t228      t229      t230      t231  \\\n",
              "0 -0.088501  0.035175 -0.006592 -0.051520  0.016688  0.022655 -0.059052   \n",
              "1  0.027584  0.107807  0.076508  0.054804 -0.033447 -0.047251 -0.046861   \n",
              "2  0.056071  0.067505 -0.039805  0.005188  0.049276  0.130452  0.001973   \n",
              "\n",
              "       t232      t233      t234      t235      t236      t237      t238  \\\n",
              "0  0.034027  0.024185 -0.039603 -0.028549  0.036865 -0.064026  0.053818   \n",
              "1  0.018314 -0.022186 -0.020949  0.035604  0.023521 -0.060888  0.028300   \n",
              "2 -0.141464 -0.020528 -0.000895 -0.056966 -0.127869 -0.053975  0.219564   \n",
              "\n",
              "       t239      t240      t241      t242      t243      t244      t245  \\\n",
              "0 -0.002987  0.043949 -0.053986 -0.013428 -0.127075 -0.058418 -0.011642   \n",
              "1  0.006326  0.058729 -0.039130  0.021708  0.004483 -0.014041  0.017256   \n",
              "2 -0.056320  0.211263  0.123708  0.099202 -0.116496 -0.060476 -0.119222   \n",
              "\n",
              "       t246      t247      t248      t249      t250      t251      t252  \\\n",
              "0  0.005737  0.029709 -0.010563  0.009186  0.021261 -0.050323  0.066284   \n",
              "1 -0.089837  0.026177 -0.044766 -0.032808  0.052333 -0.058609  0.094645   \n",
              "2 -0.142456 -0.029867 -0.000295 -0.021240  0.033529  0.131348  0.122355   \n",
              "\n",
              "       t253      t254      t255      t256      t257      t258      t259  \\\n",
              "0  0.018356 -0.016045 -0.017506  0.010254  0.008957 -0.067612 -0.063530   \n",
              "1  0.022017  0.021366 -0.032606 -0.030555 -0.044713  0.046777 -0.020657   \n",
              "2  0.102196  0.107971 -0.078288  0.032735  0.059082 -0.039642 -0.199178   \n",
              "\n",
              "       t260      t261      t262      t263      t264      t265      t266  \\\n",
              "0  0.041290  0.013977 -0.097321  0.055268  0.057220  0.088966 -0.017834   \n",
              "1  0.058076  0.058478 -0.030192 -0.068566  0.090522  0.053301 -0.039795   \n",
              "2  0.017822  0.014206 -0.036418 -0.086543  0.168376 -0.014648 -0.094889   \n",
              "\n",
              "       t267      t268      t269      t270      t271      t272      t273  \\\n",
              "0 -0.037994 -0.142250  0.029236  0.000065  0.058197  0.107529  0.047714   \n",
              "1 -0.099763 -0.081404  0.045653 -0.044968  0.005537 -0.017688 -0.008752   \n",
              "2 -0.043864 -0.084798 -0.144979 -0.146200  0.097005 -0.057414  0.021444   \n",
              "\n",
              "       t274      t275      t276      t277      t278      t279      t280  \\\n",
              "0  0.005437 -0.102744 -0.076538 -0.098022 -0.053116 -0.079941  0.051016   \n",
              "1  0.072693 -0.001940 -0.044600 -0.168209 -0.051210  0.148349 -0.029427   \n",
              "2  0.165527 -0.053385 -0.003255 -0.083171 -0.060465  0.075195 -0.100586   \n",
              "\n",
              "       t281      t282      t283      t284      t285      t286      t287  \\\n",
              "0  0.034670  0.034798  0.155212  0.014664 -0.036133 -0.025864 -0.029800   \n",
              "1  0.040434  0.040182  0.008592 -0.077905  0.043165 -0.035611 -0.004595   \n",
              "2  0.208944  0.154622 -0.034342  0.002035 -0.022583 -0.077738  0.023600   \n",
              "\n",
              "       t288      t289      t290      t291      t292      t293      t294  \\\n",
              "0 -0.032715  0.091858 -0.078674  0.096649 -0.100101 -0.024906 -0.036613   \n",
              "1  0.073013 -0.015964 -0.062968  0.025066 -0.042209  0.080261  0.040852   \n",
              "2  0.071777 -0.048442  0.010722 -0.040629 -0.101440  0.057841  0.016947   \n",
              "\n",
              "       t295      t296      t297      t298      t299  \n",
              "0 -0.032570 -0.121918 -0.018051  0.046509 -0.085266  \n",
              "1 -0.025947 -0.033207 -0.043254 -0.045268  0.000799  \n",
              "2 -0.161906 -0.097087 -0.121745 -0.038757  0.076462  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "      <th>128</th>\n",
              "      <th>129</th>\n",
              "      <th>130</th>\n",
              "      <th>131</th>\n",
              "      <th>132</th>\n",
              "      <th>133</th>\n",
              "      <th>134</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "      <th>141</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>144</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "      <th>151</th>\n",
              "      <th>152</th>\n",
              "      <th>153</th>\n",
              "      <th>154</th>\n",
              "      <th>155</th>\n",
              "      <th>156</th>\n",
              "      <th>157</th>\n",
              "      <th>158</th>\n",
              "      <th>159</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "      <th>200</th>\n",
              "      <th>201</th>\n",
              "      <th>202</th>\n",
              "      <th>203</th>\n",
              "      <th>204</th>\n",
              "      <th>205</th>\n",
              "      <th>206</th>\n",
              "      <th>207</th>\n",
              "      <th>208</th>\n",
              "      <th>209</th>\n",
              "      <th>210</th>\n",
              "      <th>211</th>\n",
              "      <th>212</th>\n",
              "      <th>213</th>\n",
              "      <th>214</th>\n",
              "      <th>215</th>\n",
              "      <th>216</th>\n",
              "      <th>217</th>\n",
              "      <th>218</th>\n",
              "      <th>219</th>\n",
              "      <th>220</th>\n",
              "      <th>221</th>\n",
              "      <th>222</th>\n",
              "      <th>223</th>\n",
              "      <th>224</th>\n",
              "      <th>225</th>\n",
              "      <th>226</th>\n",
              "      <th>227</th>\n",
              "      <th>228</th>\n",
              "      <th>229</th>\n",
              "      <th>230</th>\n",
              "      <th>231</th>\n",
              "      <th>232</th>\n",
              "      <th>233</th>\n",
              "      <th>234</th>\n",
              "      <th>235</th>\n",
              "      <th>236</th>\n",
              "      <th>237</th>\n",
              "      <th>238</th>\n",
              "      <th>239</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "      <th>244</th>\n",
              "      <th>245</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "      <th>259</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "      <th>t0</th>\n",
              "      <th>t1</th>\n",
              "      <th>t2</th>\n",
              "      <th>t3</th>\n",
              "      <th>t4</th>\n",
              "      <th>t5</th>\n",
              "      <th>t6</th>\n",
              "      <th>t7</th>\n",
              "      <th>t8</th>\n",
              "      <th>t9</th>\n",
              "      <th>t10</th>\n",
              "      <th>t11</th>\n",
              "      <th>t12</th>\n",
              "      <th>t13</th>\n",
              "      <th>t14</th>\n",
              "      <th>t15</th>\n",
              "      <th>t16</th>\n",
              "      <th>t17</th>\n",
              "      <th>t18</th>\n",
              "      <th>t19</th>\n",
              "      <th>t20</th>\n",
              "      <th>t21</th>\n",
              "      <th>t22</th>\n",
              "      <th>t23</th>\n",
              "      <th>t24</th>\n",
              "      <th>t25</th>\n",
              "      <th>t26</th>\n",
              "      <th>t27</th>\n",
              "      <th>t28</th>\n",
              "      <th>t29</th>\n",
              "      <th>t30</th>\n",
              "      <th>t31</th>\n",
              "      <th>t32</th>\n",
              "      <th>t33</th>\n",
              "      <th>t34</th>\n",
              "      <th>t35</th>\n",
              "      <th>t36</th>\n",
              "      <th>t37</th>\n",
              "      <th>t38</th>\n",
              "      <th>t39</th>\n",
              "      <th>t40</th>\n",
              "      <th>t41</th>\n",
              "      <th>t42</th>\n",
              "      <th>t43</th>\n",
              "      <th>t44</th>\n",
              "      <th>t45</th>\n",
              "      <th>t46</th>\n",
              "      <th>t47</th>\n",
              "      <th>t48</th>\n",
              "      <th>t49</th>\n",
              "      <th>t50</th>\n",
              "      <th>t51</th>\n",
              "      <th>t52</th>\n",
              "      <th>t53</th>\n",
              "      <th>t54</th>\n",
              "      <th>t55</th>\n",
              "      <th>t56</th>\n",
              "      <th>t57</th>\n",
              "      <th>t58</th>\n",
              "      <th>t59</th>\n",
              "      <th>t60</th>\n",
              "      <th>t61</th>\n",
              "      <th>t62</th>\n",
              "      <th>t63</th>\n",
              "      <th>t64</th>\n",
              "      <th>t65</th>\n",
              "      <th>t66</th>\n",
              "      <th>t67</th>\n",
              "      <th>t68</th>\n",
              "      <th>t69</th>\n",
              "      <th>t70</th>\n",
              "      <th>t71</th>\n",
              "      <th>t72</th>\n",
              "      <th>t73</th>\n",
              "      <th>t74</th>\n",
              "      <th>t75</th>\n",
              "      <th>t76</th>\n",
              "      <th>t77</th>\n",
              "      <th>t78</th>\n",
              "      <th>t79</th>\n",
              "      <th>t80</th>\n",
              "      <th>t81</th>\n",
              "      <th>t82</th>\n",
              "      <th>t83</th>\n",
              "      <th>t84</th>\n",
              "      <th>t85</th>\n",
              "      <th>t86</th>\n",
              "      <th>t87</th>\n",
              "      <th>t88</th>\n",
              "      <th>t89</th>\n",
              "      <th>t90</th>\n",
              "      <th>t91</th>\n",
              "      <th>t92</th>\n",
              "      <th>t93</th>\n",
              "      <th>t94</th>\n",
              "      <th>t95</th>\n",
              "      <th>t96</th>\n",
              "      <th>t97</th>\n",
              "      <th>t98</th>\n",
              "      <th>t99</th>\n",
              "      <th>t100</th>\n",
              "      <th>t101</th>\n",
              "      <th>t102</th>\n",
              "      <th>t103</th>\n",
              "      <th>t104</th>\n",
              "      <th>t105</th>\n",
              "      <th>t106</th>\n",
              "      <th>t107</th>\n",
              "      <th>t108</th>\n",
              "      <th>t109</th>\n",
              "      <th>t110</th>\n",
              "      <th>t111</th>\n",
              "      <th>t112</th>\n",
              "      <th>t113</th>\n",
              "      <th>t114</th>\n",
              "      <th>t115</th>\n",
              "      <th>t116</th>\n",
              "      <th>t117</th>\n",
              "      <th>t118</th>\n",
              "      <th>t119</th>\n",
              "      <th>t120</th>\n",
              "      <th>t121</th>\n",
              "      <th>t122</th>\n",
              "      <th>t123</th>\n",
              "      <th>t124</th>\n",
              "      <th>t125</th>\n",
              "      <th>t126</th>\n",
              "      <th>t127</th>\n",
              "      <th>t128</th>\n",
              "      <th>t129</th>\n",
              "      <th>t130</th>\n",
              "      <th>t131</th>\n",
              "      <th>t132</th>\n",
              "      <th>t133</th>\n",
              "      <th>t134</th>\n",
              "      <th>t135</th>\n",
              "      <th>t136</th>\n",
              "      <th>t137</th>\n",
              "      <th>t138</th>\n",
              "      <th>t139</th>\n",
              "      <th>t140</th>\n",
              "      <th>t141</th>\n",
              "      <th>t142</th>\n",
              "      <th>t143</th>\n",
              "      <th>t144</th>\n",
              "      <th>t145</th>\n",
              "      <th>t146</th>\n",
              "      <th>t147</th>\n",
              "      <th>t148</th>\n",
              "      <th>t149</th>\n",
              "      <th>t150</th>\n",
              "      <th>t151</th>\n",
              "      <th>t152</th>\n",
              "      <th>t153</th>\n",
              "      <th>t154</th>\n",
              "      <th>t155</th>\n",
              "      <th>t156</th>\n",
              "      <th>t157</th>\n",
              "      <th>t158</th>\n",
              "      <th>t159</th>\n",
              "      <th>t160</th>\n",
              "      <th>t161</th>\n",
              "      <th>t162</th>\n",
              "      <th>t163</th>\n",
              "      <th>t164</th>\n",
              "      <th>t165</th>\n",
              "      <th>t166</th>\n",
              "      <th>t167</th>\n",
              "      <th>t168</th>\n",
              "      <th>t169</th>\n",
              "      <th>t170</th>\n",
              "      <th>t171</th>\n",
              "      <th>t172</th>\n",
              "      <th>t173</th>\n",
              "      <th>t174</th>\n",
              "      <th>t175</th>\n",
              "      <th>t176</th>\n",
              "      <th>t177</th>\n",
              "      <th>t178</th>\n",
              "      <th>t179</th>\n",
              "      <th>t180</th>\n",
              "      <th>t181</th>\n",
              "      <th>t182</th>\n",
              "      <th>t183</th>\n",
              "      <th>t184</th>\n",
              "      <th>t185</th>\n",
              "      <th>t186</th>\n",
              "      <th>t187</th>\n",
              "      <th>t188</th>\n",
              "      <th>t189</th>\n",
              "      <th>t190</th>\n",
              "      <th>t191</th>\n",
              "      <th>t192</th>\n",
              "      <th>t193</th>\n",
              "      <th>t194</th>\n",
              "      <th>t195</th>\n",
              "      <th>t196</th>\n",
              "      <th>t197</th>\n",
              "      <th>t198</th>\n",
              "      <th>t199</th>\n",
              "      <th>t200</th>\n",
              "      <th>t201</th>\n",
              "      <th>t202</th>\n",
              "      <th>t203</th>\n",
              "      <th>t204</th>\n",
              "      <th>t205</th>\n",
              "      <th>t206</th>\n",
              "      <th>t207</th>\n",
              "      <th>t208</th>\n",
              "      <th>t209</th>\n",
              "      <th>t210</th>\n",
              "      <th>t211</th>\n",
              "      <th>t212</th>\n",
              "      <th>t213</th>\n",
              "      <th>t214</th>\n",
              "      <th>t215</th>\n",
              "      <th>t216</th>\n",
              "      <th>t217</th>\n",
              "      <th>t218</th>\n",
              "      <th>t219</th>\n",
              "      <th>t220</th>\n",
              "      <th>t221</th>\n",
              "      <th>t222</th>\n",
              "      <th>t223</th>\n",
              "      <th>t224</th>\n",
              "      <th>t225</th>\n",
              "      <th>t226</th>\n",
              "      <th>t227</th>\n",
              "      <th>t228</th>\n",
              "      <th>t229</th>\n",
              "      <th>t230</th>\n",
              "      <th>t231</th>\n",
              "      <th>t232</th>\n",
              "      <th>t233</th>\n",
              "      <th>t234</th>\n",
              "      <th>t235</th>\n",
              "      <th>t236</th>\n",
              "      <th>t237</th>\n",
              "      <th>t238</th>\n",
              "      <th>t239</th>\n",
              "      <th>t240</th>\n",
              "      <th>t241</th>\n",
              "      <th>t242</th>\n",
              "      <th>t243</th>\n",
              "      <th>t244</th>\n",
              "      <th>t245</th>\n",
              "      <th>t246</th>\n",
              "      <th>t247</th>\n",
              "      <th>t248</th>\n",
              "      <th>t249</th>\n",
              "      <th>t250</th>\n",
              "      <th>t251</th>\n",
              "      <th>t252</th>\n",
              "      <th>t253</th>\n",
              "      <th>t254</th>\n",
              "      <th>t255</th>\n",
              "      <th>t256</th>\n",
              "      <th>t257</th>\n",
              "      <th>t258</th>\n",
              "      <th>t259</th>\n",
              "      <th>t260</th>\n",
              "      <th>t261</th>\n",
              "      <th>t262</th>\n",
              "      <th>t263</th>\n",
              "      <th>t264</th>\n",
              "      <th>t265</th>\n",
              "      <th>t266</th>\n",
              "      <th>t267</th>\n",
              "      <th>t268</th>\n",
              "      <th>t269</th>\n",
              "      <th>t270</th>\n",
              "      <th>t271</th>\n",
              "      <th>t272</th>\n",
              "      <th>t273</th>\n",
              "      <th>t274</th>\n",
              "      <th>t275</th>\n",
              "      <th>t276</th>\n",
              "      <th>t277</th>\n",
              "      <th>t278</th>\n",
              "      <th>t279</th>\n",
              "      <th>t280</th>\n",
              "      <th>t281</th>\n",
              "      <th>t282</th>\n",
              "      <th>t283</th>\n",
              "      <th>t284</th>\n",
              "      <th>t285</th>\n",
              "      <th>t286</th>\n",
              "      <th>t287</th>\n",
              "      <th>t288</th>\n",
              "      <th>t289</th>\n",
              "      <th>t290</th>\n",
              "      <th>t291</th>\n",
              "      <th>t292</th>\n",
              "      <th>t293</th>\n",
              "      <th>t294</th>\n",
              "      <th>t295</th>\n",
              "      <th>t296</th>\n",
              "      <th>t297</th>\n",
              "      <th>t298</th>\n",
              "      <th>t299</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.029349</td>\n",
              "      <td>0.039355</td>\n",
              "      <td>-0.043146</td>\n",
              "      <td>0.137334</td>\n",
              "      <td>-0.050119</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.037109</td>\n",
              "      <td>-0.058620</td>\n",
              "      <td>0.036689</td>\n",
              "      <td>0.074262</td>\n",
              "      <td>-0.052490</td>\n",
              "      <td>-0.120954</td>\n",
              "      <td>-0.065456</td>\n",
              "      <td>-0.007420</td>\n",
              "      <td>-0.107020</td>\n",
              "      <td>0.025611</td>\n",
              "      <td>0.082581</td>\n",
              "      <td>0.117641</td>\n",
              "      <td>-0.080366</td>\n",
              "      <td>-0.057848</td>\n",
              "      <td>0.007472</td>\n",
              "      <td>0.040187</td>\n",
              "      <td>0.015901</td>\n",
              "      <td>0.026445</td>\n",
              "      <td>0.038720</td>\n",
              "      <td>0.048357</td>\n",
              "      <td>-0.055760</td>\n",
              "      <td>0.038609</td>\n",
              "      <td>0.034306</td>\n",
              "      <td>-0.152562</td>\n",
              "      <td>-0.105931</td>\n",
              "      <td>0.095141</td>\n",
              "      <td>-0.011897</td>\n",
              "      <td>-0.013230</td>\n",
              "      <td>-0.065061</td>\n",
              "      <td>-0.130545</td>\n",
              "      <td>0.016349</td>\n",
              "      <td>0.089068</td>\n",
              "      <td>0.017705</td>\n",
              "      <td>-0.019113</td>\n",
              "      <td>0.050886</td>\n",
              "      <td>-0.137695</td>\n",
              "      <td>0.223364</td>\n",
              "      <td>0.012050</td>\n",
              "      <td>0.026917</td>\n",
              "      <td>0.019598</td>\n",
              "      <td>-0.057053</td>\n",
              "      <td>0.055734</td>\n",
              "      <td>-0.056854</td>\n",
              "      <td>0.022518</td>\n",
              "      <td>-0.058526</td>\n",
              "      <td>0.042130</td>\n",
              "      <td>0.006001</td>\n",
              "      <td>0.010803</td>\n",
              "      <td>0.009626</td>\n",
              "      <td>-0.032889</td>\n",
              "      <td>-0.019396</td>\n",
              "      <td>-0.083147</td>\n",
              "      <td>0.046321</td>\n",
              "      <td>-0.046790</td>\n",
              "      <td>-0.085031</td>\n",
              "      <td>0.119106</td>\n",
              "      <td>-0.171291</td>\n",
              "      <td>-0.068813</td>\n",
              "      <td>0.117058</td>\n",
              "      <td>-0.02260</td>\n",
              "      <td>-0.038156</td>\n",
              "      <td>0.048811</td>\n",
              "      <td>-0.078201</td>\n",
              "      <td>0.080414</td>\n",
              "      <td>0.110713</td>\n",
              "      <td>0.029674</td>\n",
              "      <td>0.084643</td>\n",
              "      <td>0.007516</td>\n",
              "      <td>-0.212769</td>\n",
              "      <td>0.045794</td>\n",
              "      <td>0.056074</td>\n",
              "      <td>0.072498</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.045323</td>\n",
              "      <td>0.006836</td>\n",
              "      <td>0.010429</td>\n",
              "      <td>0.001552</td>\n",
              "      <td>0.008327</td>\n",
              "      <td>-0.044635</td>\n",
              "      <td>0.028966</td>\n",
              "      <td>0.043653</td>\n",
              "      <td>0.115702</td>\n",
              "      <td>0.058755</td>\n",
              "      <td>-0.003577</td>\n",
              "      <td>-0.022288</td>\n",
              "      <td>-0.006223</td>\n",
              "      <td>-0.064606</td>\n",
              "      <td>-0.084874</td>\n",
              "      <td>-0.045925</td>\n",
              "      <td>-0.010803</td>\n",
              "      <td>0.144930</td>\n",
              "      <td>0.027706</td>\n",
              "      <td>0.036451</td>\n",
              "      <td>-0.136647</td>\n",
              "      <td>-0.066066</td>\n",
              "      <td>0.026742</td>\n",
              "      <td>0.036355</td>\n",
              "      <td>-0.021649</td>\n",
              "      <td>-0.071716</td>\n",
              "      <td>-0.020543</td>\n",
              "      <td>0.048941</td>\n",
              "      <td>-0.049635</td>\n",
              "      <td>0.007207</td>\n",
              "      <td>-0.107270</td>\n",
              "      <td>-0.095982</td>\n",
              "      <td>-0.026106</td>\n",
              "      <td>0.036152</td>\n",
              "      <td>-0.081857</td>\n",
              "      <td>0.124246</td>\n",
              "      <td>-0.044870</td>\n",
              "      <td>0.009783</td>\n",
              "      <td>-0.002145</td>\n",
              "      <td>-0.028431</td>\n",
              "      <td>-0.000589</td>\n",
              "      <td>-0.108172</td>\n",
              "      <td>-0.059658</td>\n",
              "      <td>-0.093497</td>\n",
              "      <td>0.034520</td>\n",
              "      <td>0.025427</td>\n",
              "      <td>-0.050507</td>\n",
              "      <td>-0.073363</td>\n",
              "      <td>-0.030029</td>\n",
              "      <td>0.006771</td>\n",
              "      <td>0.006856</td>\n",
              "      <td>-0.106724</td>\n",
              "      <td>-0.012695</td>\n",
              "      <td>0.020935</td>\n",
              "      <td>0.178149</td>\n",
              "      <td>-0.012427</td>\n",
              "      <td>-0.010827</td>\n",
              "      <td>-0.006696</td>\n",
              "      <td>0.043917</td>\n",
              "      <td>0.002947</td>\n",
              "      <td>0.122637</td>\n",
              "      <td>0.098938</td>\n",
              "      <td>-0.079065</td>\n",
              "      <td>0.032924</td>\n",
              "      <td>-0.004040</td>\n",
              "      <td>-0.079830</td>\n",
              "      <td>0.036145</td>\n",
              "      <td>-0.063729</td>\n",
              "      <td>-0.036996</td>\n",
              "      <td>-0.019060</td>\n",
              "      <td>-0.004517</td>\n",
              "      <td>0.161743</td>\n",
              "      <td>0.030021</td>\n",
              "      <td>-0.092826</td>\n",
              "      <td>0.113676</td>\n",
              "      <td>-0.090441</td>\n",
              "      <td>-0.054308</td>\n",
              "      <td>-0.043638</td>\n",
              "      <td>-0.074184</td>\n",
              "      <td>-0.077776</td>\n",
              "      <td>-0.064273</td>\n",
              "      <td>-0.104289</td>\n",
              "      <td>0.091649</td>\n",
              "      <td>0.018354</td>\n",
              "      <td>0.064697</td>\n",
              "      <td>0.025271</td>\n",
              "      <td>-0.161298</td>\n",
              "      <td>-0.001225</td>\n",
              "      <td>-0.029922</td>\n",
              "      <td>-0.003283</td>\n",
              "      <td>-0.043265</td>\n",
              "      <td>-0.137870</td>\n",
              "      <td>0.009999</td>\n",
              "      <td>0.023056</td>\n",
              "      <td>0.051784</td>\n",
              "      <td>0.010817</td>\n",
              "      <td>0.058472</td>\n",
              "      <td>0.121730</td>\n",
              "      <td>-0.068704</td>\n",
              "      <td>-0.083221</td>\n",
              "      <td>0.079237</td>\n",
              "      <td>-0.071141</td>\n",
              "      <td>-0.091073</td>\n",
              "      <td>0.047538</td>\n",
              "      <td>0.063494</td>\n",
              "      <td>-0.018337</td>\n",
              "      <td>-0.025423</td>\n",
              "      <td>-0.020890</td>\n",
              "      <td>-0.072911</td>\n",
              "      <td>0.053816</td>\n",
              "      <td>-0.024026</td>\n",
              "      <td>0.019584</td>\n",
              "      <td>0.008074</td>\n",
              "      <td>0.011818</td>\n",
              "      <td>-0.053589</td>\n",
              "      <td>0.000462</td>\n",
              "      <td>0.176505</td>\n",
              "      <td>-0.008375</td>\n",
              "      <td>-0.136230</td>\n",
              "      <td>-0.138829</td>\n",
              "      <td>-0.090470</td>\n",
              "      <td>0.018563</td>\n",
              "      <td>0.064261</td>\n",
              "      <td>-0.087738</td>\n",
              "      <td>-0.071213</td>\n",
              "      <td>0.036503</td>\n",
              "      <td>-0.151509</td>\n",
              "      <td>-0.030448</td>\n",
              "      <td>-0.031996</td>\n",
              "      <td>-0.064571</td>\n",
              "      <td>0.040584</td>\n",
              "      <td>-0.081959</td>\n",
              "      <td>0.092773</td>\n",
              "      <td>-0.139744</td>\n",
              "      <td>0.042459</td>\n",
              "      <td>-0.032533</td>\n",
              "      <td>-0.027989</td>\n",
              "      <td>0.098371</td>\n",
              "      <td>0.043300</td>\n",
              "      <td>-0.135919</td>\n",
              "      <td>-0.003034</td>\n",
              "      <td>-0.030036</td>\n",
              "      <td>0.065979</td>\n",
              "      <td>0.013876</td>\n",
              "      <td>-0.017308</td>\n",
              "      <td>0.077567</td>\n",
              "      <td>-0.013354</td>\n",
              "      <td>0.001551</td>\n",
              "      <td>0.046534</td>\n",
              "      <td>-0.043215</td>\n",
              "      <td>0.047590</td>\n",
              "      <td>-0.033900</td>\n",
              "      <td>-0.018250</td>\n",
              "      <td>0.006884</td>\n",
              "      <td>-0.033901</td>\n",
              "      <td>0.062081</td>\n",
              "      <td>0.028909</td>\n",
              "      <td>-0.053926</td>\n",
              "      <td>-0.154968</td>\n",
              "      <td>0.101880</td>\n",
              "      <td>0.072985</td>\n",
              "      <td>0.126535</td>\n",
              "      <td>-0.017020</td>\n",
              "      <td>-0.046378</td>\n",
              "      <td>-0.105586</td>\n",
              "      <td>-0.020673</td>\n",
              "      <td>0.043405</td>\n",
              "      <td>0.009373</td>\n",
              "      <td>0.068233</td>\n",
              "      <td>-0.097857</td>\n",
              "      <td>-0.207807</td>\n",
              "      <td>-0.008571</td>\n",
              "      <td>0.104527</td>\n",
              "      <td>0.096941</td>\n",
              "      <td>0.115147</td>\n",
              "      <td>-0.008942</td>\n",
              "      <td>-0.110573</td>\n",
              "      <td>-0.040229</td>\n",
              "      <td>-0.051553</td>\n",
              "      <td>0.031355</td>\n",
              "      <td>-0.105504</td>\n",
              "      <td>0.010230</td>\n",
              "      <td>-0.020512</td>\n",
              "      <td>-0.078979</td>\n",
              "      <td>0.013674</td>\n",
              "      <td>-0.064750</td>\n",
              "      <td>0.162955</td>\n",
              "      <td>-0.077624</td>\n",
              "      <td>-0.150068</td>\n",
              "      <td>-0.085999</td>\n",
              "      <td>-0.046899</td>\n",
              "      <td>-0.023124</td>\n",
              "      <td>0.054796</td>\n",
              "      <td>0.170694</td>\n",
              "      <td>0.074289</td>\n",
              "      <td>0.063477</td>\n",
              "      <td>-0.080540</td>\n",
              "      <td>-0.018136</td>\n",
              "      <td>-0.030698</td>\n",
              "      <td>0.003611</td>\n",
              "      <td>0.047468</td>\n",
              "      <td>0.082973</td>\n",
              "      <td>-0.017027</td>\n",
              "      <td>0.099034</td>\n",
              "      <td>0.100255</td>\n",
              "      <td>0.036614</td>\n",
              "      <td>-0.064976</td>\n",
              "      <td>-0.057118</td>\n",
              "      <td>-0.088784</td>\n",
              "      <td>-0.036063</td>\n",
              "      <td>0.114485</td>\n",
              "      <td>-0.024549</td>\n",
              "      <td>0.183432</td>\n",
              "      <td>-0.109658</td>\n",
              "      <td>0.030273</td>\n",
              "      <td>-0.043125</td>\n",
              "      <td>-0.049957</td>\n",
              "      <td>-0.017133</td>\n",
              "      <td>-0.090628</td>\n",
              "      <td>-0.043672</td>\n",
              "      <td>0.064272</td>\n",
              "      <td>0.023642</td>\n",
              "      <td>-0.004700</td>\n",
              "      <td>0.041460</td>\n",
              "      <td>0.105759</td>\n",
              "      <td>0.027174</td>\n",
              "      <td>-0.027481</td>\n",
              "      <td>0.002319</td>\n",
              "      <td>-0.184525</td>\n",
              "      <td>0.031586</td>\n",
              "      <td>0.075348</td>\n",
              "      <td>-0.089478</td>\n",
              "      <td>-0.097244</td>\n",
              "      <td>-0.005665</td>\n",
              "      <td>-0.010010</td>\n",
              "      <td>-0.093909</td>\n",
              "      <td>0.007469</td>\n",
              "      <td>-0.008614</td>\n",
              "      <td>0.030029</td>\n",
              "      <td>0.015177</td>\n",
              "      <td>-0.044548</td>\n",
              "      <td>-0.016830</td>\n",
              "      <td>0.114384</td>\n",
              "      <td>-0.080585</td>\n",
              "      <td>-0.016862</td>\n",
              "      <td>0.037582</td>\n",
              "      <td>-0.053860</td>\n",
              "      <td>-0.069687</td>\n",
              "      <td>0.083313</td>\n",
              "      <td>-0.042421</td>\n",
              "      <td>0.002563</td>\n",
              "      <td>-0.013336</td>\n",
              "      <td>-0.007278</td>\n",
              "      <td>-0.004440</td>\n",
              "      <td>-0.074299</td>\n",
              "      <td>-0.005066</td>\n",
              "      <td>-0.020451</td>\n",
              "      <td>-0.059204</td>\n",
              "      <td>-0.062714</td>\n",
              "      <td>0.009560</td>\n",
              "      <td>0.078644</td>\n",
              "      <td>0.026581</td>\n",
              "      <td>-0.133003</td>\n",
              "      <td>0.090698</td>\n",
              "      <td>-0.045013</td>\n",
              "      <td>0.053688</td>\n",
              "      <td>0.012964</td>\n",
              "      <td>-0.024307</td>\n",
              "      <td>-0.016724</td>\n",
              "      <td>-0.083771</td>\n",
              "      <td>0.045853</td>\n",
              "      <td>0.017776</td>\n",
              "      <td>0.040825</td>\n",
              "      <td>0.028770</td>\n",
              "      <td>-0.047867</td>\n",
              "      <td>0.018163</td>\n",
              "      <td>0.010151</td>\n",
              "      <td>-0.028946</td>\n",
              "      <td>-0.023495</td>\n",
              "      <td>-0.023163</td>\n",
              "      <td>-0.028366</td>\n",
              "      <td>-0.114807</td>\n",
              "      <td>0.051544</td>\n",
              "      <td>-0.007233</td>\n",
              "      <td>-0.071938</td>\n",
              "      <td>0.088280</td>\n",
              "      <td>0.049053</td>\n",
              "      <td>-0.040497</td>\n",
              "      <td>0.064194</td>\n",
              "      <td>0.067669</td>\n",
              "      <td>0.138885</td>\n",
              "      <td>0.060989</td>\n",
              "      <td>0.051712</td>\n",
              "      <td>0.103764</td>\n",
              "      <td>0.040390</td>\n",
              "      <td>-0.122742</td>\n",
              "      <td>-0.037155</td>\n",
              "      <td>-0.001972</td>\n",
              "      <td>0.063379</td>\n",
              "      <td>0.097115</td>\n",
              "      <td>0.082230</td>\n",
              "      <td>0.079483</td>\n",
              "      <td>-0.069189</td>\n",
              "      <td>0.039116</td>\n",
              "      <td>-0.018723</td>\n",
              "      <td>-0.022295</td>\n",
              "      <td>-0.085098</td>\n",
              "      <td>-0.025173</td>\n",
              "      <td>0.111206</td>\n",
              "      <td>0.005409</td>\n",
              "      <td>0.042976</td>\n",
              "      <td>-0.012733</td>\n",
              "      <td>0.034204</td>\n",
              "      <td>-0.066124</td>\n",
              "      <td>-0.106903</td>\n",
              "      <td>-0.101349</td>\n",
              "      <td>-0.002483</td>\n",
              "      <td>0.054401</td>\n",
              "      <td>-0.014191</td>\n",
              "      <td>0.063248</td>\n",
              "      <td>-0.031708</td>\n",
              "      <td>-0.049109</td>\n",
              "      <td>-0.057486</td>\n",
              "      <td>0.060177</td>\n",
              "      <td>-0.006256</td>\n",
              "      <td>-0.023739</td>\n",
              "      <td>-0.055176</td>\n",
              "      <td>-0.048543</td>\n",
              "      <td>0.019409</td>\n",
              "      <td>-0.000183</td>\n",
              "      <td>-0.060661</td>\n",
              "      <td>0.004597</td>\n",
              "      <td>0.035736</td>\n",
              "      <td>0.012192</td>\n",
              "      <td>-0.032845</td>\n",
              "      <td>0.085388</td>\n",
              "      <td>-0.046345</td>\n",
              "      <td>-0.019450</td>\n",
              "      <td>-0.035706</td>\n",
              "      <td>0.016754</td>\n",
              "      <td>0.057755</td>\n",
              "      <td>-0.098450</td>\n",
              "      <td>-0.018036</td>\n",
              "      <td>-0.022797</td>\n",
              "      <td>0.022186</td>\n",
              "      <td>0.032612</td>\n",
              "      <td>0.025421</td>\n",
              "      <td>-0.035217</td>\n",
              "      <td>-0.030624</td>\n",
              "      <td>0.108231</td>\n",
              "      <td>0.066277</td>\n",
              "      <td>-0.103188</td>\n",
              "      <td>-0.026997</td>\n",
              "      <td>-0.147491</td>\n",
              "      <td>0.000961</td>\n",
              "      <td>-0.031845</td>\n",
              "      <td>-0.033691</td>\n",
              "      <td>0.071167</td>\n",
              "      <td>0.050507</td>\n",
              "      <td>-0.081879</td>\n",
              "      <td>0.144764</td>\n",
              "      <td>0.058868</td>\n",
              "      <td>-0.106548</td>\n",
              "      <td>0.035995</td>\n",
              "      <td>-0.035400</td>\n",
              "      <td>-0.017593</td>\n",
              "      <td>-0.016199</td>\n",
              "      <td>0.002197</td>\n",
              "      <td>-0.060684</td>\n",
              "      <td>-0.082272</td>\n",
              "      <td>-0.071381</td>\n",
              "      <td>0.042133</td>\n",
              "      <td>0.035461</td>\n",
              "      <td>-0.060865</td>\n",
              "      <td>0.055679</td>\n",
              "      <td>-0.009415</td>\n",
              "      <td>-0.028581</td>\n",
              "      <td>-0.104553</td>\n",
              "      <td>-0.082756</td>\n",
              "      <td>-0.066826</td>\n",
              "      <td>-0.052322</td>\n",
              "      <td>-0.017059</td>\n",
              "      <td>-0.005196</td>\n",
              "      <td>0.032303</td>\n",
              "      <td>0.047432</td>\n",
              "      <td>0.025467</td>\n",
              "      <td>-0.045532</td>\n",
              "      <td>0.003082</td>\n",
              "      <td>-0.071457</td>\n",
              "      <td>0.083565</td>\n",
              "      <td>-0.048666</td>\n",
              "      <td>-0.035172</td>\n",
              "      <td>-0.048920</td>\n",
              "      <td>-0.047852</td>\n",
              "      <td>-0.078445</td>\n",
              "      <td>-0.051291</td>\n",
              "      <td>-0.057014</td>\n",
              "      <td>0.113098</td>\n",
              "      <td>-0.071762</td>\n",
              "      <td>-0.040543</td>\n",
              "      <td>0.025101</td>\n",
              "      <td>-0.082382</td>\n",
              "      <td>-0.020996</td>\n",
              "      <td>-0.039651</td>\n",
              "      <td>0.081390</td>\n",
              "      <td>-0.031908</td>\n",
              "      <td>-0.030106</td>\n",
              "      <td>0.076168</td>\n",
              "      <td>0.021408</td>\n",
              "      <td>0.110954</td>\n",
              "      <td>0.008377</td>\n",
              "      <td>-0.019379</td>\n",
              "      <td>-0.041382</td>\n",
              "      <td>0.105824</td>\n",
              "      <td>0.037804</td>\n",
              "      <td>-0.063751</td>\n",
              "      <td>0.114182</td>\n",
              "      <td>-0.053772</td>\n",
              "      <td>0.013447</td>\n",
              "      <td>-0.096786</td>\n",
              "      <td>-0.102066</td>\n",
              "      <td>0.046633</td>\n",
              "      <td>0.029480</td>\n",
              "      <td>-0.029068</td>\n",
              "      <td>-0.016441</td>\n",
              "      <td>0.052124</td>\n",
              "      <td>-0.068390</td>\n",
              "      <td>0.010468</td>\n",
              "      <td>0.065750</td>\n",
              "      <td>0.053902</td>\n",
              "      <td>-0.043030</td>\n",
              "      <td>-0.076973</td>\n",
              "      <td>0.046310</td>\n",
              "      <td>-0.089283</td>\n",
              "      <td>0.053295</td>\n",
              "      <td>-0.090179</td>\n",
              "      <td>-0.024185</td>\n",
              "      <td>0.044952</td>\n",
              "      <td>-0.026138</td>\n",
              "      <td>-0.108727</td>\n",
              "      <td>-0.003693</td>\n",
              "      <td>-0.024208</td>\n",
              "      <td>0.080994</td>\n",
              "      <td>0.096851</td>\n",
              "      <td>0.065338</td>\n",
              "      <td>0.093872</td>\n",
              "      <td>-0.088501</td>\n",
              "      <td>0.035175</td>\n",
              "      <td>-0.006592</td>\n",
              "      <td>-0.051520</td>\n",
              "      <td>0.016688</td>\n",
              "      <td>0.022655</td>\n",
              "      <td>-0.059052</td>\n",
              "      <td>0.034027</td>\n",
              "      <td>0.024185</td>\n",
              "      <td>-0.039603</td>\n",
              "      <td>-0.028549</td>\n",
              "      <td>0.036865</td>\n",
              "      <td>-0.064026</td>\n",
              "      <td>0.053818</td>\n",
              "      <td>-0.002987</td>\n",
              "      <td>0.043949</td>\n",
              "      <td>-0.053986</td>\n",
              "      <td>-0.013428</td>\n",
              "      <td>-0.127075</td>\n",
              "      <td>-0.058418</td>\n",
              "      <td>-0.011642</td>\n",
              "      <td>0.005737</td>\n",
              "      <td>0.029709</td>\n",
              "      <td>-0.010563</td>\n",
              "      <td>0.009186</td>\n",
              "      <td>0.021261</td>\n",
              "      <td>-0.050323</td>\n",
              "      <td>0.066284</td>\n",
              "      <td>0.018356</td>\n",
              "      <td>-0.016045</td>\n",
              "      <td>-0.017506</td>\n",
              "      <td>0.010254</td>\n",
              "      <td>0.008957</td>\n",
              "      <td>-0.067612</td>\n",
              "      <td>-0.063530</td>\n",
              "      <td>0.041290</td>\n",
              "      <td>0.013977</td>\n",
              "      <td>-0.097321</td>\n",
              "      <td>0.055268</td>\n",
              "      <td>0.057220</td>\n",
              "      <td>0.088966</td>\n",
              "      <td>-0.017834</td>\n",
              "      <td>-0.037994</td>\n",
              "      <td>-0.142250</td>\n",
              "      <td>0.029236</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>0.058197</td>\n",
              "      <td>0.107529</td>\n",
              "      <td>0.047714</td>\n",
              "      <td>0.005437</td>\n",
              "      <td>-0.102744</td>\n",
              "      <td>-0.076538</td>\n",
              "      <td>-0.098022</td>\n",
              "      <td>-0.053116</td>\n",
              "      <td>-0.079941</td>\n",
              "      <td>0.051016</td>\n",
              "      <td>0.034670</td>\n",
              "      <td>0.034798</td>\n",
              "      <td>0.155212</td>\n",
              "      <td>0.014664</td>\n",
              "      <td>-0.036133</td>\n",
              "      <td>-0.025864</td>\n",
              "      <td>-0.029800</td>\n",
              "      <td>-0.032715</td>\n",
              "      <td>0.091858</td>\n",
              "      <td>-0.078674</td>\n",
              "      <td>0.096649</td>\n",
              "      <td>-0.100101</td>\n",
              "      <td>-0.024906</td>\n",
              "      <td>-0.036613</td>\n",
              "      <td>-0.032570</td>\n",
              "      <td>-0.121918</td>\n",
              "      <td>-0.018051</td>\n",
              "      <td>0.046509</td>\n",
              "      <td>-0.085266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.005580</td>\n",
              "      <td>0.085300</td>\n",
              "      <td>-0.023612</td>\n",
              "      <td>0.059867</td>\n",
              "      <td>-0.084558</td>\n",
              "      <td>0.055219</td>\n",
              "      <td>0.045292</td>\n",
              "      <td>-0.013961</td>\n",
              "      <td>0.073800</td>\n",
              "      <td>0.073568</td>\n",
              "      <td>-0.064131</td>\n",
              "      <td>-0.087977</td>\n",
              "      <td>-0.001510</td>\n",
              "      <td>0.007506</td>\n",
              "      <td>-0.118765</td>\n",
              "      <td>0.070056</td>\n",
              "      <td>0.081668</td>\n",
              "      <td>0.098182</td>\n",
              "      <td>0.016869</td>\n",
              "      <td>-0.128859</td>\n",
              "      <td>-0.024112</td>\n",
              "      <td>0.031227</td>\n",
              "      <td>0.036170</td>\n",
              "      <td>0.034805</td>\n",
              "      <td>0.022005</td>\n",
              "      <td>0.028271</td>\n",
              "      <td>-0.061917</td>\n",
              "      <td>0.089236</td>\n",
              "      <td>0.052048</td>\n",
              "      <td>-0.070236</td>\n",
              "      <td>-0.072764</td>\n",
              "      <td>0.036908</td>\n",
              "      <td>-0.039874</td>\n",
              "      <td>-0.017802</td>\n",
              "      <td>-0.065566</td>\n",
              "      <td>-0.058662</td>\n",
              "      <td>0.045484</td>\n",
              "      <td>0.060569</td>\n",
              "      <td>0.025748</td>\n",
              "      <td>0.038731</td>\n",
              "      <td>0.050980</td>\n",
              "      <td>-0.052367</td>\n",
              "      <td>0.114041</td>\n",
              "      <td>-0.010232</td>\n",
              "      <td>-0.028403</td>\n",
              "      <td>-0.056405</td>\n",
              "      <td>-0.051394</td>\n",
              "      <td>-0.009512</td>\n",
              "      <td>0.070647</td>\n",
              "      <td>-0.041611</td>\n",
              "      <td>-0.045579</td>\n",
              "      <td>-0.011794</td>\n",
              "      <td>-0.022420</td>\n",
              "      <td>-0.055827</td>\n",
              "      <td>-0.003942</td>\n",
              "      <td>0.041191</td>\n",
              "      <td>-0.021208</td>\n",
              "      <td>-0.079629</td>\n",
              "      <td>-0.036742</td>\n",
              "      <td>-0.022661</td>\n",
              "      <td>-0.138956</td>\n",
              "      <td>0.013124</td>\n",
              "      <td>-0.094502</td>\n",
              "      <td>-0.027921</td>\n",
              "      <td>-0.038336</td>\n",
              "      <td>-0.01165</td>\n",
              "      <td>-0.038306</td>\n",
              "      <td>0.027518</td>\n",
              "      <td>-0.062284</td>\n",
              "      <td>0.097517</td>\n",
              "      <td>0.071055</td>\n",
              "      <td>0.019842</td>\n",
              "      <td>0.093523</td>\n",
              "      <td>-0.024006</td>\n",
              "      <td>-0.124910</td>\n",
              "      <td>-0.081706</td>\n",
              "      <td>0.115077</td>\n",
              "      <td>0.074918</td>\n",
              "      <td>0.014500</td>\n",
              "      <td>0.057425</td>\n",
              "      <td>0.036139</td>\n",
              "      <td>0.020951</td>\n",
              "      <td>0.037078</td>\n",
              "      <td>-0.024989</td>\n",
              "      <td>-0.018503</td>\n",
              "      <td>0.050570</td>\n",
              "      <td>-0.029624</td>\n",
              "      <td>0.062621</td>\n",
              "      <td>0.000421</td>\n",
              "      <td>0.022855</td>\n",
              "      <td>0.025969</td>\n",
              "      <td>-0.038253</td>\n",
              "      <td>-0.012213</td>\n",
              "      <td>-0.031447</td>\n",
              "      <td>0.004621</td>\n",
              "      <td>-0.032744</td>\n",
              "      <td>-0.007686</td>\n",
              "      <td>0.011745</td>\n",
              "      <td>0.081771</td>\n",
              "      <td>0.020228</td>\n",
              "      <td>-0.044685</td>\n",
              "      <td>-0.069836</td>\n",
              "      <td>0.041860</td>\n",
              "      <td>-0.045683</td>\n",
              "      <td>-0.063999</td>\n",
              "      <td>-0.038992</td>\n",
              "      <td>-0.002189</td>\n",
              "      <td>-0.011811</td>\n",
              "      <td>0.044066</td>\n",
              "      <td>-0.044411</td>\n",
              "      <td>-0.047256</td>\n",
              "      <td>0.002104</td>\n",
              "      <td>-0.007041</td>\n",
              "      <td>-0.011091</td>\n",
              "      <td>0.129405</td>\n",
              "      <td>0.018276</td>\n",
              "      <td>-0.043437</td>\n",
              "      <td>-0.050995</td>\n",
              "      <td>0.021507</td>\n",
              "      <td>0.030798</td>\n",
              "      <td>-0.032876</td>\n",
              "      <td>-0.018366</td>\n",
              "      <td>-0.084074</td>\n",
              "      <td>0.048709</td>\n",
              "      <td>0.018249</td>\n",
              "      <td>-0.020988</td>\n",
              "      <td>-0.066153</td>\n",
              "      <td>-0.051706</td>\n",
              "      <td>0.044230</td>\n",
              "      <td>0.009216</td>\n",
              "      <td>-0.024518</td>\n",
              "      <td>-0.026715</td>\n",
              "      <td>-0.053068</td>\n",
              "      <td>0.047969</td>\n",
              "      <td>-0.022892</td>\n",
              "      <td>-0.018158</td>\n",
              "      <td>0.028669</td>\n",
              "      <td>0.019290</td>\n",
              "      <td>0.031894</td>\n",
              "      <td>0.023224</td>\n",
              "      <td>0.090406</td>\n",
              "      <td>-0.056694</td>\n",
              "      <td>-0.025045</td>\n",
              "      <td>0.019466</td>\n",
              "      <td>0.026692</td>\n",
              "      <td>-0.018428</td>\n",
              "      <td>-0.035688</td>\n",
              "      <td>0.010103</td>\n",
              "      <td>-0.035806</td>\n",
              "      <td>-0.020723</td>\n",
              "      <td>0.036973</td>\n",
              "      <td>-0.038289</td>\n",
              "      <td>-0.082985</td>\n",
              "      <td>0.088022</td>\n",
              "      <td>-0.075249</td>\n",
              "      <td>-0.032980</td>\n",
              "      <td>-0.062438</td>\n",
              "      <td>-0.050168</td>\n",
              "      <td>-0.032538</td>\n",
              "      <td>0.008664</td>\n",
              "      <td>-0.014264</td>\n",
              "      <td>0.014338</td>\n",
              "      <td>0.011957</td>\n",
              "      <td>-0.034922</td>\n",
              "      <td>-0.013335</td>\n",
              "      <td>-0.047260</td>\n",
              "      <td>0.021864</td>\n",
              "      <td>-0.013537</td>\n",
              "      <td>-0.040186</td>\n",
              "      <td>0.003110</td>\n",
              "      <td>-0.085474</td>\n",
              "      <td>-0.014884</td>\n",
              "      <td>-0.037386</td>\n",
              "      <td>-0.004133</td>\n",
              "      <td>-0.032421</td>\n",
              "      <td>0.031046</td>\n",
              "      <td>0.108533</td>\n",
              "      <td>-0.189647</td>\n",
              "      <td>-0.065071</td>\n",
              "      <td>-0.008955</td>\n",
              "      <td>-0.127483</td>\n",
              "      <td>-0.074811</td>\n",
              "      <td>0.016072</td>\n",
              "      <td>-0.015883</td>\n",
              "      <td>-0.043636</td>\n",
              "      <td>-0.031738</td>\n",
              "      <td>-0.094789</td>\n",
              "      <td>0.020677</td>\n",
              "      <td>0.075420</td>\n",
              "      <td>0.039272</td>\n",
              "      <td>0.011520</td>\n",
              "      <td>-0.014802</td>\n",
              "      <td>-0.013358</td>\n",
              "      <td>-0.026666</td>\n",
              "      <td>0.002438</td>\n",
              "      <td>0.070307</td>\n",
              "      <td>-0.001578</td>\n",
              "      <td>-0.068348</td>\n",
              "      <td>-0.031263</td>\n",
              "      <td>-0.049007</td>\n",
              "      <td>0.049735</td>\n",
              "      <td>0.068944</td>\n",
              "      <td>-0.060033</td>\n",
              "      <td>-0.062541</td>\n",
              "      <td>0.079628</td>\n",
              "      <td>-0.038818</td>\n",
              "      <td>-0.046268</td>\n",
              "      <td>0.001947</td>\n",
              "      <td>-0.017050</td>\n",
              "      <td>-0.052253</td>\n",
              "      <td>-0.029678</td>\n",
              "      <td>0.118643</td>\n",
              "      <td>-0.110048</td>\n",
              "      <td>0.008860</td>\n",
              "      <td>-0.050145</td>\n",
              "      <td>0.000312</td>\n",
              "      <td>0.003922</td>\n",
              "      <td>0.030736</td>\n",
              "      <td>-0.087338</td>\n",
              "      <td>0.007633</td>\n",
              "      <td>-0.047783</td>\n",
              "      <td>0.033778</td>\n",
              "      <td>-0.028245</td>\n",
              "      <td>0.038771</td>\n",
              "      <td>0.008593</td>\n",
              "      <td>-0.007277</td>\n",
              "      <td>0.022737</td>\n",
              "      <td>0.042860</td>\n",
              "      <td>0.026604</td>\n",
              "      <td>-0.034759</td>\n",
              "      <td>0.009034</td>\n",
              "      <td>-0.037268</td>\n",
              "      <td>0.010572</td>\n",
              "      <td>-0.016896</td>\n",
              "      <td>0.042140</td>\n",
              "      <td>0.007601</td>\n",
              "      <td>-0.008338</td>\n",
              "      <td>-0.056065</td>\n",
              "      <td>0.034869</td>\n",
              "      <td>0.014307</td>\n",
              "      <td>-0.009829</td>\n",
              "      <td>-0.039078</td>\n",
              "      <td>0.009282</td>\n",
              "      <td>-0.096402</td>\n",
              "      <td>0.043873</td>\n",
              "      <td>-0.016066</td>\n",
              "      <td>-0.041573</td>\n",
              "      <td>0.060135</td>\n",
              "      <td>-0.001188</td>\n",
              "      <td>-0.094655</td>\n",
              "      <td>-0.011820</td>\n",
              "      <td>-0.003895</td>\n",
              "      <td>0.079302</td>\n",
              "      <td>0.086066</td>\n",
              "      <td>0.034077</td>\n",
              "      <td>-0.044135</td>\n",
              "      <td>-0.016489</td>\n",
              "      <td>-0.024242</td>\n",
              "      <td>-0.079945</td>\n",
              "      <td>-0.015627</td>\n",
              "      <td>0.008575</td>\n",
              "      <td>-0.000057</td>\n",
              "      <td>-0.038578</td>\n",
              "      <td>0.003882</td>\n",
              "      <td>0.092860</td>\n",
              "      <td>0.070185</td>\n",
              "      <td>0.008929</td>\n",
              "      <td>0.003402</td>\n",
              "      <td>-0.067874</td>\n",
              "      <td>-0.014913</td>\n",
              "      <td>-0.010251</td>\n",
              "      <td>0.092048</td>\n",
              "      <td>0.048665</td>\n",
              "      <td>0.004263</td>\n",
              "      <td>0.082684</td>\n",
              "      <td>-0.070974</td>\n",
              "      <td>-0.085914</td>\n",
              "      <td>-0.121464</td>\n",
              "      <td>-0.108557</td>\n",
              "      <td>0.056362</td>\n",
              "      <td>0.003471</td>\n",
              "      <td>-0.004600</td>\n",
              "      <td>0.027936</td>\n",
              "      <td>0.081766</td>\n",
              "      <td>-0.001350</td>\n",
              "      <td>0.025344</td>\n",
              "      <td>-0.049986</td>\n",
              "      <td>-0.025076</td>\n",
              "      <td>0.005720</td>\n",
              "      <td>0.006427</td>\n",
              "      <td>-0.040072</td>\n",
              "      <td>0.103039</td>\n",
              "      <td>-0.082554</td>\n",
              "      <td>0.022182</td>\n",
              "      <td>-0.003769</td>\n",
              "      <td>0.015608</td>\n",
              "      <td>-0.013759</td>\n",
              "      <td>-0.031591</td>\n",
              "      <td>-0.034226</td>\n",
              "      <td>-0.004203</td>\n",
              "      <td>0.072074</td>\n",
              "      <td>0.040558</td>\n",
              "      <td>-0.059041</td>\n",
              "      <td>0.076077</td>\n",
              "      <td>-0.085263</td>\n",
              "      <td>0.053602</td>\n",
              "      <td>0.013017</td>\n",
              "      <td>-0.031647</td>\n",
              "      <td>0.110952</td>\n",
              "      <td>0.122038</td>\n",
              "      <td>0.018046</td>\n",
              "      <td>-0.103100</td>\n",
              "      <td>-0.042667</td>\n",
              "      <td>-0.065543</td>\n",
              "      <td>-0.096259</td>\n",
              "      <td>0.084649</td>\n",
              "      <td>-0.019592</td>\n",
              "      <td>0.133745</td>\n",
              "      <td>-0.010486</td>\n",
              "      <td>-0.140469</td>\n",
              "      <td>0.026974</td>\n",
              "      <td>0.075194</td>\n",
              "      <td>-0.016741</td>\n",
              "      <td>0.027879</td>\n",
              "      <td>-0.047309</td>\n",
              "      <td>-0.034644</td>\n",
              "      <td>-0.145486</td>\n",
              "      <td>0.197649</td>\n",
              "      <td>0.067005</td>\n",
              "      <td>-0.110731</td>\n",
              "      <td>-0.054667</td>\n",
              "      <td>-0.026079</td>\n",
              "      <td>-0.095256</td>\n",
              "      <td>0.090323</td>\n",
              "      <td>-0.092984</td>\n",
              "      <td>-0.095876</td>\n",
              "      <td>0.015055</td>\n",
              "      <td>0.088971</td>\n",
              "      <td>-0.027774</td>\n",
              "      <td>0.020678</td>\n",
              "      <td>0.092035</td>\n",
              "      <td>0.041648</td>\n",
              "      <td>0.160799</td>\n",
              "      <td>0.062739</td>\n",
              "      <td>-0.045525</td>\n",
              "      <td>-0.020688</td>\n",
              "      <td>-0.037571</td>\n",
              "      <td>-0.005925</td>\n",
              "      <td>0.103875</td>\n",
              "      <td>-0.088866</td>\n",
              "      <td>-0.019413</td>\n",
              "      <td>-0.054698</td>\n",
              "      <td>0.002770</td>\n",
              "      <td>-0.009474</td>\n",
              "      <td>-0.008789</td>\n",
              "      <td>0.008799</td>\n",
              "      <td>-0.117137</td>\n",
              "      <td>-0.147083</td>\n",
              "      <td>-0.035261</td>\n",
              "      <td>0.004141</td>\n",
              "      <td>-0.160217</td>\n",
              "      <td>0.046421</td>\n",
              "      <td>-0.166667</td>\n",
              "      <td>-0.012882</td>\n",
              "      <td>-0.030312</td>\n",
              "      <td>-0.048543</td>\n",
              "      <td>-0.066866</td>\n",
              "      <td>0.066101</td>\n",
              "      <td>-0.088707</td>\n",
              "      <td>0.146139</td>\n",
              "      <td>0.064121</td>\n",
              "      <td>0.055908</td>\n",
              "      <td>0.029626</td>\n",
              "      <td>0.014513</td>\n",
              "      <td>-0.140981</td>\n",
              "      <td>-0.078273</td>\n",
              "      <td>0.176829</td>\n",
              "      <td>0.071353</td>\n",
              "      <td>0.033986</td>\n",
              "      <td>-0.027561</td>\n",
              "      <td>0.065012</td>\n",
              "      <td>0.030216</td>\n",
              "      <td>0.013129</td>\n",
              "      <td>-0.042867</td>\n",
              "      <td>0.037198</td>\n",
              "      <td>0.072281</td>\n",
              "      <td>-0.031041</td>\n",
              "      <td>0.153687</td>\n",
              "      <td>0.034637</td>\n",
              "      <td>-0.035149</td>\n",
              "      <td>-0.017297</td>\n",
              "      <td>-0.057248</td>\n",
              "      <td>-0.065253</td>\n",
              "      <td>0.033555</td>\n",
              "      <td>0.001017</td>\n",
              "      <td>0.006477</td>\n",
              "      <td>0.004241</td>\n",
              "      <td>-0.019572</td>\n",
              "      <td>0.163083</td>\n",
              "      <td>0.047638</td>\n",
              "      <td>0.028456</td>\n",
              "      <td>0.036516</td>\n",
              "      <td>0.090596</td>\n",
              "      <td>-0.024985</td>\n",
              "      <td>0.015789</td>\n",
              "      <td>0.031962</td>\n",
              "      <td>0.009528</td>\n",
              "      <td>0.027685</td>\n",
              "      <td>0.141507</td>\n",
              "      <td>-0.084938</td>\n",
              "      <td>-0.076904</td>\n",
              "      <td>-0.036393</td>\n",
              "      <td>-0.051358</td>\n",
              "      <td>-0.027690</td>\n",
              "      <td>0.072727</td>\n",
              "      <td>0.063784</td>\n",
              "      <td>-0.131812</td>\n",
              "      <td>-0.100488</td>\n",
              "      <td>0.061903</td>\n",
              "      <td>-0.030070</td>\n",
              "      <td>0.018216</td>\n",
              "      <td>-0.053216</td>\n",
              "      <td>-0.031942</td>\n",
              "      <td>0.001055</td>\n",
              "      <td>0.056776</td>\n",
              "      <td>-0.113120</td>\n",
              "      <td>-0.071985</td>\n",
              "      <td>-0.015476</td>\n",
              "      <td>0.114777</td>\n",
              "      <td>0.080234</td>\n",
              "      <td>-0.001495</td>\n",
              "      <td>0.047900</td>\n",
              "      <td>-0.004254</td>\n",
              "      <td>-0.002325</td>\n",
              "      <td>0.007401</td>\n",
              "      <td>0.047967</td>\n",
              "      <td>0.049201</td>\n",
              "      <td>0.001354</td>\n",
              "      <td>-0.045737</td>\n",
              "      <td>-0.022102</td>\n",
              "      <td>0.169608</td>\n",
              "      <td>-0.049750</td>\n",
              "      <td>-0.123857</td>\n",
              "      <td>0.026048</td>\n",
              "      <td>0.003852</td>\n",
              "      <td>-0.061346</td>\n",
              "      <td>0.066515</td>\n",
              "      <td>0.001324</td>\n",
              "      <td>-0.094447</td>\n",
              "      <td>-0.071023</td>\n",
              "      <td>0.109818</td>\n",
              "      <td>-0.101891</td>\n",
              "      <td>-0.065055</td>\n",
              "      <td>0.018982</td>\n",
              "      <td>-0.087540</td>\n",
              "      <td>-0.075412</td>\n",
              "      <td>-0.080638</td>\n",
              "      <td>0.016439</td>\n",
              "      <td>-0.062612</td>\n",
              "      <td>0.029961</td>\n",
              "      <td>-0.036604</td>\n",
              "      <td>-0.047197</td>\n",
              "      <td>0.011225</td>\n",
              "      <td>-0.037971</td>\n",
              "      <td>-0.029541</td>\n",
              "      <td>-0.013615</td>\n",
              "      <td>0.025279</td>\n",
              "      <td>0.004001</td>\n",
              "      <td>-0.163066</td>\n",
              "      <td>-0.010736</td>\n",
              "      <td>-0.177494</td>\n",
              "      <td>-0.076650</td>\n",
              "      <td>0.048767</td>\n",
              "      <td>-0.049095</td>\n",
              "      <td>-0.071594</td>\n",
              "      <td>0.028727</td>\n",
              "      <td>0.077076</td>\n",
              "      <td>-0.230523</td>\n",
              "      <td>-0.016568</td>\n",
              "      <td>-0.063239</td>\n",
              "      <td>-0.107525</td>\n",
              "      <td>-0.075107</td>\n",
              "      <td>-0.042996</td>\n",
              "      <td>-0.085531</td>\n",
              "      <td>0.019552</td>\n",
              "      <td>-0.006953</td>\n",
              "      <td>-0.095984</td>\n",
              "      <td>-0.011824</td>\n",
              "      <td>0.011069</td>\n",
              "      <td>0.071967</td>\n",
              "      <td>-0.028858</td>\n",
              "      <td>0.059741</td>\n",
              "      <td>-0.050117</td>\n",
              "      <td>-0.049461</td>\n",
              "      <td>-0.019352</td>\n",
              "      <td>0.101257</td>\n",
              "      <td>0.059842</td>\n",
              "      <td>-0.037360</td>\n",
              "      <td>0.009777</td>\n",
              "      <td>-0.011286</td>\n",
              "      <td>0.039090</td>\n",
              "      <td>0.058506</td>\n",
              "      <td>0.012838</td>\n",
              "      <td>0.042984</td>\n",
              "      <td>0.124607</td>\n",
              "      <td>-0.092646</td>\n",
              "      <td>-0.010668</td>\n",
              "      <td>-0.075703</td>\n",
              "      <td>-0.053804</td>\n",
              "      <td>-0.044622</td>\n",
              "      <td>-0.051129</td>\n",
              "      <td>0.099270</td>\n",
              "      <td>-0.100833</td>\n",
              "      <td>0.008849</td>\n",
              "      <td>-0.119866</td>\n",
              "      <td>0.032648</td>\n",
              "      <td>0.006958</td>\n",
              "      <td>0.025728</td>\n",
              "      <td>-0.105520</td>\n",
              "      <td>-0.012360</td>\n",
              "      <td>-0.081112</td>\n",
              "      <td>0.010891</td>\n",
              "      <td>0.072445</td>\n",
              "      <td>0.064259</td>\n",
              "      <td>-0.055200</td>\n",
              "      <td>0.027584</td>\n",
              "      <td>0.107807</td>\n",
              "      <td>0.076508</td>\n",
              "      <td>0.054804</td>\n",
              "      <td>-0.033447</td>\n",
              "      <td>-0.047251</td>\n",
              "      <td>-0.046861</td>\n",
              "      <td>0.018314</td>\n",
              "      <td>-0.022186</td>\n",
              "      <td>-0.020949</td>\n",
              "      <td>0.035604</td>\n",
              "      <td>0.023521</td>\n",
              "      <td>-0.060888</td>\n",
              "      <td>0.028300</td>\n",
              "      <td>0.006326</td>\n",
              "      <td>0.058729</td>\n",
              "      <td>-0.039130</td>\n",
              "      <td>0.021708</td>\n",
              "      <td>0.004483</td>\n",
              "      <td>-0.014041</td>\n",
              "      <td>0.017256</td>\n",
              "      <td>-0.089837</td>\n",
              "      <td>0.026177</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.032808</td>\n",
              "      <td>0.052333</td>\n",
              "      <td>-0.058609</td>\n",
              "      <td>0.094645</td>\n",
              "      <td>0.022017</td>\n",
              "      <td>0.021366</td>\n",
              "      <td>-0.032606</td>\n",
              "      <td>-0.030555</td>\n",
              "      <td>-0.044713</td>\n",
              "      <td>0.046777</td>\n",
              "      <td>-0.020657</td>\n",
              "      <td>0.058076</td>\n",
              "      <td>0.058478</td>\n",
              "      <td>-0.030192</td>\n",
              "      <td>-0.068566</td>\n",
              "      <td>0.090522</td>\n",
              "      <td>0.053301</td>\n",
              "      <td>-0.039795</td>\n",
              "      <td>-0.099763</td>\n",
              "      <td>-0.081404</td>\n",
              "      <td>0.045653</td>\n",
              "      <td>-0.044968</td>\n",
              "      <td>0.005537</td>\n",
              "      <td>-0.017688</td>\n",
              "      <td>-0.008752</td>\n",
              "      <td>0.072693</td>\n",
              "      <td>-0.001940</td>\n",
              "      <td>-0.044600</td>\n",
              "      <td>-0.168209</td>\n",
              "      <td>-0.051210</td>\n",
              "      <td>0.148349</td>\n",
              "      <td>-0.029427</td>\n",
              "      <td>0.040434</td>\n",
              "      <td>0.040182</td>\n",
              "      <td>0.008592</td>\n",
              "      <td>-0.077905</td>\n",
              "      <td>0.043165</td>\n",
              "      <td>-0.035611</td>\n",
              "      <td>-0.004595</td>\n",
              "      <td>0.073013</td>\n",
              "      <td>-0.015964</td>\n",
              "      <td>-0.062968</td>\n",
              "      <td>0.025066</td>\n",
              "      <td>-0.042209</td>\n",
              "      <td>0.080261</td>\n",
              "      <td>0.040852</td>\n",
              "      <td>-0.025947</td>\n",
              "      <td>-0.033207</td>\n",
              "      <td>-0.043254</td>\n",
              "      <td>-0.045268</td>\n",
              "      <td>0.000799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.016178</td>\n",
              "      <td>0.032145</td>\n",
              "      <td>-0.021306</td>\n",
              "      <td>0.052707</td>\n",
              "      <td>0.063334</td>\n",
              "      <td>0.089003</td>\n",
              "      <td>0.116973</td>\n",
              "      <td>-0.108175</td>\n",
              "      <td>-0.008477</td>\n",
              "      <td>0.036092</td>\n",
              "      <td>0.030233</td>\n",
              "      <td>-0.057129</td>\n",
              "      <td>-0.012912</td>\n",
              "      <td>-0.036621</td>\n",
              "      <td>-0.168918</td>\n",
              "      <td>0.137004</td>\n",
              "      <td>0.072500</td>\n",
              "      <td>0.156752</td>\n",
              "      <td>0.005157</td>\n",
              "      <td>-0.032732</td>\n",
              "      <td>-0.072367</td>\n",
              "      <td>0.013156</td>\n",
              "      <td>0.070360</td>\n",
              "      <td>0.033040</td>\n",
              "      <td>0.072171</td>\n",
              "      <td>-0.026530</td>\n",
              "      <td>-0.108005</td>\n",
              "      <td>-0.045164</td>\n",
              "      <td>-0.040053</td>\n",
              "      <td>-0.062771</td>\n",
              "      <td>-0.045529</td>\n",
              "      <td>0.029690</td>\n",
              "      <td>0.041395</td>\n",
              "      <td>-0.037693</td>\n",
              "      <td>0.070801</td>\n",
              "      <td>-0.049967</td>\n",
              "      <td>0.081733</td>\n",
              "      <td>0.040507</td>\n",
              "      <td>0.071876</td>\n",
              "      <td>0.079305</td>\n",
              "      <td>0.080473</td>\n",
              "      <td>-0.023105</td>\n",
              "      <td>0.105306</td>\n",
              "      <td>0.063653</td>\n",
              "      <td>-0.114855</td>\n",
              "      <td>-0.010607</td>\n",
              "      <td>-0.070160</td>\n",
              "      <td>-0.068264</td>\n",
              "      <td>-0.009111</td>\n",
              "      <td>0.037232</td>\n",
              "      <td>-0.004612</td>\n",
              "      <td>0.061157</td>\n",
              "      <td>0.038808</td>\n",
              "      <td>-0.024482</td>\n",
              "      <td>0.019803</td>\n",
              "      <td>-0.011163</td>\n",
              "      <td>-0.060235</td>\n",
              "      <td>-0.065362</td>\n",
              "      <td>0.084656</td>\n",
              "      <td>-0.125081</td>\n",
              "      <td>0.001655</td>\n",
              "      <td>0.090416</td>\n",
              "      <td>-0.073310</td>\n",
              "      <td>0.002609</td>\n",
              "      <td>0.008274</td>\n",
              "      <td>-0.04796</td>\n",
              "      <td>-0.049181</td>\n",
              "      <td>0.043165</td>\n",
              "      <td>-0.062676</td>\n",
              "      <td>0.011047</td>\n",
              "      <td>0.082601</td>\n",
              "      <td>0.003052</td>\n",
              "      <td>0.113376</td>\n",
              "      <td>0.021973</td>\n",
              "      <td>-0.134576</td>\n",
              "      <td>0.004964</td>\n",
              "      <td>0.085649</td>\n",
              "      <td>0.083908</td>\n",
              "      <td>-0.016900</td>\n",
              "      <td>0.025180</td>\n",
              "      <td>0.014476</td>\n",
              "      <td>-0.015774</td>\n",
              "      <td>0.081594</td>\n",
              "      <td>-0.017470</td>\n",
              "      <td>-0.116699</td>\n",
              "      <td>0.006321</td>\n",
              "      <td>-0.035468</td>\n",
              "      <td>0.115857</td>\n",
              "      <td>0.026408</td>\n",
              "      <td>0.031477</td>\n",
              "      <td>0.058539</td>\n",
              "      <td>0.053507</td>\n",
              "      <td>-0.065267</td>\n",
              "      <td>-0.044169</td>\n",
              "      <td>-0.013536</td>\n",
              "      <td>-0.029460</td>\n",
              "      <td>0.063104</td>\n",
              "      <td>0.020223</td>\n",
              "      <td>0.061117</td>\n",
              "      <td>-0.051639</td>\n",
              "      <td>-0.121392</td>\n",
              "      <td>-0.038249</td>\n",
              "      <td>0.071242</td>\n",
              "      <td>-0.024224</td>\n",
              "      <td>-0.027161</td>\n",
              "      <td>-0.023473</td>\n",
              "      <td>-0.006612</td>\n",
              "      <td>-0.025869</td>\n",
              "      <td>0.074678</td>\n",
              "      <td>-0.058160</td>\n",
              "      <td>0.002631</td>\n",
              "      <td>-0.057838</td>\n",
              "      <td>0.068186</td>\n",
              "      <td>0.005851</td>\n",
              "      <td>0.054091</td>\n",
              "      <td>-0.056695</td>\n",
              "      <td>-0.021244</td>\n",
              "      <td>-0.109402</td>\n",
              "      <td>-0.001017</td>\n",
              "      <td>0.028402</td>\n",
              "      <td>-0.072873</td>\n",
              "      <td>0.033407</td>\n",
              "      <td>-0.041880</td>\n",
              "      <td>0.083550</td>\n",
              "      <td>-0.066332</td>\n",
              "      <td>-0.010379</td>\n",
              "      <td>-0.018158</td>\n",
              "      <td>-0.035507</td>\n",
              "      <td>-0.020643</td>\n",
              "      <td>0.038194</td>\n",
              "      <td>-0.026259</td>\n",
              "      <td>-0.119439</td>\n",
              "      <td>-0.069383</td>\n",
              "      <td>0.009928</td>\n",
              "      <td>-0.034403</td>\n",
              "      <td>-0.020060</td>\n",
              "      <td>-0.034234</td>\n",
              "      <td>0.097619</td>\n",
              "      <td>0.040541</td>\n",
              "      <td>0.036620</td>\n",
              "      <td>-0.002007</td>\n",
              "      <td>-0.006273</td>\n",
              "      <td>0.042508</td>\n",
              "      <td>-0.036251</td>\n",
              "      <td>-0.042101</td>\n",
              "      <td>0.049873</td>\n",
              "      <td>-0.114882</td>\n",
              "      <td>-0.051134</td>\n",
              "      <td>-0.062039</td>\n",
              "      <td>-0.025370</td>\n",
              "      <td>0.063938</td>\n",
              "      <td>0.036553</td>\n",
              "      <td>-0.043240</td>\n",
              "      <td>0.084825</td>\n",
              "      <td>-0.044798</td>\n",
              "      <td>-0.007982</td>\n",
              "      <td>-0.021142</td>\n",
              "      <td>-0.037876</td>\n",
              "      <td>-0.042996</td>\n",
              "      <td>-0.048151</td>\n",
              "      <td>-0.037842</td>\n",
              "      <td>0.102960</td>\n",
              "      <td>-0.030192</td>\n",
              "      <td>0.122423</td>\n",
              "      <td>0.056207</td>\n",
              "      <td>-0.155138</td>\n",
              "      <td>0.060086</td>\n",
              "      <td>-0.094957</td>\n",
              "      <td>0.008613</td>\n",
              "      <td>0.034614</td>\n",
              "      <td>-0.023926</td>\n",
              "      <td>0.044067</td>\n",
              "      <td>-0.051721</td>\n",
              "      <td>-0.049127</td>\n",
              "      <td>-0.106676</td>\n",
              "      <td>0.038439</td>\n",
              "      <td>0.126329</td>\n",
              "      <td>-0.045193</td>\n",
              "      <td>0.038323</td>\n",
              "      <td>0.029165</td>\n",
              "      <td>-0.081943</td>\n",
              "      <td>-0.040249</td>\n",
              "      <td>0.023356</td>\n",
              "      <td>-0.035556</td>\n",
              "      <td>-0.011146</td>\n",
              "      <td>-0.024127</td>\n",
              "      <td>0.034336</td>\n",
              "      <td>0.014733</td>\n",
              "      <td>-0.037503</td>\n",
              "      <td>0.079902</td>\n",
              "      <td>-0.007161</td>\n",
              "      <td>0.015959</td>\n",
              "      <td>0.023739</td>\n",
              "      <td>-0.019365</td>\n",
              "      <td>-0.036099</td>\n",
              "      <td>-0.036492</td>\n",
              "      <td>0.013604</td>\n",
              "      <td>0.036818</td>\n",
              "      <td>-0.060306</td>\n",
              "      <td>-0.010193</td>\n",
              "      <td>-0.064548</td>\n",
              "      <td>0.039951</td>\n",
              "      <td>-0.097358</td>\n",
              "      <td>-0.046848</td>\n",
              "      <td>-0.040405</td>\n",
              "      <td>-0.014883</td>\n",
              "      <td>-0.093262</td>\n",
              "      <td>0.016825</td>\n",
              "      <td>0.006015</td>\n",
              "      <td>-0.006538</td>\n",
              "      <td>0.067824</td>\n",
              "      <td>-0.042813</td>\n",
              "      <td>-0.106947</td>\n",
              "      <td>-0.000834</td>\n",
              "      <td>-0.022359</td>\n",
              "      <td>0.021817</td>\n",
              "      <td>0.088433</td>\n",
              "      <td>0.088169</td>\n",
              "      <td>-0.063761</td>\n",
              "      <td>-0.025002</td>\n",
              "      <td>0.001268</td>\n",
              "      <td>-0.074894</td>\n",
              "      <td>0.034695</td>\n",
              "      <td>0.035434</td>\n",
              "      <td>0.083103</td>\n",
              "      <td>-0.079054</td>\n",
              "      <td>0.005493</td>\n",
              "      <td>0.086297</td>\n",
              "      <td>0.002045</td>\n",
              "      <td>-0.011959</td>\n",
              "      <td>0.053433</td>\n",
              "      <td>-0.061761</td>\n",
              "      <td>0.021627</td>\n",
              "      <td>0.016259</td>\n",
              "      <td>0.094652</td>\n",
              "      <td>-0.042352</td>\n",
              "      <td>-0.056396</td>\n",
              "      <td>-0.068651</td>\n",
              "      <td>0.053593</td>\n",
              "      <td>0.038059</td>\n",
              "      <td>-0.001107</td>\n",
              "      <td>-0.004272</td>\n",
              "      <td>-0.027066</td>\n",
              "      <td>-0.054498</td>\n",
              "      <td>0.010640</td>\n",
              "      <td>0.057631</td>\n",
              "      <td>0.004868</td>\n",
              "      <td>0.038642</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>-0.102471</td>\n",
              "      <td>-0.035177</td>\n",
              "      <td>0.098728</td>\n",
              "      <td>0.009087</td>\n",
              "      <td>0.038574</td>\n",
              "      <td>0.080838</td>\n",
              "      <td>-0.014540</td>\n",
              "      <td>-0.026733</td>\n",
              "      <td>0.013217</td>\n",
              "      <td>-0.112739</td>\n",
              "      <td>-0.050128</td>\n",
              "      <td>-0.004123</td>\n",
              "      <td>-0.004612</td>\n",
              "      <td>-0.067491</td>\n",
              "      <td>0.011224</td>\n",
              "      <td>-0.000841</td>\n",
              "      <td>0.143175</td>\n",
              "      <td>0.010417</td>\n",
              "      <td>-0.015523</td>\n",
              "      <td>-0.070543</td>\n",
              "      <td>-0.007595</td>\n",
              "      <td>0.055779</td>\n",
              "      <td>0.053182</td>\n",
              "      <td>0.076362</td>\n",
              "      <td>0.029573</td>\n",
              "      <td>0.070814</td>\n",
              "      <td>0.003160</td>\n",
              "      <td>-0.110260</td>\n",
              "      <td>-0.047377</td>\n",
              "      <td>-0.164252</td>\n",
              "      <td>0.011909</td>\n",
              "      <td>-0.011544</td>\n",
              "      <td>-0.013593</td>\n",
              "      <td>0.032905</td>\n",
              "      <td>0.106418</td>\n",
              "      <td>0.099162</td>\n",
              "      <td>0.093560</td>\n",
              "      <td>-0.094699</td>\n",
              "      <td>0.010837</td>\n",
              "      <td>0.008592</td>\n",
              "      <td>0.138273</td>\n",
              "      <td>-0.102301</td>\n",
              "      <td>0.092102</td>\n",
              "      <td>-0.125210</td>\n",
              "      <td>-0.039361</td>\n",
              "      <td>-0.045234</td>\n",
              "      <td>-0.004466</td>\n",
              "      <td>-0.005018</td>\n",
              "      <td>-0.056888</td>\n",
              "      <td>-0.016968</td>\n",
              "      <td>-0.004015</td>\n",
              "      <td>0.049540</td>\n",
              "      <td>0.030131</td>\n",
              "      <td>0.048677</td>\n",
              "      <td>0.112376</td>\n",
              "      <td>-0.118184</td>\n",
              "      <td>0.075155</td>\n",
              "      <td>-0.032440</td>\n",
              "      <td>-0.157471</td>\n",
              "      <td>0.084554</td>\n",
              "      <td>0.061971</td>\n",
              "      <td>-0.008842</td>\n",
              "      <td>-0.102376</td>\n",
              "      <td>-0.016032</td>\n",
              "      <td>0.018026</td>\n",
              "      <td>-0.093058</td>\n",
              "      <td>0.114014</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.097900</td>\n",
              "      <td>-0.077749</td>\n",
              "      <td>-0.202301</td>\n",
              "      <td>0.066462</td>\n",
              "      <td>0.010315</td>\n",
              "      <td>0.037567</td>\n",
              "      <td>-0.062113</td>\n",
              "      <td>-0.016734</td>\n",
              "      <td>0.035411</td>\n",
              "      <td>-0.215027</td>\n",
              "      <td>0.166280</td>\n",
              "      <td>0.173828</td>\n",
              "      <td>-0.040609</td>\n",
              "      <td>0.025513</td>\n",
              "      <td>-0.039469</td>\n",
              "      <td>-0.029968</td>\n",
              "      <td>-0.024277</td>\n",
              "      <td>-0.201660</td>\n",
              "      <td>-0.081136</td>\n",
              "      <td>0.205729</td>\n",
              "      <td>0.055949</td>\n",
              "      <td>-0.103602</td>\n",
              "      <td>0.048035</td>\n",
              "      <td>0.013550</td>\n",
              "      <td>0.039266</td>\n",
              "      <td>0.135132</td>\n",
              "      <td>-0.061584</td>\n",
              "      <td>-0.073730</td>\n",
              "      <td>-0.134725</td>\n",
              "      <td>-0.194756</td>\n",
              "      <td>-0.100952</td>\n",
              "      <td>-0.008748</td>\n",
              "      <td>-0.010335</td>\n",
              "      <td>-0.079671</td>\n",
              "      <td>0.058400</td>\n",
              "      <td>0.106242</td>\n",
              "      <td>-0.007507</td>\n",
              "      <td>0.139437</td>\n",
              "      <td>-0.105672</td>\n",
              "      <td>0.088623</td>\n",
              "      <td>-0.014404</td>\n",
              "      <td>-0.024495</td>\n",
              "      <td>0.004211</td>\n",
              "      <td>-0.132365</td>\n",
              "      <td>0.033783</td>\n",
              "      <td>-0.049057</td>\n",
              "      <td>-0.082535</td>\n",
              "      <td>-0.037354</td>\n",
              "      <td>0.100342</td>\n",
              "      <td>-0.074188</td>\n",
              "      <td>-0.025838</td>\n",
              "      <td>0.146139</td>\n",
              "      <td>0.140951</td>\n",
              "      <td>0.188639</td>\n",
              "      <td>0.015462</td>\n",
              "      <td>0.010213</td>\n",
              "      <td>-0.049723</td>\n",
              "      <td>-0.151118</td>\n",
              "      <td>-0.121053</td>\n",
              "      <td>0.079712</td>\n",
              "      <td>-0.009766</td>\n",
              "      <td>-0.039144</td>\n",
              "      <td>0.002360</td>\n",
              "      <td>0.112203</td>\n",
              "      <td>-0.028544</td>\n",
              "      <td>-0.174967</td>\n",
              "      <td>0.044576</td>\n",
              "      <td>-0.036621</td>\n",
              "      <td>0.075928</td>\n",
              "      <td>0.050334</td>\n",
              "      <td>0.225586</td>\n",
              "      <td>0.010173</td>\n",
              "      <td>-0.019674</td>\n",
              "      <td>-0.036377</td>\n",
              "      <td>-0.045858</td>\n",
              "      <td>-0.078003</td>\n",
              "      <td>-0.144470</td>\n",
              "      <td>0.002563</td>\n",
              "      <td>-0.043396</td>\n",
              "      <td>-0.047526</td>\n",
              "      <td>-0.072795</td>\n",
              "      <td>0.128764</td>\n",
              "      <td>0.018717</td>\n",
              "      <td>-0.142253</td>\n",
              "      <td>-0.053650</td>\n",
              "      <td>0.069539</td>\n",
              "      <td>-0.037516</td>\n",
              "      <td>-0.055806</td>\n",
              "      <td>-0.016073</td>\n",
              "      <td>0.165812</td>\n",
              "      <td>0.059326</td>\n",
              "      <td>-0.067932</td>\n",
              "      <td>-0.014455</td>\n",
              "      <td>-0.161458</td>\n",
              "      <td>-0.130625</td>\n",
              "      <td>0.146383</td>\n",
              "      <td>-0.041812</td>\n",
              "      <td>0.042140</td>\n",
              "      <td>0.005208</td>\n",
              "      <td>0.030622</td>\n",
              "      <td>-0.086589</td>\n",
              "      <td>-0.014771</td>\n",
              "      <td>0.020426</td>\n",
              "      <td>0.109375</td>\n",
              "      <td>-0.023234</td>\n",
              "      <td>-0.054057</td>\n",
              "      <td>0.033997</td>\n",
              "      <td>0.064173</td>\n",
              "      <td>0.056193</td>\n",
              "      <td>-0.034139</td>\n",
              "      <td>0.051158</td>\n",
              "      <td>-0.026057</td>\n",
              "      <td>0.071899</td>\n",
              "      <td>-0.019063</td>\n",
              "      <td>-0.080526</td>\n",
              "      <td>-0.101440</td>\n",
              "      <td>0.091370</td>\n",
              "      <td>-0.068603</td>\n",
              "      <td>0.192301</td>\n",
              "      <td>0.017049</td>\n",
              "      <td>-0.013547</td>\n",
              "      <td>0.117269</td>\n",
              "      <td>0.069351</td>\n",
              "      <td>0.060455</td>\n",
              "      <td>-0.226278</td>\n",
              "      <td>0.011121</td>\n",
              "      <td>-0.089945</td>\n",
              "      <td>-0.049449</td>\n",
              "      <td>-0.045812</td>\n",
              "      <td>0.001841</td>\n",
              "      <td>-0.010661</td>\n",
              "      <td>0.032572</td>\n",
              "      <td>-0.044271</td>\n",
              "      <td>0.104187</td>\n",
              "      <td>-0.042969</td>\n",
              "      <td>0.023397</td>\n",
              "      <td>0.051015</td>\n",
              "      <td>-0.037394</td>\n",
              "      <td>-0.049784</td>\n",
              "      <td>-0.174072</td>\n",
              "      <td>-0.022603</td>\n",
              "      <td>-0.075033</td>\n",
              "      <td>-0.042643</td>\n",
              "      <td>0.083221</td>\n",
              "      <td>0.051804</td>\n",
              "      <td>0.191488</td>\n",
              "      <td>0.083567</td>\n",
              "      <td>-0.050090</td>\n",
              "      <td>-0.137573</td>\n",
              "      <td>0.139303</td>\n",
              "      <td>0.016479</td>\n",
              "      <td>-0.057536</td>\n",
              "      <td>-0.018000</td>\n",
              "      <td>-0.081299</td>\n",
              "      <td>-0.048096</td>\n",
              "      <td>-0.064535</td>\n",
              "      <td>0.148913</td>\n",
              "      <td>-0.156893</td>\n",
              "      <td>0.019011</td>\n",
              "      <td>0.171056</td>\n",
              "      <td>-0.147888</td>\n",
              "      <td>-0.071533</td>\n",
              "      <td>-0.052816</td>\n",
              "      <td>-0.074910</td>\n",
              "      <td>-0.126526</td>\n",
              "      <td>0.032046</td>\n",
              "      <td>-0.020020</td>\n",
              "      <td>0.049642</td>\n",
              "      <td>-0.102132</td>\n",
              "      <td>-0.006785</td>\n",
              "      <td>0.110982</td>\n",
              "      <td>-0.052734</td>\n",
              "      <td>0.140055</td>\n",
              "      <td>-0.071777</td>\n",
              "      <td>-0.022461</td>\n",
              "      <td>-0.106486</td>\n",
              "      <td>0.139598</td>\n",
              "      <td>0.009562</td>\n",
              "      <td>0.083903</td>\n",
              "      <td>-0.136271</td>\n",
              "      <td>-0.045227</td>\n",
              "      <td>-0.084066</td>\n",
              "      <td>-0.185547</td>\n",
              "      <td>-0.071838</td>\n",
              "      <td>-0.076701</td>\n",
              "      <td>-0.132988</td>\n",
              "      <td>-0.138468</td>\n",
              "      <td>0.033694</td>\n",
              "      <td>-0.171956</td>\n",
              "      <td>0.023407</td>\n",
              "      <td>0.004903</td>\n",
              "      <td>-0.031570</td>\n",
              "      <td>0.109174</td>\n",
              "      <td>-0.023600</td>\n",
              "      <td>0.150757</td>\n",
              "      <td>-0.019979</td>\n",
              "      <td>-0.019511</td>\n",
              "      <td>-0.033244</td>\n",
              "      <td>0.002116</td>\n",
              "      <td>-0.046936</td>\n",
              "      <td>-0.125366</td>\n",
              "      <td>-0.165771</td>\n",
              "      <td>-0.057454</td>\n",
              "      <td>-0.132843</td>\n",
              "      <td>0.138407</td>\n",
              "      <td>0.047607</td>\n",
              "      <td>0.002848</td>\n",
              "      <td>0.055717</td>\n",
              "      <td>0.056071</td>\n",
              "      <td>0.067505</td>\n",
              "      <td>-0.039805</td>\n",
              "      <td>0.005188</td>\n",
              "      <td>0.049276</td>\n",
              "      <td>0.130452</td>\n",
              "      <td>0.001973</td>\n",
              "      <td>-0.141464</td>\n",
              "      <td>-0.020528</td>\n",
              "      <td>-0.000895</td>\n",
              "      <td>-0.056966</td>\n",
              "      <td>-0.127869</td>\n",
              "      <td>-0.053975</td>\n",
              "      <td>0.219564</td>\n",
              "      <td>-0.056320</td>\n",
              "      <td>0.211263</td>\n",
              "      <td>0.123708</td>\n",
              "      <td>0.099202</td>\n",
              "      <td>-0.116496</td>\n",
              "      <td>-0.060476</td>\n",
              "      <td>-0.119222</td>\n",
              "      <td>-0.142456</td>\n",
              "      <td>-0.029867</td>\n",
              "      <td>-0.000295</td>\n",
              "      <td>-0.021240</td>\n",
              "      <td>0.033529</td>\n",
              "      <td>0.131348</td>\n",
              "      <td>0.122355</td>\n",
              "      <td>0.102196</td>\n",
              "      <td>0.107971</td>\n",
              "      <td>-0.078288</td>\n",
              "      <td>0.032735</td>\n",
              "      <td>0.059082</td>\n",
              "      <td>-0.039642</td>\n",
              "      <td>-0.199178</td>\n",
              "      <td>0.017822</td>\n",
              "      <td>0.014206</td>\n",
              "      <td>-0.036418</td>\n",
              "      <td>-0.086543</td>\n",
              "      <td>0.168376</td>\n",
              "      <td>-0.014648</td>\n",
              "      <td>-0.094889</td>\n",
              "      <td>-0.043864</td>\n",
              "      <td>-0.084798</td>\n",
              "      <td>-0.144979</td>\n",
              "      <td>-0.146200</td>\n",
              "      <td>0.097005</td>\n",
              "      <td>-0.057414</td>\n",
              "      <td>0.021444</td>\n",
              "      <td>0.165527</td>\n",
              "      <td>-0.053385</td>\n",
              "      <td>-0.003255</td>\n",
              "      <td>-0.083171</td>\n",
              "      <td>-0.060465</td>\n",
              "      <td>0.075195</td>\n",
              "      <td>-0.100586</td>\n",
              "      <td>0.208944</td>\n",
              "      <td>0.154622</td>\n",
              "      <td>-0.034342</td>\n",
              "      <td>0.002035</td>\n",
              "      <td>-0.022583</td>\n",
              "      <td>-0.077738</td>\n",
              "      <td>0.023600</td>\n",
              "      <td>0.071777</td>\n",
              "      <td>-0.048442</td>\n",
              "      <td>0.010722</td>\n",
              "      <td>-0.040629</td>\n",
              "      <td>-0.101440</td>\n",
              "      <td>0.057841</td>\n",
              "      <td>0.016947</td>\n",
              "      <td>-0.161906</td>\n",
              "      <td>-0.097087</td>\n",
              "      <td>-0.121745</td>\n",
              "      <td>-0.038757</td>\n",
              "      <td>0.076462</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "algos = {\n",
        "  'RF' : RandomForestRegressor(n_estimators=50,random_state=1,n_jobs=-1),\n",
        "  'KNN' : KNeighborsRegressor(n_neighbors=5,n_jobs=-1,metric='cosine'),\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(20,10),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models(rev_a_title_train_wv_google, y_train, rev_a_title_test_wv_google, y_test.to_numpy(), algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "Sk7gTY2AQQB1",
        "outputId": "10df2fed-a8e1-4ff1-8c49-71e4a6378ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## RF #############\n",
            "MAE = 1.086, Accuracy = 0.242, Customized Accuracy = 0.711\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   6,  690, 1205,   77,    1],\n",
              "       [   0,  442, 1421,  120,    0],\n",
              "       [   0,  243, 1497,  211,    0],\n",
              "       [   0,  124, 1417,  453,    7],\n",
              "       [   0,   87, 1274,  701,   24]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## KNN #############\n",
            "MAE = 1.245, Accuracy = 0.218, Customized Accuracy = 0.629\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  43,  613, 1004,  312,    7],\n",
              "       [  18,  558,  995,  400,   12],\n",
              "       [  17,  468, 1081,  373,   12],\n",
              "       [  29,  398, 1081,  469,   24],\n",
              "       [   9,  377, 1128,  545,   27]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## MLP #############\n",
            "MAE = 1.044, Accuracy = 0.317, Customized Accuracy = 0.748\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[467, 825, 373, 229,  85],\n",
              "       [228, 754, 531, 348, 122],\n",
              "       [119, 534, 642, 504, 152],\n",
              "       [ 56, 319, 588, 679, 359],\n",
              "       [ 50, 243, 428, 736, 629]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les performances sont inférieurs a son équivalent dans la partie précédente."
      ],
      "metadata": {
        "id": "O6r9VilURyjw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Courbe d'apprentissage du meilleur modèle"
      ],
      "metadata": {
        "id": "ptUVcVPRQzKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# utiliser la fonction learning_curve\n",
        "train_sizes, train_scores, test_scores = learning_curve(\\\n",
        "  algos[\"MLP\"], pd.concat([rev_a_title_train_wv_google, rev_a_title_test_wv_google]),\\\n",
        "  pd.concat([y_train, y_test]))\n",
        "\n",
        "# tracer les courbes d'apprentissage\n",
        "plt.figure()\n",
        "plt.title(\"Courbe d'apprentissage\")\n",
        "plt.xlabel(\"Taille de l'échantillon d'entraînement\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.plot(train_sizes, train_scores.mean(axis=1), 'o-', color=\"r\", label=\"Score d'entraînement\")\n",
        "plt.plot(train_sizes, test_scores.mean(axis=1), 'o-', color=\"g\", label=\"Score de validation croisée\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5aGumOjaQ0iV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "c4771c42-3b6e-41b0-c3bf-428b4e7e1c5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous voyons qu'il n'y a presque pas besoin de plus de données pour l'entraînement."
      ],
      "metadata": {
        "id": "sA4fOrxjVxu0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Amélioration du meilleur modèle"
      ],
      "metadata": {
        "id": "dpfnnqbPR-UV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plus de neurones\n",
        "algos = {\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(40, 20),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models(rev_a_title_train_wv_google, y_train, rev_a_title_test_wv_google, y_test.to_numpy(), algos)"
      ],
      "metadata": {
        "id": "zuNAY5f7Qf9S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "4bfbac12-69c0-4f46-9366-f27c4f4632e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## MLP #############\n",
            "MAE = 1.174, Accuracy = 0.293, Customized Accuracy = 0.689\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[722, 558, 336, 223, 140],\n",
              "       [467, 535, 437, 357, 187],\n",
              "       [299, 460, 463, 486, 243],\n",
              "       [202, 335, 463, 565, 436],\n",
              "       [162, 255, 376, 649, 644]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "C'est pas mieux."
      ],
      "metadata": {
        "id": "Whk9UUiBYbZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Moins de neurones\n",
        "algos = {\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(10, 5),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models(rev_a_title_train_wv_google, y_train, rev_a_title_test_wv_google, y_test.to_numpy(), algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "10aT4gqRSh5o",
        "outputId": "749a513c-5953-4ef7-f796-3ddbf832369e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## MLP #############\n",
            "MAE = 0.981, Accuracy = 0.310, Customized Accuracy = 0.787\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[ 365,  934,  477,  182,   21],\n",
              "       [ 177,  731,  704,  346,   25],\n",
              "       [  61,  531,  772,  546,   41],\n",
              "       [  33,  253,  685,  881,  149],\n",
              "       [  38,  200,  448, 1047,  353]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Petite amélioration."
      ],
      "metadata": {
        "id": "Nu3ykmjbY0dC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### II. Prise en compte de la longueur, de la spécificité et de la subjectivité du texte"
      ],
      "metadata": {
        "id": "dvUWQf4M3nJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul de la longueur, la spécificité et la subjectivité\n",
        "df_meta_train=pd.DataFrame([])\n",
        "df_meta_train['length_text'] = [len(reviews_train_tokens.iloc[i]) for i in range(len(reviews_train_tokens))]\n",
        "df_meta_train['polarity'] = [polarity_txt(X_train[\"reviewText\"].iloc[i]) for i in range(len(X_train))]\n",
        "df_meta_train['subjectivity'] = [subj_txt(X_train[\"reviewText\"].iloc[i]) for i in range(len(X_train))]\n",
        "\n",
        "df_meta_test=pd.DataFrame([])\n",
        "df_meta_test['length_text'] = [len(reviews_test_tokens.iloc[i]) for i in range(len(reviews_test_tokens))]\n",
        "df_meta_test['polarity'] = [polarity_txt(X_test[\"reviewText\"].iloc[i]) for i in range(len(X_test))]\n",
        "df_meta_test['subjectivity'] = [subj_txt(X_test[\"reviewText\"].iloc[i]) for i in range(len(X_test))]"
      ],
      "metadata": {
        "id": "GereJ5vqV_9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concaténation\n",
        "corpus_train_wv_google_meta=pd.concat([reviews_train_wv_google,title_train_wv_google,df_meta_train],axis=1)\n",
        "corpus_test_wv_google_meta=pd.concat([reviews_test_wv_google,title_test_wv_google,df_meta_test],axis=1)"
      ],
      "metadata": {
        "id": "cSMDJoCiWPei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entraînement et évaluation\n",
        "algos = {\n",
        "  'RF' : RandomForestRegressor(n_estimators=50,random_state=1,n_jobs=-1),\n",
        "  'KNN' : KNeighborsRegressor(n_neighbors=5,n_jobs=-1,metric='cosine'),\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(20,10),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models(corpus_train_wv_google_meta,y_train,corpus_test_wv_google_meta,y_test.to_numpy(),algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "UzdiU1yvWSkR",
        "outputId": "15b87065-d578-410a-c0f9-9725aec0cfb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## RF #############\n",
            "MAE = 0.985, Accuracy = 0.279, Customized Accuracy = 0.781\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  87, 1016,  752,  123,    1],\n",
              "       [  25,  736, 1028,  194,    0],\n",
              "       [  10,  421, 1149,  367,    4],\n",
              "       [   0,  192, 1060,  731,   18],\n",
              "       [   3,  113,  798, 1082,   90]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## KNN #############\n",
            "MAE = 1.157, Accuracy = 0.243, Customized Accuracy = 0.684\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  64,  671,  944,  287,   13],\n",
              "       [  42,  554, 1013,  359,   15],\n",
              "       [  20,  397, 1058,  450,   26],\n",
              "       [  11,  318,  991,  617,   64],\n",
              "       [   5,  256,  901,  788,  136]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## MLP #############\n",
            "MAE = 0.944, Accuracy = 0.338, Customized Accuracy = 0.793\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[484, 869, 443, 147,  36],\n",
              "       [210, 787, 678, 273,  35],\n",
              "       [ 87, 537, 819, 446,  62],\n",
              "       [ 40, 256, 691, 812, 202],\n",
              "       [ 33, 158, 500, 917, 478]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les modèles donnent des meilleurs performance que'avec juste les embeddings sans les méta-données, le Random Forest donne des meilleurs résultats que sans l'utilisation du titre, le KNN est pire que sans les titres et le MLP est meilleurs avec les titres."
      ],
      "metadata": {
        "id": "G59UUampZfig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Amélioration par rapports au précédent."
      ],
      "metadata": {
        "id": "D8tEN_8bX-gH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### III. Prise en compte de l'ordre des mots (LSTM)"
      ],
      "metadata": {
        "id": "ClG8YXdza1LC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concaténation des reviews et des titres\n",
        "title_reviews_train_tokens = pd.Series([title_train_tokens[i] + reviews_train_tokens[i] for i in reviews_train_tokens.index])\n",
        "title_reviews_test_tokens = pd.Series([title_test_tokens[i] + reviews_test_tokens[i] for i in reviews_test_tokens.index])"
      ],
      "metadata": {
        "id": "ph1OlOEjZRum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_train_tokens.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYtLr9dmlUOm",
        "outputId": "82e15212-32a1-4c76-e46e-8a03a91a0697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "595364    [good, crib, live, room, temp, portabl, sleep,...\n",
              "533385    [the, screen, fit, perfect, slightli, difficul...\n",
              "500429    [like, prize, husband, happi, sai, confterbl, ...\n",
              "Name: reviewText, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_sequences = [to_sequence(word2idx, x) for x in title_reviews_train_tokens]\n",
        "X_test_sequences = [to_sequence(word2idx, x) for x in  title_reviews_test_tokens]"
      ],
      "metadata": {
        "id": "_HyQIIc8kLG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGHT=75\n",
        "N_FEATURES = len(model_google_vec_neg_300.index_to_key)\n",
        "X_train_sequences = pad_sequences(X_train_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)\n",
        "X_test_sequences = pad_sequences(X_test_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)"
      ],
      "metadata": {
        "id": "EfGHTyL1kZ0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDINGS_LEN = model_google_vec_neg_300.vector_size\n",
        "embeddings_index = np.zeros((len(model_google_vec_neg_300.index_to_key)+1, EMBEDDINGS_LEN))\n",
        "embeddings_index.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGe0yEu7kn6u",
        "outputId": "95323ed0-182d-4205-eb9a-ea2b77dcb931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000001, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word, idx in word2idx.items():\n",
        "    try:\n",
        "        embedding = model_google_vec_neg_300[word]\n",
        "        embeddings_index[idx] = embedding\n",
        "    except:\n",
        "        pass\n",
        "      \n",
        "print(\"EMBEDDINGS_LEN=\", EMBEDDINGS_LEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LkyVIh1kofm",
        "outputId": "726f4e77-4bf2-4a86-e6af-a0528b31420f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EMBEDDINGS_LEN= 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(len(model_google_vec_neg_300.key_to_index)+1,\n",
        "                    EMBEDDINGS_LEN,  # Embedding size\n",
        "                    weights=[embeddings_index],\n",
        "                    trainable=False))\n",
        "\n",
        "model_lstm.add(LSTM(300, dropout=0.2))\n",
        "\n",
        "model_lstm.add(Dense(1))\n",
        "model_lstm.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "model_lstm.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2jdbmVtkrn6",
        "outputId": "982b6f59-5aab-49c9-ceea-d31018531eda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, None, 300)         900000300 \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 300)               721200    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 301       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 900,721,801\n",
            "Trainable params: 721,501\n",
            "Non-trainable params: 900,000,300\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_lstm.fit(X_train_sequences, y_train, epochs=10, batch_size=128, verbose=1,validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVmBMgYZkxgO",
        "outputId": "99ddfdff-66d1-4a10-c788-6cc2b02d3881"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "282/282 [==============================] - 60s 207ms/step - loss: 1.0919 - mean_absolute_error: 1.0919 - val_loss: 0.9751 - val_mean_absolute_error: 0.9751\n",
            "Epoch 2/10\n",
            "282/282 [==============================] - 56s 199ms/step - loss: 0.9774 - mean_absolute_error: 0.9774 - val_loss: 0.9405 - val_mean_absolute_error: 0.9405\n",
            "Epoch 3/10\n",
            "282/282 [==============================] - 56s 200ms/step - loss: 0.9317 - mean_absolute_error: 0.9317 - val_loss: 0.9104 - val_mean_absolute_error: 0.9104\n",
            "Epoch 4/10\n",
            "282/282 [==============================] - 56s 200ms/step - loss: 0.8956 - mean_absolute_error: 0.8956 - val_loss: 0.8791 - val_mean_absolute_error: 0.8791\n",
            "Epoch 5/10\n",
            "282/282 [==============================] - 56s 199ms/step - loss: 0.8713 - mean_absolute_error: 0.8713 - val_loss: 0.8650 - val_mean_absolute_error: 0.8650\n",
            "Epoch 6/10\n",
            "282/282 [==============================] - 56s 198ms/step - loss: 0.8537 - mean_absolute_error: 0.8537 - val_loss: 0.8545 - val_mean_absolute_error: 0.8545\n",
            "Epoch 7/10\n",
            "282/282 [==============================] - 55s 195ms/step - loss: 0.8340 - mean_absolute_error: 0.8340 - val_loss: 0.8533 - val_mean_absolute_error: 0.8533\n",
            "Epoch 8/10\n",
            "282/282 [==============================] - 55s 194ms/step - loss: 0.8132 - mean_absolute_error: 0.8132 - val_loss: 0.8378 - val_mean_absolute_error: 0.8378\n",
            "Epoch 9/10\n",
            "282/282 [==============================] - 55s 195ms/step - loss: 0.7952 - mean_absolute_error: 0.7952 - val_loss: 0.8370 - val_mean_absolute_error: 0.8370\n",
            "Epoch 10/10\n",
            "282/282 [==============================] - 55s 196ms/step - loss: 0.7770 - mean_absolute_error: 0.7770 - val_loss: 0.8332 - val_mean_absolute_error: 0.8332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores_lstm = model_lstm.evaluate(X_test_sequences, y_test, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpvUq4xbk4iV",
        "outputId": "22dbde48-09b8-4a3f-a3fb-5f6290e751fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 11s 35ms/step - loss: 0.8544 - mean_absolute_error: 0.8544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction=model_lstm.predict(X_test_sequences)\n",
        "prediction[prediction<1]=1\n",
        "prediction[prediction>5]=5\n",
        "ACC=accuracy_score(y_test,np.round(prediction))\n",
        "MAE=mean_absolute_error(y_test,prediction)\n",
        "CACC=customized_accuracy(y_test.to_numpy(), np.round(prediction))\n",
        "print('For LSTM MAE = {0:.3f}, Accuracy = {1:.3f}, Customized Accuracy = {2:.3f}'.format(MAE,ACC,CACC))\n",
        "display(confusion_matrix(y_test,np.round(prediction)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "rr2-jGZgnX2B",
        "outputId": "6ad95e14-822d-492b-e0ca-5128fcb6453f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 11s 35ms/step\n",
            "For LSTM MAE = 0.847, Accuracy = 0.399, Customized Accuracy = 0.830\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[730, 682, 342, 186,  39],\n",
              "       [285, 747, 605, 304,  42],\n",
              "       [ 76, 470, 749, 561,  95],\n",
              "       [ 18, 156, 536, 905, 386],\n",
              "       [ 21, 124, 300, 781, 860]])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modèle meilleur que son équivalent dans la partie précédente."
      ],
      "metadata": {
        "id": "80RaSL1J_GEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### IV. Utilisation de la sortie du LSTM dans d'autres modèles seupervisés"
      ],
      "metadata": {
        "id": "FGlZkFq1gQLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "Model_rf = Model(inputs=model_lstm.inputs, outputs=model_lstm.layers[1].output)\n",
        "#Model_rf.save('../models/model_lstm_features.h5')\n",
        "corpus_train_lstm = Model_rf.predict(X_train_sequences)\n",
        "corpus_test_lstm = Model_rf.predict(X_test_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EKJ7-MpgVaD",
        "outputId": "b8507fe3-05d0-4a61-8e99-ad2748b525ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1250/1250 [==============================] - 44s 35ms/step\n",
            "313/313 [==============================] - 11s 35ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "algos = {\n",
        "  'RF' : RandomForestRegressor(n_estimators=50,random_state=1,n_jobs=-1),\n",
        "  'KNN' : KNeighborsRegressor(n_neighbors=5,n_jobs=-1,metric='cosine'),\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(20,10),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models(corpus_train_lstm,y_train,corpus_test_lstm,y_test.to_numpy(),algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "vbKcArLFgWLZ",
        "outputId": "d9d042db-8bea-4b3e-cf15-e268153deb94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## RF #############\n",
            "MAE = 0.867, Accuracy = 0.365, Customized Accuracy = 0.835\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[ 492,  936,  394,  138,   19],\n",
              "       [ 156,  902,  687,  227,   11],\n",
              "       [  40,  514,  925,  447,   25],\n",
              "       [   5,  192,  736,  935,  133],\n",
              "       [   9,  144,  448, 1092,  393]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## KNN #############\n",
            "MAE = 0.898, Accuracy = 0.362, Customized Accuracy = 0.813\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[572, 790, 391, 192,  34],\n",
              "       [243, 743, 651, 305,  41],\n",
              "       [ 78, 482, 779, 527,  85],\n",
              "       [ 27, 177, 608, 895, 294],\n",
              "       [ 19, 136, 383, 919, 629]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## MLP #############\n",
            "MAE = 0.904, Accuracy = 0.365, Customized Accuracy = 0.813\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[ 665,  696,  402,  192,   24],\n",
              "       [ 291,  713,  684,  269,   26],\n",
              "       [ 108,  445,  837,  516,   45],\n",
              "       [  45,  184,  583, 1009,  180],\n",
              "       [  40,  142,  393, 1084,  427]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aucune amélioration."
      ],
      "metadata": {
        "id": "iIoDKS9jgQ1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Modèle qui ne prédit que l'utilité"
      ],
      "metadata": {
        "id": "f5rHukToY_Xb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A. Préparation du corpus et des labels"
      ],
      "metadata": {
        "id": "ZWTzSJrxpIP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I. Sélection d'un certain nombre de 0 et de 1 et de tous les autres"
      ],
      "metadata": {
        "id": "o9tAvumGGsHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Renvoie n lignes du dataset avec une utilité égal à helpful\n",
        "def data_sample_by_helpful(data, n, helpful):\n",
        "  tmp_data = data[data['helpful'] == helpful]\n",
        "  idx = list(tmp_data.index)\n",
        "  idx = sample(range(len(idx)), min(n, len(idx)))\n",
        "  return tmp_data.iloc[idx]"
      ],
      "metadata": {
        "id": "pb-Dtb2JGqnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Mettre 20000 dans la version finale si pas suffisamment de données\n",
        "nb_line_per_category = 15000"
      ],
      "metadata": {
        "id": "WmNPoVfSHddK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset utilisé pour la prédiction de l'utilité\n",
        "helpful_df = pd.concat([data_sample_by_helpful(df, nb_line_per_category, i) for i in df['helpful'].unique()])\n",
        "# Vérification de la taille\n",
        "helpful_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tDPq5FtHu-O",
        "outputId": "92c14808-4be3-43e1-a2c0-84b25fcbf7f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(102737, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### II. Split du dataset"
      ],
      "metadata": {
        "id": "GKB6dmhvICRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Affectation et transformation\n",
        "Y = helpful_df['helpful']\n",
        "X = helpful_df[['title', 'reviewText']]"
      ],
      "metadata": {
        "id": "iOGLbbdNIGPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution des valeurs de l'utilité\n",
        "plt.hist(Y)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "-3oRUCpAIPs0",
        "outputId": "3f3700c1-e7c9-4378-c6c2-797cd37cf543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqp7L5rjMRgt",
        "outputId": "b66c0137-3504-4cf5-c4b7-eb741fcb496a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     15000\n",
              "10    15000\n",
              "5     15000\n",
              "8     15000\n",
              "7     12636\n",
              "9     11328\n",
              "3      5323\n",
              "6      4891\n",
              "2      3976\n",
              "4      2907\n",
              "1      1676\n",
              "Name: helpful, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,shuffle=True,random_state=1)"
      ],
      "metadata": {
        "id": "Tu9GrD28JZ4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_train_tokens = X_train['reviewText'].apply(lambda line : preprocess_text(line))\n",
        "reviews_test_tokens = X_test['reviewText'].apply(lambda line : preprocess_text(line))"
      ],
      "metadata": {
        "id": "Efju436CJcqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title_train_tokens = X_train['title'].apply(lambda line : preprocess_text(line))\n",
        "title_test_tokens = X_test['title'].apply(lambda line : preprocess_text(line))"
      ],
      "metadata": {
        "id": "nJ39SEDlJfTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B. Utilisation que du commentaire"
      ],
      "metadata": {
        "id": "Yy2ZcgjDbkwm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I. Moyenne des embeddings"
      ],
      "metadata": {
        "id": "4EL96njhbm5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul de score basé sur le seuillage binaire de l'utilité\n",
        "def binary_tresholding_score(y_true, y_pred, t = 6):\n",
        "  return mean([(1 if (y_true[i] < t and y_pred[i] < t) or (y_true[i] >= t and y_pred[i] >= t) else 0) for i in range(len(y_true))])"
      ],
      "metadata": {
        "id": "7IqfeVP8NeCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exécuter les algos en paramètre sur les données en utilisant en plus le score basé sur le seuillage binaire\n",
        "def run_models_with_binary_tresholding_score(X_train,Y_train,X_test,Y_test,algos):\n",
        "    for algo_name in algos:\n",
        "        model=algos[algo_name]\n",
        "        model.fit(X_train,Y_train)\n",
        "        prediction=model.predict(X_test)\n",
        "        prediction[prediction<1]=1\n",
        "        prediction[prediction>10]=10\n",
        "        MAE=mean_absolute_error(Y_test,prediction)\n",
        "        ACC=accuracy_score(Y_test,np.round(prediction))\n",
        "        CACC=customized_accuracy(Y_test,np.round(prediction),2)\n",
        "        BTS=binary_tresholding_score(Y_test,np.round(prediction))\n",
        "        \n",
        "        print('################## {0} #############'.format(algo_name))\n",
        "        print('MAE = {0:.3f}, Accuracy = {1:.3f}, Customized Accuracy = {2:.3f}, Binary Thresholding Score = {3:.3f}'.format(MAE,ACC, CACC, BTS))\n",
        "        display(confusion_matrix(Y_test,np.round(prediction)))\n",
        "        print()"
      ],
      "metadata": {
        "id": "5w1iTgYQO7UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul de de l'embedding des phrases\n",
        "vector_size=model_google_vec_neg_300.vector_size\n",
        "corpus_train_wv_google=word2vec_generator(reviews_train_tokens,model_google_vec_neg_300,vector_size)\n",
        "corpus_test_wv_google=word2vec_generator(reviews_test_tokens,model_google_vec_neg_300,vector_size)"
      ],
      "metadata": {
        "id": "N06L_8oOJxlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modèles\n",
        "algos = {\n",
        "  'RF' : RandomForestRegressor(n_estimators=50,random_state=1,n_jobs=-1),\n",
        "  'KNN' : KNeighborsRegressor(n_neighbors=5,n_jobs=-1,metric='cosine'),\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(20,10),max_iter=800,random_state=1,alpha=0.001)\n",
        "}"
      ],
      "metadata": {
        "id": "K6SK7A6PJ6kZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_models_with_binary_tresholding_score(corpus_train_wv_google, y_train, corpus_test_wv_google, y_test.to_numpy(), algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "id": "ZCxpPGdOJ_iK",
        "outputId": "724af696-a48d-4f61-c482-9c9fdfbd2102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## RF #############\n",
            "MAE = 2.705, Accuracy = 0.106, Customized Accuracy = 0.507, Binary Thresholding Score = 0.613\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,    4,    2,   59,  613, 1245,  854,  310,   25,    0,    0],\n",
              "       [   0,    1,    0,    2,   56,  142,  128,   36,    2,    0,    0],\n",
              "       [   0,    1,    0,    8,  110,  248,  297,  125,   14,    0,    0],\n",
              "       [   0,    0,    0,    4,  113,  349,  367,  177,   17,    0,    0],\n",
              "       [   0,    0,    0,    4,   50,  147,  228,  117,   13,    0,    0],\n",
              "       [   0,    0,    1,   24,  351,  933, 1009,  557,   85,    0,    0],\n",
              "       [   0,    0,    0,    3,   54,  222,  367,  251,   41,    1,    0],\n",
              "       [   0,    0,    0,   12,  227,  621,  896,  656,   85,    0,    0],\n",
              "       [   0,    0,    0,   12,  191,  652, 1106,  902,  169,    0,    0],\n",
              "       [   0,    0,    0,    5,   70,  358,  790,  792,  233,    3,    0],\n",
              "       [   0,    0,    1,   24,  296,  851, 1024,  676,  129,    0,    0]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## KNN #############\n",
            "MAE = 2.831, Accuracy = 0.118, Customized Accuracy = 0.541, Binary Thresholding Score = 0.583\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  0,  19,  48, 113, 202, 391, 657, 779, 660, 235,   8],\n",
              "       [  0,   0,   1,  10,  26,  56,  98,  84,  70,  22,   0],\n",
              "       [  0,   2,   7,  19,  46, 132, 167, 207, 161,  61,   1],\n",
              "       [  0,   5,   1,  17,  51, 124, 241, 291, 220,  75,   2],\n",
              "       [  0,   0,   2,  10,  24,  65, 124, 172, 121,  40,   1],\n",
              "       [  0,   6,  23,  59, 146, 348, 618, 816, 703, 235,   6],\n",
              "       [  0,   1,   2,   7,  32, 103, 186, 256, 269,  81,   2],\n",
              "       [  0,   1,  12,  32,  99, 246, 516, 692, 675, 220,   4],\n",
              "       [  0,   0,   8,  26,  97, 248, 550, 930, 854, 315,   4],\n",
              "       [  0,   3,   1,  15,  48, 160, 390, 646, 698, 282,   8],\n",
              "       [  0,   8,  15,  54, 153, 324, 545, 829, 779, 284,  10]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## MLP #############\n",
            "MAE = 2.688, Accuracy = 0.106, Customized Accuracy = 0.536, Binary Thresholding Score = 0.621\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,   50,   88,  210,  501,  733,  848,  572,  108,    2,    0],\n",
              "       [   0,    9,    5,   20,   54,   91,  122,   55,   11,    0,    0],\n",
              "       [   0,   10,   20,   42,   93,  185,  236,  194,   23,    0,    0],\n",
              "       [   0,    9,   12,   46,  105,  222,  327,  258,   48,    0,    0],\n",
              "       [   0,    4,    4,   16,   33,  120,  198,  156,   25,    3,    0],\n",
              "       [   0,   24,   50,  117,  275,  604,  907,  828,  153,    2,    0],\n",
              "       [   0,    4,    6,   18,   56,  133,  312,  342,   66,    2,    0],\n",
              "       [   0,   10,   24,   63,  155,  413,  835,  812,  178,    7,    0],\n",
              "       [   0,   11,   23,   61,  152,  402,  923, 1130,  321,    9,    0],\n",
              "       [   0,    6,    6,   17,   62,  189,  604, 1024,  329,   14,    0],\n",
              "       [   0,   15,   29,  113,  216,  531,  894,  927,  274,    2,    0]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Courbe d'apprentissage du meilleur modèle"
      ],
      "metadata": {
        "id": "qGMdRNLOKMJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "# utiliser la fonction learning_curve\n",
        "train_sizes, train_scores, test_scores = learning_curve(\\\n",
        "  algos[\"MLP\"], pd.concat([corpus_train_wv_google, corpus_test_wv_google]),\\\n",
        "  pd.concat([y_train, y_test]), verbose=1)\n",
        "\n",
        "# tracer les courbes d'apprentissage\n",
        "plt.figure()\n",
        "plt.title(\"Courbe d'apprentissage\")\n",
        "plt.xlabel(\"Taille de l'échantillon d'entraînement\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.plot(train_sizes, train_scores.mean(axis=1), 'o-', color=\"r\", label=\"Score d'entraînement\")\n",
        "plt.plot(train_sizes, test_scores.mean(axis=1), 'o-', color=\"g\", label=\"Score de validation croisée\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "I8grE3iuMqe3",
        "outputId": "80db3878-92f0-4c4e-9c0e-788032ac4cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[learning_curve] Training set sizes: [ 8218 26711 45203 63696 82189]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed: 38.6min finished\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il y a suffisament de données."
      ],
      "metadata": {
        "id": "Q9FdR0pXsMgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Amélioration du meilleur modèle"
      ],
      "metadata": {
        "id": "JB-DjyfSMxNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plus de neurones\n",
        "algos = {\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(40, 20),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models_with_binary_tresholding_score(corpus_train_wv_google, y_train, corpus_test_wv_google, y_test.to_numpy(), algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "EZsxw4d7M1FG",
        "outputId": "51563509-0372-415b-f2ec-91f9522b09a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## MLP #############\n",
            "MAE = 2.840, Accuracy = 0.109, Customized Accuracy = 0.532, Binary Thresholding Score = 0.593\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  0, 251, 151, 213, 324, 427, 573, 636, 347, 134,  56],\n",
              "       [  0,  24,   9,  20,  41,  71, 100,  68,  25,   6,   3],\n",
              "       [  0,  42,  27,  37,  69, 118, 207, 206,  74,  15,   8],\n",
              "       [  0,  48,  30,  46,  89, 137, 249, 273, 118,  30,   7],\n",
              "       [  0,  21,  13,  13,  30,  94, 136, 169,  69,  12,   2],\n",
              "       [  0, 129,  76, 144, 232, 367, 689, 783, 395, 107,  38],\n",
              "       [  0,  25,  17,  31,  55, 101, 227, 298, 139,  34,  12],\n",
              "       [  0,  80,  44,  94, 142, 290, 579, 766, 390,  84,  28],\n",
              "       [  0,  91,  45,  83, 143, 341, 631, 973, 583, 118,  24],\n",
              "       [  0,  38,  30,  39,  88, 178, 414, 759, 549, 125,  31],\n",
              "       [  0, 142,  71, 136, 202, 317, 568, 829, 532, 158,  46]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pas d'amélioration."
      ],
      "metadata": {
        "id": "2xL6pcfd_yZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Moins de neurones\n",
        "algos = {\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(10, 5),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models_with_binary_tresholding_score(corpus_train_wv_google, y_train, corpus_test_wv_google, y_test.to_numpy(), algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "0bjQu81mM4gL",
        "outputId": "8cd0e0eb-2ce8-47d3-d741-ae90bbca0f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## MLP #############\n",
            "MAE = 2.715, Accuracy = 0.102, Customized Accuracy = 0.519, Binary Thresholding Score = 0.609\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  0,  39,  72, 221, 571, 824, 815, 459, 100,  10,   1],\n",
              "       [  0,   6,  10,  22,  52, 103, 114,  47,  13,   0,   0],\n",
              "       [  0,   5,  12,  32, 110, 225, 246, 153,  20,   0,   0],\n",
              "       [  0,   7,  14,  32, 134, 258, 310, 227,  44,   1,   0],\n",
              "       [  0,   1,   7,  21,  39, 130, 193, 142,  22,   4,   0],\n",
              "       [  0,  13,  31, 110, 357, 694, 949, 668, 132,   5,   1],\n",
              "       [  0,   0,   7,  21,  75, 189, 329, 260,  52,   5,   1],\n",
              "       [  0,   4,  17,  72, 223, 497, 832, 700, 146,   6,   0],\n",
              "       [  0,   3,  15,  67, 184, 527, 991, 967, 263,  14,   1],\n",
              "       [  0,   3,   6,  28,  81, 278, 685, 865, 286,  19,   0],\n",
              "       [  0,   9,  27,  96, 317, 614, 888, 794, 247,   8,   1]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pas d'amélioration."
      ],
      "metadata": {
        "id": "grv6ZOR6vbzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### II. Prise en compte de la longueur, de la spécificité et de la subjectivité du texte"
      ],
      "metadata": {
        "id": "qxfCBP0K3wdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul de la longueur, la spécificité et la subjectivité\n",
        "df_meta_train=pd.DataFrame([])\n",
        "df_meta_train['length_text'] = [len(reviews_train_tokens.iloc[i]) for i in range(len(reviews_train_tokens))]\n",
        "df_meta_train['polarity'] = [polarity_txt(X_train[\"reviewText\"].iloc[i]) for i in range(len(X_train))]\n",
        "df_meta_train['subjectivity'] = [subj_txt(X_train[\"reviewText\"].iloc[i]) for i in range(len(X_train))]\n",
        "\n",
        "df_meta_test=pd.DataFrame([])\n",
        "df_meta_test['length_text'] = [len(reviews_test_tokens.iloc[i]) for i in range(len(reviews_test_tokens))]\n",
        "df_meta_test['polarity'] = [polarity_txt(X_test[\"reviewText\"].iloc[i]) for i in range(len(X_test))]\n",
        "df_meta_test['subjectivity'] = [subj_txt(X_test[\"reviewText\"].iloc[i]) for i in range(len(X_test))]"
      ],
      "metadata": {
        "id": "0AnWR-DOSbnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concaténation\n",
        "corpus_train_wv_google_meta=pd.concat([corpus_train_wv_google,df_meta_train],axis=1)\n",
        "corpus_test_wv_google_meta=pd.concat([corpus_test_wv_google,df_meta_test],axis=1)"
      ],
      "metadata": {
        "id": "t6CIC6gWStrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entraînement\n",
        "algos = {\n",
        "  'RF' : RandomForestRegressor(n_estimators=50,random_state=1,n_jobs=-1),\n",
        "  'KNN' : KNeighborsRegressor(n_neighbors=5,n_jobs=-1,metric='cosine'),\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(20,10),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models_with_binary_tresholding_score(corpus_train_wv_google_meta,y_train,corpus_test_wv_google_meta,y_test.to_numpy(),algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "id": "ILp9iXLeTdEX",
        "outputId": "0f538171-9a83-45f2-cc58-105e99171dc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## RF #############\n",
            "MAE = 2.691, Accuracy = 0.103, Customized Accuracy = 0.513, Binary Thresholding Score = 0.619\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,   16,   10,  105,  705, 1071,  834,  337,   34,    0,    0],\n",
              "       [   0,    2,    3,    7,   57,  132,  125,   39,    2,    0,    0],\n",
              "       [   0,    0,    2,   11,  122,  256,  266,  130,   16,    0,    0],\n",
              "       [   0,    0,    0,   16,  128,  303,  374,  185,   21,    0,    0],\n",
              "       [   0,    1,    0,    6,   58,  142,  207,  124,   21,    0,    0],\n",
              "       [   0,    2,    4,   54,  367,  830, 1078,  535,   90,    0,    0],\n",
              "       [   0,    0,    0,    8,   55,  184,  381,  267,   44,    0,    0],\n",
              "       [   0,    1,    0,   14,  204,  601,  921,  648,  108,    0,    0],\n",
              "       [   0,    0,    0,   21,  187,  611, 1114,  914,  183,    2,    0],\n",
              "       [   0,    0,    0,    3,   62,  311,  805,  814,  253,    3,    0],\n",
              "       [   0,    2,    2,   31,  362,  761,  986,  700,  157,    0,    0]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## KNN #############\n",
            "MAE = 2.805, Accuracy = 0.108, Customized Accuracy = 0.516, Binary Thresholding Score = 0.586\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  0,  59, 146, 273, 493, 610, 630, 554, 260,  81,   6],\n",
              "       [  0,   4,  11,  33,  57,  73,  72,  71,  38,   8,   0],\n",
              "       [  0,   5,  24,  54, 108, 155, 219, 141,  84,  12,   1],\n",
              "       [  0,   5,  31,  55, 107, 185, 249, 238, 125,  29,   3],\n",
              "       [  0,   3,   8,  19,  70,  99, 146, 128,  68,  18,   0],\n",
              "       [  0,  23,  61, 166, 345, 526, 681, 667, 380, 106,   5],\n",
              "       [  0,   2,  10,  29,  87, 168, 202, 238, 161,  40,   2],\n",
              "       [  0,  19,  34, 115, 233, 436, 585, 579, 384, 109,   3],\n",
              "       [  0,  16,  47,  99, 235, 449, 713, 764, 547, 155,   7],\n",
              "       [  0,   2,  11,  58, 135, 274, 491, 593, 488, 193,   6],\n",
              "       [  0,  26,  63, 166, 309, 468, 677, 663, 443, 175,  11]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## MLP #############\n",
            "MAE = 2.710, Accuracy = 0.102, Customized Accuracy = 0.536, Binary Thresholding Score = 0.618\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,  151,  139,  267,  445,  552,  786,  640,  106,   17,    9],\n",
              "       [   0,    9,   12,   25,   50,   94,  119,   48,    9,    1,    0],\n",
              "       [   0,   19,   17,   55,   90,  180,  275,  146,   18,    2,    1],\n",
              "       [   0,   26,   15,   55,  101,  206,  336,  262,   23,    3,    0],\n",
              "       [   0,    7,   14,   15,   36,  118,  187,  161,   17,    4,    0],\n",
              "       [   0,   56,   56,  143,  260,  571,  937,  813,  109,   13,    2],\n",
              "       [   0,   11,   11,   19,   55,  124,  319,  349,   49,    1,    1],\n",
              "       [   0,   28,   36,   79,  157,  402,  820,  864,  107,    3,    1],\n",
              "       [   0,   33,   31,   59,  142,  397,  944, 1212,  204,    8,    2],\n",
              "       [   0,   16,   16,   29,   59,  175,  645, 1091,  209,   10,    1],\n",
              "       [   0,   55,   67,  141,  242,  419,  813, 1039,  208,   12,    5]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il y a amélioration du Random Forest et du KNN mais pas du MLP."
      ],
      "metadata": {
        "id": "6qGebeWBTeW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### III. Prise en compte de l'ordre des mots (LSTM)"
      ],
      "metadata": {
        "id": "6MrXD70KbqYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = {word: idx for idx, word in enumerate(model_google_vec_neg_300.index_to_key)}\n",
        "X_train_sequences = [to_sequence(word2idx, x) for x in reviews_train_tokens]\n",
        "X_test_sequences = [to_sequence(word2idx, x) for x in  reviews_test_tokens]\n",
        "\n",
        "print(reviews_train_tokens.values[0])\n",
        "print(X_train_sequences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2daeI07ZGA1",
        "outputId": "be0415c9-78d0-4920-d896-301c0a75dfce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['normal', 'bag', 'star', 'kirbi', 'lot', 'associ', 'rip', 'off', 'scam', 'kirbi', 'rep', 'come', 'hous', 'beat', 'old', 'ladi', 'fork', 'thousand', 'dollar', 'prei', 'hard', 'work', 'innoc', 'peopl', 'bui', 'junk', 'don', 'start', 'stai', 'awai', 'kirbi', 'trust', 'me']\n",
            "[1845, 3621, 948, 235, 2540857, 14175, 104, 8712, 15008, 216, 737810, 747, 154, 1957165, 18318, 4583, 1075, 387, 141, 496260, 972514, 10106, 19599, 284, 2071, 170]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import pad_sequences\n",
        "\n",
        "MAX_SEQ_LENGHT=50\n",
        "N_FEATURES = len(model_google_vec_neg_300.index_to_key)\n",
        "X_train_sequences = pad_sequences(X_train_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)\n",
        "X_test_sequences = pad_sequences(X_test_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)\n",
        "print(X_train_sequences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvM8DztcZLBe",
        "outputId": "970e8b2d-5fd7-48b7-ff50-6f30314646fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000000 3000000 3000000 3000000 3000000 3000000 3000000 3000000 3000000\n",
            " 3000000 3000000 3000000 3000000 3000000 3000000 3000000 3000000 3000000\n",
            " 3000000 3000000 3000000 3000000 3000000 3000000    1845    3621     948\n",
            "     235 2540857   14175     104    8712   15008     216  737810     747\n",
            "     154 1957165   18318    4583    1075     387     141  496260  972514\n",
            "   10106   19599     284    2071     170]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDINGS_LEN = model_google_vec_neg_300.vector_size\n",
        "embeddings_index = np.zeros((len(model_google_vec_neg_300.index_to_key)+1, EMBEDDINGS_LEN))\n",
        "embeddings_index.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hus5tsfzZSi5",
        "outputId": "1bb6f628-03ff-4040-a897-027319b2d7d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000001, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word, idx in word2idx.items():\n",
        "    try:\n",
        "        embedding = model_google_vec_neg_300[word]\n",
        "        embeddings_index[idx] = embedding\n",
        "    except:\n",
        "        pass\n",
        "      \n",
        "print(\"EMBEDDINGS_LEN=\", EMBEDDINGS_LEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gjIp6FbZWY1",
        "outputId": "e3bf1b4c-35c4-491d-94d4-56d8db0ebd53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EMBEDDINGS_LEN= 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense,LSTM\n",
        "\n",
        "\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(len(model_google_vec_neg_300.key_to_index)+1,\n",
        "                    EMBEDDINGS_LEN,  # Embedding size\n",
        "                    weights=[embeddings_index],\n",
        "                    trainable=False))\n",
        "\n",
        "model_lstm.add(LSTM(300, dropout=0.2))\n",
        "\n",
        "model_lstm.add(Dense(1))\n",
        "model_lstm.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "model_lstm.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFpt77gdZaT0",
        "outputId": "33b46bd1-39ab-45e4-ac36-774b0f62c234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, None, 300)         900000300 \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 300)               721200    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 301       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 900,721,801\n",
            "Trainable params: 721,501\n",
            "Non-trainable params: 900,000,300\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_lstm.fit(X_train_sequences, y_train, epochs=10, batch_size=128, verbose=1,validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvKz-dddZiMy",
        "outputId": "0b62b230-3b8d-4380-cfb9-1811e36011ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "578/578 [==============================] - 82s 139ms/step - loss: 2.6444 - mean_absolute_error: 2.6444 - val_loss: 2.5889 - val_mean_absolute_error: 2.5889\n",
            "Epoch 2/10\n",
            "578/578 [==============================] - 80s 138ms/step - loss: 2.5884 - mean_absolute_error: 2.5884 - val_loss: 2.5774 - val_mean_absolute_error: 2.5774\n",
            "Epoch 3/10\n",
            "578/578 [==============================] - 80s 138ms/step - loss: 2.5792 - mean_absolute_error: 2.5792 - val_loss: 2.5792 - val_mean_absolute_error: 2.5792\n",
            "Epoch 4/10\n",
            "578/578 [==============================] - 80s 139ms/step - loss: 2.5678 - mean_absolute_error: 2.5678 - val_loss: 2.5792 - val_mean_absolute_error: 2.5792\n",
            "Epoch 5/10\n",
            "578/578 [==============================] - 80s 139ms/step - loss: 2.5598 - mean_absolute_error: 2.5598 - val_loss: 2.5707 - val_mean_absolute_error: 2.5707\n",
            "Epoch 6/10\n",
            "578/578 [==============================] - 78s 136ms/step - loss: 2.5525 - mean_absolute_error: 2.5525 - val_loss: 2.5700 - val_mean_absolute_error: 2.5700\n",
            "Epoch 7/10\n",
            "578/578 [==============================] - 76s 131ms/step - loss: 2.5430 - mean_absolute_error: 2.5430 - val_loss: 2.5756 - val_mean_absolute_error: 2.5756\n",
            "Epoch 8/10\n",
            "578/578 [==============================] - 75s 130ms/step - loss: 2.5302 - mean_absolute_error: 2.5302 - val_loss: 2.5909 - val_mean_absolute_error: 2.5909\n",
            "Epoch 9/10\n",
            "578/578 [==============================] - 75s 130ms/step - loss: 2.5167 - mean_absolute_error: 2.5167 - val_loss: 2.5905 - val_mean_absolute_error: 2.5905\n",
            "Epoch 10/10\n",
            "578/578 [==============================] - 75s 130ms/step - loss: 2.5046 - mean_absolute_error: 2.5046 - val_loss: 2.6123 - val_mean_absolute_error: 2.6123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores_lstm = model_lstm.evaluate(X_test_sequences, y_test, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jtklt7ZsZmDZ",
        "outputId": "78fbec4d-b9c9-4dfb-aeda-e70f0747d563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "643/643 [==============================] - 16s 24ms/step - loss: 2.6339 - mean_absolute_error: 2.6339\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction=model_lstm.predict(X_test_sequences)\n",
        "prediction[prediction<1]=1\n",
        "prediction[prediction>10]=10\n",
        "ACC=accuracy_score(y_test,np.round(prediction))\n",
        "MAE=mean_absolute_error(y_test,prediction)\n",
        "CACC=customized_accuracy(y_test.to_numpy(), np.round(prediction))\n",
        "BTS=binary_tresholding_score(y_test.to_numpy(),np.round(prediction))\n",
        "print('For LSTM MAE = {0:.3f}, Accuracy = {1:.3f}, Customized Accuracy = {2:.3f}, Binary Thresholding Score = {3:.3f}'.format(MAE,ACC,CACC,BTS))\n",
        "display(confusion_matrix(y_test,np.round(prediction)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "QI1zGq__ZvXk",
        "outputId": "6b5fe252-092d-49ee-f2e4-730ec78ef320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "643/643 [==============================] - 16s 24ms/step\n",
            "For LSTM MAE = 2.633, Accuracy = 0.124, Customized Accuracy = 0.355, Binary Thresholding Score = 0.631\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,  156,  180,  268,  454,  589,  540,  609,  313,    3,    0],\n",
              "       [   0,    7,   13,   24,   50,   92,   75,   80,   26,    0,    0],\n",
              "       [   0,   16,   32,   38,   92,  173,  151,  214,   87,    0,    0],\n",
              "       [   0,   11,   34,   48,  105,  208,  209,  264,  148,    0,    0],\n",
              "       [   0,    8,   11,   22,   36,  105,  122,  178,   77,    0,    0],\n",
              "       [   0,   59,   83,  153,  301,  471,  511,  856,  519,    7,    0],\n",
              "       [   0,    3,   14,   22,   50,  123,  169,  331,  226,    1,    0],\n",
              "       [   0,   21,   44,   79,  159,  346,  453,  839,  550,    6,    0],\n",
              "       [   0,   20,   33,   78,  153,  308,  478, 1026,  925,   11,    0],\n",
              "       [   0,    8,   13,   23,   56,  137,  273,  781,  941,   19,    0],\n",
              "       [   0,   48,   57,  127,  246,  416,  479,  834,  783,   11,    0]])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Petite amélioration."
      ],
      "metadata": {
        "id": "IT8tdIU9BbY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### IV. Utilisation de la sortie du LSTM dans d'autres modèles seupervisés"
      ],
      "metadata": {
        "id": "2-_HjvyvhbRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "Model_rf = Model(inputs=model_lstm.inputs, outputs=model_lstm.layers[1].output)\n",
        "#Model_rf.save('../models/model_lstm_features.h5')\n",
        "corpus_train_lstm = Model_rf.predict(X_train_sequences)\n",
        "corpus_test_lstm = Model_rf.predict(X_test_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaIEbu6hhdXb",
        "outputId": "4b870e77-1da9-4681-cba3-9135f8d86b02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2569/2569 [==============================] - 63s 24ms/step\n",
            "643/643 [==============================] - 16s 24ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "algos = {\n",
        "  'RF' : RandomForestRegressor(n_estimators=50,random_state=1,n_jobs=-1),\n",
        "  'KNN' : KNeighborsRegressor(n_neighbors=5,n_jobs=-1,metric='cosine'),\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(20,10),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models_with_binary_tresholding_score(corpus_train_lstm,y_train,corpus_test_lstm,y_test.to_numpy(),algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "id": "xalqdzFEhfHt",
        "outputId": "678c3d57-3204-496f-ae9b-5c1596d1f570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## RF #############\n",
            "MAE = 2.700, Accuracy = 0.100, Customized Accuracy = 0.524, Binary Thresholding Score = 0.617\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,   38,   89,  192,  529,  923,  845,  427,   69,    0,    0],\n",
              "       [   0,    2,    0,   23,   55,  126,  109,   48,    4,    0,    0],\n",
              "       [   0,    4,    7,   50,  105,  223,  261,  134,   18,    1,    0],\n",
              "       [   0,    5,   11,   49,  125,  267,  343,  202,   24,    1,    0],\n",
              "       [   0,    3,    5,    9,   60,  144,  177,  134,   27,    0,    0],\n",
              "       [   0,   14,   32,  113,  359,  705,  977,  644,  115,    1,    0],\n",
              "       [   0,    0,    4,    9,   73,  177,  321,  295,   59,    1,    0],\n",
              "       [   0,    2,   17,   45,  220,  559,  846,  674,  132,    2,    0],\n",
              "       [   0,    2,   15,   52,  187,  512, 1060,  968,  231,    5,    0],\n",
              "       [   0,    1,    5,   19,   73,  259,  735,  889,  268,    2,    0],\n",
              "       [   0,   12,   24,   90,  304,  643,  904,  793,  230,    1,    0]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## KNN #############\n",
            "MAE = 2.833, Accuracy = 0.105, Customized Accuracy = 0.512, Binary Thresholding Score = 0.584\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  0, 107, 161, 300, 438, 596, 610, 530, 278,  87,   5],\n",
              "       [  0,   5,  11,  25,  65,  74,  78,  62,  34,  13,   0],\n",
              "       [  0,  10,  26,  67, 113, 134, 182, 155,  91,  24,   1],\n",
              "       [  0,  16,  23,  71, 123, 195, 237, 208, 123,  29,   2],\n",
              "       [  0,   6,  18,  30,  53,  98, 136, 129,  74,  15,   0],\n",
              "       [  0,  45,  84, 202, 354, 525, 632, 615, 361, 135,   7],\n",
              "       [  0,   5,  10,  43,  69, 170, 226, 220, 148,  47,   1],\n",
              "       [  0,  23,  46, 121, 254, 434, 557, 544, 391, 124,   3],\n",
              "       [  0,  24,  59, 115, 249, 450, 681, 724, 525, 196,   9],\n",
              "       [  0,  14,  26,  51, 134, 307, 461, 591, 482, 175,  10],\n",
              "       [  0,  43,  74, 170, 311, 497, 611, 667, 436, 187,   5]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## MLP #############\n",
            "MAE = 2.703, Accuracy = 0.105, Customized Accuracy = 0.548, Binary Thresholding Score = 0.617\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,  187,  110,  190,  295,  626,  832,  698,  107,   23,   44],\n",
              "       [   0,   11,   10,   11,   46,   85,  107,   87,    6,    1,    3],\n",
              "       [   0,   30,   17,   40,   70,  158,  229,  221,   32,    3,    3],\n",
              "       [   0,   23,   20,   34,   91,  183,  314,  309,   41,    5,    7],\n",
              "       [   0,   14,    8,   13,   21,  101,  172,  195,   31,    1,    3],\n",
              "       [   0,   85,   51,  111,  212,  490,  854,  961,  167,   14,   15],\n",
              "       [   0,    6,   15,   20,   28,  122,  250,  433,   60,    2,    3],\n",
              "       [   0,   43,   28,   62,  122,  346,  721,  997,  167,    4,    7],\n",
              "       [   0,   28,   21,   60,  109,  340,  769, 1375,  316,    6,    8],\n",
              "       [   0,   11,   11,   14,   58,  145,  442, 1234,  323,    9,    4],\n",
              "       [   0,   75,   38,   77,  159,  446,  797, 1109,  264,   23,   13]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pas d'amélioration."
      ],
      "metadata": {
        "id": "NZDFULLQCXW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C. Utilisation du commentaire et du titre du produit"
      ],
      "metadata": {
        "id": "xjtElnUqb-_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I. Moyenne des embeddings"
      ],
      "metadata": {
        "id": "kSFRd6cHcZdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size=model_google_vec_neg_300.vector_size\n",
        "\n",
        "reviews_train_wv_google=word2vec_generator(reviews_train_tokens,model_google_vec_neg_300,vector_size)\n",
        "reviews_test_wv_google=word2vec_generator(reviews_test_tokens,model_google_vec_neg_300,vector_size)\n",
        "\n",
        "title_train_wv_google=word2vec_generator(title_train_tokens,model_google_vec_neg_300,vector_size)\n",
        "title_test_wv_google=word2vec_generator(title_test_tokens,model_google_vec_neg_300,vector_size)\n",
        "\n",
        "# renommage des colonnes de titles...\n",
        "title_train_wv_google.columns = [\"t\" + str(c) for c in title_train_wv_google.columns]\n",
        "title_test_wv_google.columns = [\"t\" + str(c) for c in title_test_wv_google.columns]"
      ],
      "metadata": {
        "id": "hJ5DG1iJakpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rev_a_title_train_wv_google = pd.concat([reviews_train_wv_google, title_train_wv_google], axis=1)\n",
        "rev_a_title_test_wv_google = pd.concat([reviews_test_wv_google, title_test_wv_google], axis=1)\n",
        "\n",
        "rev_a_title_train_wv_google.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "PVRVdRLVany5",
        "outputId": "70f763e1-cb26-4201-c3fb-dac0c69d5a39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3         4         5         6  \\\n",
              "0  0.051895  0.037328 -0.034177  0.101743 -0.058414  0.054615 -0.041165   \n",
              "1  0.001054  0.001782 -0.044656  0.097634  0.048234 -0.050914  0.082231   \n",
              "2  0.024440 -0.019189 -0.017484  0.115548 -0.080122  0.034342  0.108956   \n",
              "\n",
              "          7         8         9        10        11        12        13  \\\n",
              "0 -0.046734  0.038159  0.087764  0.003641 -0.067222 -0.049265  0.003996   \n",
              "1 -0.041166 -0.003163  0.062847 -0.014182 -0.113599 -0.078436  0.016851   \n",
              "2 -0.102426  0.056501 -0.008850  0.019420 -0.131958 -0.004109 -0.035932   \n",
              "\n",
              "         14        15        16        17        18        19        20  \\\n",
              "0 -0.131833  0.101383  0.028708  0.075796 -0.012462 -0.011240  0.008695   \n",
              "1 -0.009305  0.150368  0.109996  0.136547  0.062544 -0.062195  0.077737   \n",
              "2 -0.126325  0.074991  0.056996  0.084442  0.071490 -0.065927 -0.008837   \n",
              "\n",
              "         21        22        23        24        25        26        27  \\\n",
              "0  0.041100  0.084000  0.001998  0.057393 -0.026329 -0.040030  0.017978   \n",
              "1  0.091286  0.108676  0.037065  0.091064 -0.012357 -0.018322 -0.042345   \n",
              "2  0.061724 -0.032928 -0.031252 -0.006649  0.035767 -0.019636  0.040798   \n",
              "\n",
              "         28        29        30        31        32        33        34  \\\n",
              "0  0.063161 -0.043544 -0.017762  0.047229  0.000327 -0.043785  0.012311   \n",
              "1  0.017696  0.016801 -0.080247  0.076333  0.005660  0.046167  0.002042   \n",
              "2  0.040303 -0.078404 -0.046112  0.047523 -0.149671  0.095799  0.048139   \n",
              "\n",
              "         35        36        37        38        39        40        41  \\\n",
              "0 -0.032266  0.040161  0.030547  0.010000  0.101680  0.085920 -0.113623   \n",
              "1 -0.020963  0.099282 -0.033167  0.010920  0.044533  0.136308 -0.053655   \n",
              "2 -0.054740  0.067553 -0.046378  0.034215  0.069903  0.109706  0.011852   \n",
              "\n",
              "         42        43        44        45        46        47        48  \\\n",
              "0  0.134226 -0.021433 -0.023916 -0.075753 -0.078853 -0.009371  0.002723   \n",
              "1  0.111411  0.032424  0.003096 -0.047646 -0.064209 -0.025413  0.044914   \n",
              "2  0.059383  0.017539 -0.078974 -0.042868  0.042350 -0.000314 -0.039991   \n",
              "\n",
              "         49        50        51        52        53        54        55  \\\n",
              "0  0.022405 -0.005610  0.115219  0.019362 -0.019095  0.001305 -0.028062   \n",
              "1  0.020874 -0.037770  0.050557  0.132291  0.012962 -0.030063  0.012551   \n",
              "2  0.045389 -0.011892  0.062012 -0.078090 -0.098218 -0.065909  0.094793   \n",
              "\n",
              "         56        57        58        59        60        61        62  \\\n",
              "0 -0.047248 -0.083741  0.019699 -0.085840 -0.016829  0.037920 -0.076820   \n",
              "1  0.009452 -0.055475  0.059297 -0.098962 -0.093600  0.058651 -0.100142   \n",
              "2  0.010773 -0.072719  0.074815 -0.128605  0.090681 -0.012864 -0.062618   \n",
              "\n",
              "         63        64        65        66        67        68        69  \\\n",
              "0 -0.002485  0.009050 -0.073896 -0.065966  0.052086  0.028344  0.036428   \n",
              "1 -0.019221  0.004052  0.002907 -0.093119 -0.041920 -0.036821  0.150257   \n",
              "2 -0.123455 -0.032645 -0.101922 -0.142417  0.045845 -0.068571  0.103021   \n",
              "\n",
              "         70        71        72        73        74        75        76  \\\n",
              "0  0.041802  0.028025  0.055240 -0.009629 -0.161032 -0.020440  0.031243   \n",
              "1  0.087511  0.024636  0.076089 -0.032618 -0.154652 -0.200550  0.007850   \n",
              "2 -0.014577 -0.007952  0.058071  0.100691 -0.170471  0.007307 -0.009129   \n",
              "\n",
              "         77        78        79        80        81        82        83  \\\n",
              "0  0.071313 -0.021661 -0.021817 -0.086409 -0.039698  0.012998  0.000663   \n",
              "1 -0.001337  0.086673  0.124856 -0.044778 -0.033749  0.000300 -0.018230   \n",
              "2  0.103352  0.034467  0.031198 -0.080663 -0.023499  0.079476 -0.030576   \n",
              "\n",
              "         84        85        86        87        88        89        90  \\\n",
              "0 -0.056211 -0.004524 -0.060419  0.085980  0.031487  0.028893 -0.004157   \n",
              "1 -0.025102  0.023553 -0.034768  0.091353 -0.007187  0.023593  0.047476   \n",
              "2  0.003067 -0.023821 -0.172573  0.077101  0.011588  0.063492  0.074267   \n",
              "\n",
              "         91        92        93        94        95        96        97  \\\n",
              "0  0.061732 -0.068722 -0.113466 -0.067157 -0.034516  0.039504  0.064622   \n",
              "1 -0.016369 -0.056674 -0.039155 -0.047608 -0.053237  0.067560  0.085790   \n",
              "2 -0.038339 -0.079266 -0.059419 -0.059952 -0.051793  0.070010  0.023330   \n",
              "\n",
              "         98        99       100       101       102       103       104  \\\n",
              "0  0.074289 -0.057171 -0.075850 -0.036300  0.065043 -0.015186  0.010391   \n",
              "1  0.016147 -0.067441 -0.155218  0.024892  0.038164  0.054416 -0.100134   \n",
              "2  0.031503 -0.030300  0.007483  0.045401 -0.026986 -0.099906  0.045763   \n",
              "\n",
              "        105       106       107       108       109       110       111  \\\n",
              "0 -0.045006  0.004535 -0.006627  0.064333 -0.068010 -0.052673 -0.007005   \n",
              "1  0.029646  0.016868 -0.048662  0.065652 -0.068615 -0.085283 -0.027765   \n",
              "2 -0.030796 -0.004225 -0.035932 -0.023579 -0.132747 -0.046112  0.047355   \n",
              "\n",
              "        112       113       114       115       116       117       118  \\\n",
              "0 -0.006933 -0.014125  0.090703  0.041767  0.017151 -0.055764  0.012064   \n",
              "1 -0.035756  0.022450  0.147131 -0.050526 -0.010859 -0.034463  0.010476   \n",
              "2 -0.054749  0.012486  0.141724  0.037678  0.002816 -0.040149  0.109887   \n",
              "\n",
              "        119       120       121       122       123       124       125  \\\n",
              "0  0.042873  0.007832  0.009667 -0.078502  0.091327  0.053238  0.028802   \n",
              "1  0.017160 -0.062181 -0.104420 -0.009566  0.121113  0.024969 -0.044083   \n",
              "2  0.011034  0.089085  0.024684 -0.011967  0.115417 -0.007860 -0.014169   \n",
              "\n",
              "        126       127       128       129       130       131       132  \\\n",
              "0 -0.009848 -0.019586  0.058703  0.018897 -0.067695 -0.079420 -0.035956   \n",
              "1 -0.045151 -0.086857  0.021973  0.126792 -0.129838 -0.059061 -0.094316   \n",
              "2 -0.061820 -0.011394 -0.033591  0.067644 -0.060865 -0.071586 -0.051348   \n",
              "\n",
              "        133       134       135       136       137       138       139  \\\n",
              "0 -0.004591 -0.036042  0.023869  0.018929  0.004610 -0.018475  0.058678   \n",
              "1  0.066262  0.027890 -0.049882  0.070534 -0.001731 -0.020641  0.165394   \n",
              "2  0.007917  0.036743 -0.021198 -0.025108  0.004341  0.031023  0.057602   \n",
              "\n",
              "        140       141       142       143       144       145       146  \\\n",
              "0  0.056634 -0.106849  0.048059  0.013481 -0.016872  0.022587 -0.035953   \n",
              "1 -0.021368 -0.098885 -0.019942 -0.116599 -0.016441  0.042259 -0.010454   \n",
              "2  0.087646 -0.025502 -0.079930  0.031259 -0.027605  0.006768  0.062465   \n",
              "\n",
              "        147       148       149       150       151       152       153  \\\n",
              "0 -0.055542 -0.026154 -0.079539  0.086903 -0.038297 -0.048888  0.093569   \n",
              "1  0.015103  0.048406 -0.084101  0.112654  0.028453 -0.158001  0.120938   \n",
              "2  0.049685 -0.126223 -0.054025  0.115180  0.085100  0.014936  0.189784   \n",
              "\n",
              "        154       155       156       157       158       159       160  \\\n",
              "0 -0.066556 -0.000924 -0.093473 -0.054987 -0.037182 -0.046404 -0.041381   \n",
              "1 -0.092618  0.054968  0.025969 -0.108920 -0.116943 -0.086725 -0.051371   \n",
              "2 -0.022626 -0.074969 -0.064679 -0.084045 -0.161970 -0.080627  0.008013   \n",
              "\n",
              "        161       162       163       164       165       166       167  \\\n",
              "0  0.066814  0.068162 -0.022368  0.018231 -0.103666  0.056653 -0.108814   \n",
              "1  0.060797  0.005488  0.038386  0.061250 -0.090021  0.061210 -0.054016   \n",
              "2  0.074486 -0.006906  0.057783  0.096531  0.040353  0.073356 -0.149391   \n",
              "\n",
              "        168       169       170       171       172       173       174  \\\n",
              "0  0.012367  0.011420 -0.144671 -0.009916 -0.041884 -0.001014 -0.029669   \n",
              "1 -0.086337 -0.005388 -0.056616 -0.017226  0.003385 -0.046697 -0.043990   \n",
              "2 -0.024833 -0.039019  0.001670  0.033116  0.015350 -0.045323  0.041508   \n",
              "\n",
              "        175       176       177       178       179       180       181  \\\n",
              "0  0.011479  0.093996 -0.030344  0.022801  0.045775 -0.044797 -0.080822   \n",
              "1  0.109053  0.091625 -0.089866 -0.094371  0.019520 -0.188721 -0.099576   \n",
              "2 -0.040880  0.100784 -0.088475  0.016064 -0.029297 -0.064985 -0.125746   \n",
              "\n",
              "        182       183       184       185       186       187       188  \\\n",
              "0  0.023369  0.040966  0.005505 -0.038203 -0.042134  0.061333  0.069623   \n",
              "1  0.086737  0.030562 -0.136009 -0.055500 -0.027821 -0.016632  0.050981   \n",
              "2 -0.007875 -0.107812 -0.001500 -0.023482 -0.051274 -0.044964  0.039952   \n",
              "\n",
              "        189       190       191       192       193       194       195  \\\n",
              "0  0.003920  0.021034  0.012171  0.004338  0.049119 -0.008002  0.041074   \n",
              "1 -0.043346 -0.034557  0.016025  0.020308 -0.000954  0.032720  0.007446   \n",
              "2  0.038707 -0.012763 -0.004634  0.050814  0.004141 -0.034511 -0.064514   \n",
              "\n",
              "        196       197       198       199       200       201       202  \\\n",
              "0  0.011200  0.025526 -0.052760 -0.025777  0.005203  0.095475 -0.096175   \n",
              "1  0.001737 -0.049458 -0.075029 -0.108776  0.014321  0.067483 -0.102644   \n",
              "2  0.064621  0.046927 -0.044859 -0.089714 -0.038112  0.060093 -0.065798   \n",
              "\n",
              "        203       204       205       206       207       208       209  \\\n",
              "0  0.014684 -0.000366 -0.020392 -0.060521 -0.020938  0.050421  0.001144   \n",
              "1 -0.045013  0.059350 -0.079948 -0.127491 -0.062223 -0.021512 -0.022272   \n",
              "2  0.001641 -0.082546 -0.022811 -0.114556  0.004238 -0.000314 -0.019163   \n",
              "\n",
              "        210       211       212       213       214       215       216  \\\n",
              "0  0.060202  0.052409 -0.031468  0.037297 -0.044965  0.051063  0.052388   \n",
              "1 -0.036333  0.100087 -0.145974  0.080578 -0.044201 -0.043307  0.200195   \n",
              "2 -0.065134  0.079867  0.002223  0.001635 -0.125506  0.053493  0.053502   \n",
              "\n",
              "        217       218       219       220       221       222       223  \\\n",
              "0  0.006491 -0.129092 -0.052728 -0.050609  0.026036  0.004733  0.042223   \n",
              "1  0.051958 -0.067055 -0.066084 -0.009458 -0.026900 -0.055853 -0.072121   \n",
              "2  0.074639 -0.036534 -0.071846  0.010553  0.004024 -0.102129  0.040251   \n",
              "\n",
              "        224       225       226       227       228       229       230  \\\n",
              "0  0.084900 -0.054232  0.016700 -0.018987 -0.004762 -0.033665  0.027724   \n",
              "1  0.065344  0.010720  0.046392 -0.038976 -0.023837  0.045377 -0.039108   \n",
              "2  0.057155 -0.013085 -0.066462  0.017840  0.003993 -0.068726 -0.008889   \n",
              "\n",
              "        231       232       233       234       235       236       237  \\\n",
              "0 -0.022977 -0.009939 -0.017045  0.034892  0.041107  0.000366 -0.053192   \n",
              "1 -0.038517 -0.052901 -0.063382  0.150236  0.035785 -0.074485  0.024268   \n",
              "2  0.019064 -0.115060  0.095494  0.089475 -0.020464  0.131217  0.027825   \n",
              "\n",
              "        238       239       240       241       242       243       244  \\\n",
              "0  0.066096  0.036053  0.103144  0.002409 -0.029148 -0.118922 -0.026632   \n",
              "1  0.087624  0.011253  0.043257 -0.000627 -0.057000 -0.041005 -0.043680   \n",
              "2 -0.012373 -0.047389  0.017735 -0.120793  0.057539  0.007649 -0.095213   \n",
              "\n",
              "        245       246       247       248       249       250       251  \\\n",
              "0  0.042847 -0.018780  0.022491  0.018803 -0.052682  0.036296  0.058932   \n",
              "1  0.006481 -0.001105  0.036183 -0.063053 -0.164828  0.014438  0.016227   \n",
              "2  0.007350 -0.019235  0.078650  0.017717 -0.126347  0.028739  0.049500   \n",
              "\n",
              "        252       253       254       255       256       257       258  \\\n",
              "0  0.048758  0.086733  0.015181 -0.020578 -0.002301  0.017927 -0.082257   \n",
              "1  0.043427  0.136253  0.009216 -0.032669  0.009327 -0.013017 -0.080031   \n",
              "2  0.092585  0.047727  0.023660 -0.024222 -0.046077  0.028678 -0.098001   \n",
              "\n",
              "        259       260       261       262       263       264       265  \\\n",
              "0 -0.097707  0.013906 -0.023128 -0.052425  0.012839  0.044951  0.075966   \n",
              "1 -0.058527 -0.009344 -0.001243 -0.122148 -0.008071  0.050593  0.087290   \n",
              "2  0.088161  0.001770  0.007455 -0.078526 -0.034435 -0.001112  0.133676   \n",
              "\n",
              "        266       267       268       269       270       271       272  \\\n",
              "0 -0.060223  0.018071 -0.014205 -0.000439 -0.046774  0.044653  0.099763   \n",
              "1  0.012525  0.007815 -0.061798 -0.074912 -0.085494  0.111972  0.042913   \n",
              "2  0.042603 -0.041260 -0.071237 -0.069057  0.077026  0.066044  0.042341   \n",
              "\n",
              "        273       274       275       276       277       278       279  \\\n",
              "0 -0.012508  0.080197 -0.018646 -0.061245 -0.081346 -0.074165  0.027213   \n",
              "1  0.087025  0.076061 -0.126753 -0.084853 -0.076638 -0.056785  0.031760   \n",
              "2  0.066668  0.139125 -0.000296 -0.076050 -0.061872 -0.095869  0.049816   \n",
              "\n",
              "        280       281       282       283       284       285       286  \\\n",
              "0 -0.018275  0.013956  0.067887  0.098910  0.051680 -0.012935 -0.086912   \n",
              "1  0.072879  0.000888  0.066967  0.078114  0.010620 -0.008073 -0.042034   \n",
              "2  0.075640  0.036830  0.089923  0.048976  0.042324 -0.022932 -0.080719   \n",
              "\n",
              "        287       288       289       290       291       292       293  \\\n",
              "0  0.039813  0.019609  0.034114  0.016492  0.101426 -0.098417 -0.008000   \n",
              "1 -0.009710 -0.010537  0.006836  0.003540  0.039720 -0.179488 -0.036050   \n",
              "2  0.088243  0.112810  0.063232 -0.151803 -0.008364 -0.070173 -0.054688   \n",
              "\n",
              "        294       295       296       297       298       299        t0  \\\n",
              "0 -0.042937 -0.026282 -0.011188 -0.034331 -0.041593 -0.018652  0.004442   \n",
              "1 -0.042403 -0.041787  0.031367 -0.023621 -0.063424 -0.004039 -0.033264   \n",
              "2 -0.014561 -0.038535  0.032900 -0.009990 -0.029566 -0.058221  0.020142   \n",
              "\n",
              "         t1        t2        t3        t4        t5        t6        t7  \\\n",
              "0  0.110379  0.029680  0.050332 -0.094188  0.014221 -0.066811 -0.130055   \n",
              "1  0.085437 -0.028619  0.065173 -0.053516 -0.038281 -0.106885 -0.088867   \n",
              "2 -0.023315 -0.018112  0.170898 -0.043823  0.111816  0.061768 -0.097961   \n",
              "\n",
              "         t8        t9       t10       t11       t12       t13       t14  \\\n",
              "0  0.057018  0.074929 -0.026917 -0.095598  0.011858 -0.073314 -0.091153   \n",
              "1  0.093604  0.062354 -0.040540 -0.104346 -0.052124  0.038077 -0.081763   \n",
              "2 -0.069885  0.069214 -0.038818 -0.198730 -0.166992 -0.044495 -0.044556   \n",
              "\n",
              "        t15       t16       t17       t18       t19       t20       t21  \\\n",
              "0  0.080200  0.005538  0.060813 -0.000502 -0.062600  0.017908  0.009444   \n",
              "1 -0.001077 -0.019141  0.104065 -0.029080 -0.124295  0.002368  0.095801   \n",
              "2 -0.010925  0.099228  0.183594  0.048340 -0.018127  0.037231  0.106995   \n",
              "\n",
              "        t22       t23       t24       t25       t26       t27       t28  \\\n",
              "0 -0.043845  0.023987 -0.011497  0.029896 -0.156672  0.107794  0.030462   \n",
              "1 -0.003790 -0.008057 -0.046313 -0.045959 -0.128809  0.118750  0.118701   \n",
              "2 -0.042603  0.107605 -0.010254 -0.001953 -0.103027  0.075500  0.021439   \n",
              "\n",
              "        t29       t30       t31       t32       t33       t34       t35  \\\n",
              "0 -0.059143 -0.018854  0.017811 -0.020064  0.121718  0.027555 -0.016341   \n",
              "1 -0.046130 -0.036485  0.059882 -0.050143 -0.152246  0.006885  0.003784   \n",
              "2 -0.293945 -0.117188  0.165283 -0.291992  0.013184  0.040649 -0.080200   \n",
              "\n",
              "        t36       t37       t38       t39       t40       t41       t42  \\\n",
              "0  0.032487  0.010143 -0.018625 -0.014671  0.024180 -0.052368  0.040882   \n",
              "1 -0.014929 -0.036832 -0.050330  0.001270  0.100003  0.002271  0.149582   \n",
              "2  0.046021 -0.172363 -0.062500  0.078125  0.063354  0.150391 -0.054863   \n",
              "\n",
              "        t43       t44       t45       t46       t47       t48       t49  \\\n",
              "0  0.031294  0.026090 -0.125816 -0.054099  0.065482  0.091919  0.048230   \n",
              "1  0.002316 -0.112860 -0.042432  0.049930 -0.052710  0.099289  0.060638   \n",
              "2  0.097900  0.015577 -0.058136  0.042847  0.000977 -0.114075 -0.046387   \n",
              "\n",
              "        t50       t51       t52       t53       t54       t55       t56  \\\n",
              "0 -0.022039  0.026163 -0.023338 -0.036591  0.058660 -0.000732 -0.052091   \n",
              "1 -0.024561  0.006726  0.098896  0.022693 -0.032422 -0.089270  0.036096   \n",
              "2 -0.024063  0.112823 -0.002441 -0.162109 -0.026611 -0.104004 -0.076782   \n",
              "\n",
              "        t57       t58       t59       t60       t61       t62       t63  \\\n",
              "0 -0.062472  0.063044 -0.113015 -0.019875  0.030806 -0.073398 -0.032360   \n",
              "1 -0.060977 -0.017202 -0.059467 -0.009930 -0.016235 -0.104175 -0.093433   \n",
              "2 -0.095459  0.079102 -0.143799  0.159668 -0.074707 -0.109375 -0.111328   \n",
              "\n",
              "        t64       t65       t66       t67       t68       t69       t70  \\\n",
              "0 -0.001298 -0.053411 -0.049039  0.051447  0.011314  0.114367  0.109385   \n",
              "1  0.033162 -0.010995 -0.167554  0.103186  0.088501  0.055981  0.029272   \n",
              "2 -0.073914 -0.115723 -0.083740  0.087158 -0.043213  0.164062  0.017578   \n",
              "\n",
              "        t71       t72       t73       t74       t75       t76       t77  \\\n",
              "0  0.035894  0.060073  0.017412 -0.078097 -0.040300  0.095834  0.045030   \n",
              "1  0.052771  0.106641 -0.058228 -0.086279 -0.101135  0.057367  0.037744   \n",
              "2  0.012589  0.099854  0.062378 -0.020996  0.054443 -0.071289 -0.110107   \n",
              "\n",
              "        t78       t79       t80       t81       t82       t83       t84  \\\n",
              "0 -0.000794 -0.003540  0.071563 -0.010137  0.014205 -0.020541 -0.052723   \n",
              "1 -0.004025  0.002179  0.033357 -0.038501  0.025476 -0.002698  0.011501   \n",
              "2 -0.084473 -0.099365 -0.145020 -0.063660  0.044922 -0.036560 -0.040405   \n",
              "\n",
              "        t85       t86       t87       t88       t89       t90       t91  \\\n",
              "0 -0.005993 -0.005693  0.149625  0.013894  0.013161  0.048676 -0.062112   \n",
              "1  0.005823 -0.060974  0.082837  0.022705  0.123291 -0.133838  0.059204   \n",
              "2  0.163330 -0.159180  0.124023  0.036621  0.011902  0.114861 -0.062744   \n",
              "\n",
              "        t92       t93       t94       t95       t96       t97       t98  \\\n",
              "0 -0.039655 -0.025914  0.020169 -0.018444 -0.114774 -0.075290  0.052856   \n",
              "1 -0.039185  0.015979  0.065918 -0.045572  0.010422  0.018085  0.073129   \n",
              "2 -0.095947 -0.102409 -0.173828  0.145508  0.128906  0.028809  0.097656   \n",
              "\n",
              "        t99      t100      t101      t102      t103      t104      t105  \\\n",
              "0  0.014759  0.049572 -0.030538 -0.003890  0.036954 -0.063743 -0.041883   \n",
              "1 -0.027090 -0.011981 -0.053564  0.034216  0.018085 -0.069226 -0.099809   \n",
              "2 -0.014893 -0.064327  0.128662 -0.043701 -0.081055 -0.163086 -0.093872   \n",
              "\n",
              "       t106      t107      t108      t109      t110      t111      t112  \\\n",
              "0  0.005612  0.064248 -0.005346 -0.029982  0.033203  0.010628 -0.071422   \n",
              "1  0.077966 -0.002281  0.070276  0.000894  0.077640  0.049038 -0.118469   \n",
              "2  0.079346 -0.146484 -0.009277 -0.171875 -0.063354  0.150452 -0.002014   \n",
              "\n",
              "       t113      t114      t115      t116      t117      t118      t119  \\\n",
              "0 -0.053151  0.041149  0.052809 -0.017057 -0.040638  0.053520  0.057617   \n",
              "1  0.065125  0.063452 -0.029886 -0.003882 -0.062134  0.018213  0.045096   \n",
              "2  0.073608  0.039551  0.136047  0.005371 -0.077393  0.069702 -0.057861   \n",
              "\n",
              "       t120      t121      t122      t123      t124      t125      t126  \\\n",
              "0 -0.005815 -0.155806 -0.013117  0.117254  0.063010 -0.008609  0.014107   \n",
              "1 -0.015625  0.030215 -0.087355  0.077466  0.061343 -0.010736 -0.087750   \n",
              "2  0.102417 -0.024414  0.004883  0.147949  0.036926  0.034515 -0.160645   \n",
              "\n",
              "       t127      t128      t129      t130      t131      t132      t133  \\\n",
              "0 -0.009610  0.123281  0.041504  0.060613 -0.089697 -0.088568  0.032582   \n",
              "1  0.076697  0.081256  0.171240 -0.050885 -0.024017 -0.029041  0.035010   \n",
              "2 -0.042542 -0.030029  0.050842 -0.023865 -0.055786  0.014465  0.172852   \n",
              "\n",
              "       t134      t135      t136      t137      t138      t139      t140  \\\n",
              "0 -0.011282  0.072585 -0.036643 -0.017361 -0.047641  0.098666 -0.010476   \n",
              "1  0.058936  0.016943 -0.040715  0.009045  0.016602 -0.058224  0.068182   \n",
              "2 -0.030029 -0.024834  0.042267 -0.071960  0.045410  0.116699  0.080688   \n",
              "\n",
              "       t141      t142      t143      t144      t145      t146      t147  \\\n",
              "0 -0.084445  0.045787 -0.000666  0.033580 -0.056388 -0.014804  0.016768   \n",
              "1 -0.169360  0.022827 -0.022589  0.033661  0.156598 -0.039621  0.064331   \n",
              "2 -0.013000  0.002930  0.097229 -0.057220  0.001183  0.213379 -0.051025   \n",
              "\n",
              "       t148      t149      t150      t151      t152      t153      t154  \\\n",
              "0 -0.016688 -0.018676  0.074019 -0.084339 -0.042025  0.070657  0.003070   \n",
              "1 -0.044409 -0.040804  0.158966 -0.116748 -0.051065  0.058643 -0.012624   \n",
              "2 -0.141479  0.074097  0.170166  0.270508  0.095215  0.156250 -0.141113   \n",
              "\n",
              "       t155      t156      t157      t158      t159      t160      t161  \\\n",
              "0 -0.029180 -0.139393  0.022228 -0.044123  0.053151  0.037864  0.021118   \n",
              "1 -0.040546 -0.169482 -0.011092 -0.053809 -0.047314 -0.028320  0.026489   \n",
              "2 -0.086914 -0.057343 -0.138672 -0.213379 -0.110718 -0.095947  0.111931   \n",
              "\n",
              "       t162      t163      t164      t165      t166      t167      t168  \\\n",
              "0  0.007016 -0.021040  0.041519 -0.040871  0.107455  0.002960  0.015791   \n",
              "1  0.026306 -0.054993 -0.009918 -0.058942  0.050195 -0.069815  0.002750   \n",
              "2 -0.145020  0.164551  0.125732  0.085510  0.158203 -0.153698 -0.055450   \n",
              "\n",
              "       t169      t170      t171      t172      t173      t174      t175  \\\n",
              "0  0.007790 -0.100822 -0.049408 -0.032812 -0.038358 -0.032005 -0.004494   \n",
              "1 -0.050122 -0.099222 -0.023523 -0.041675  0.008414 -0.020383 -0.005309   \n",
              "2 -0.113770 -0.181152 -0.057129  0.109619  0.022339  0.107910  0.018066   \n",
              "\n",
              "       t176      t177      t178      t179      t180      t181      t182  \\\n",
              "0  0.120062 -0.144465 -0.047530  0.100048 -0.080145 -0.072188 -0.017001   \n",
              "1  0.111230 -0.168311 -0.051233 -0.051663  0.013602 -0.118787 -0.032056   \n",
              "2 -0.043991  0.035156  0.007812  0.053833 -0.011658 -0.073486 -0.044067   \n",
              "\n",
              "       t183      t184      t185      t186      t187      t188      t189  \\\n",
              "0  0.082586  0.011271  0.018682  0.073875  0.073836 -0.014970  0.061257   \n",
              "1  0.018774 -0.006619 -0.094467  0.095776 -0.018164  0.129248  0.021240   \n",
              "2 -0.149902  0.105225  0.080811  0.019453 -0.033661  0.125000  0.093994   \n",
              "\n",
              "       t190      t191      t192      t193      t194      t195      t196  \\\n",
              "0 -0.009455  0.022261  0.062506  0.046353 -0.070817  0.006303 -0.011968   \n",
              "1  0.077496 -0.008240 -0.059033  0.081934 -0.011026  0.147864 -0.005255   \n",
              "2  0.016632  0.080444  0.095947 -0.082764 -0.134521 -0.146240  0.080627   \n",
              "\n",
              "       t197      t198      t199      t200      t201      t202      t203  \\\n",
              "0 -0.014515  0.048345 -0.030784  0.010282  0.109222 -0.087562  0.013050   \n",
              "1  0.020117 -0.066284  0.045508  0.026172  0.059106 -0.099658 -0.079663   \n",
              "2  0.054321 -0.035339  0.030151 -0.088013  0.047974 -0.082550 -0.039017   \n",
              "\n",
              "       t204      t205      t206      t207      t208      t209      t210  \\\n",
              "0 -0.051114 -0.041482  0.039540  0.010509  0.100701 -0.035606 -0.009999   \n",
              "1  0.026416 -0.098218  0.016309 -0.154184  0.012402  0.021631 -0.039868   \n",
              "2 -0.114624 -0.129761 -0.079712  0.056763  0.007324 -0.096161 -0.220215   \n",
              "\n",
              "       t211      t212      t213      t214      t215      t216      t217  \\\n",
              "0  0.045122 -0.090132  0.114459 -0.012068  0.031195  0.085574 -0.004328   \n",
              "1  0.141357 -0.123218  0.136963 -0.082266  0.017908  0.053600 -0.036914   \n",
              "2  0.095779 -0.027405 -0.070923 -0.092651  0.024414 -0.001221  0.164307   \n",
              "\n",
              "       t218      t219      t220      t221      t222      t223      t224  \\\n",
              "0 -0.124545  0.039407 -0.036588  0.092751 -0.003435  0.035624 -0.000236   \n",
              "1 -0.163770 -0.067383 -0.138672  0.029297 -0.082422  0.083191  0.005103   \n",
              "2 -0.140137 -0.175537 -0.060989 -0.053406 -0.094727 -0.038574  0.072205   \n",
              "\n",
              "       t225      t226      t227      t228      t229      t230      t231  \\\n",
              "0 -0.075739  0.037828 -0.000441  0.011838 -0.084488  0.043875 -0.021368   \n",
              "1 -0.042999  0.059521  0.004687  0.050659 -0.035229  0.029028 -0.068005   \n",
              "2 -0.035034 -0.010254  0.182495 -0.174316 -0.098022  0.059021 -0.150078   \n",
              "\n",
              "       t232      t233      t234      t235      t236      t237      t238  \\\n",
              "0  0.056835 -0.029683 -0.003407 -0.010703 -0.022872  0.041926  0.078635   \n",
              "1 -0.040979 -0.025415 -0.007959 -0.064954 -0.044617 -0.038359  0.076807   \n",
              "2 -0.042969  0.100098  0.290527 -0.060059  0.078125  0.027832  0.082520   \n",
              "\n",
              "       t239      t240      t241      t242      t243      t244      t245  \\\n",
              "0  0.017168  0.037098 -0.049500  0.028617 -0.109264  0.083523  0.012756   \n",
              "1  0.003052  0.116927 -0.067413 -0.004865 -0.072168 -0.003876 -0.025293   \n",
              "2  0.032104  0.170410 -0.290039  0.037109 -0.034912 -0.016357  0.014282   \n",
              "\n",
              "       t246      t247      t248      t249      t250      t251      t252  \\\n",
              "0 -0.093439  0.070401  0.075828 -0.022039  0.031403  0.054291  0.063883   \n",
              "1  0.021417  0.040814 -0.048975 -0.102832  0.047266  0.080347  0.051373   \n",
              "2  0.015381  0.083374 -0.090042 -0.278809  0.020599  0.031494 -0.003906   \n",
              "\n",
              "       t253      t254      t255      t256      t257      t258      t259  \\\n",
              "0  0.105386  0.028265  0.002674 -0.023482  0.040991 -0.042487 -0.066567   \n",
              "1  0.001697  0.014594 -0.040234  0.085754 -0.050488 -0.041284 -0.001678   \n",
              "2  0.082520  0.089966 -0.026062 -0.048218  0.049713 -0.064209  0.149414   \n",
              "\n",
              "       t260      t261      t262      t263      t264      t265      t266  \\\n",
              "0  0.092205  0.009408 -0.083651 -0.018915  0.055505  0.037384 -0.007301   \n",
              "1  0.027881  0.009937 -0.009497  0.053183  0.066504  0.137244 -0.044849   \n",
              "2 -0.089371  0.040771  0.086914  0.093750  0.142578  0.348633  0.104248   \n",
              "\n",
              "       t267      t268      t269      t270      t271      t272      t273  \\\n",
              "0 -0.051247 -0.115146  0.037015 -0.025568 -0.056647 -0.019456  0.019936   \n",
              "1  0.004687  0.023503 -0.062231 -0.044312  0.129553  0.003674  0.051208   \n",
              "2 -0.182373 -0.118774 -0.038208 -0.090698  0.082123 -0.055328  0.023438   \n",
              "\n",
              "       t274      t275      t276      t277      t278      t279      t280  \\\n",
              "0  0.030862 -0.036383 -0.009282 -0.062328 -0.072415  0.034452 -0.083957   \n",
              "1  0.112500 -0.041443 -0.101257 -0.134863 -0.084351  0.068066 -0.039642   \n",
              "2  0.247070 -0.165283 -0.127075  0.090698 -0.168457  0.077637  0.144043   \n",
              "\n",
              "       t281      t282      t283      t284      t285      t286      t287  \\\n",
              "0  0.016255  0.060856  0.006026  0.028193 -0.031367 -0.000033 -0.047677   \n",
              "1 -0.040778  0.043164 -0.095618 -0.041675 -0.105194 -0.090332  0.007379   \n",
              "2  0.117676  0.150391  0.022766 -0.079834 -0.053833  0.167969 -0.052795   \n",
              "\n",
              "       t288      t289      t290      t291      t292      t293      t294  \\\n",
              "0 -0.020241 -0.028700 -0.130510  0.013438 -0.092540  0.014499 -0.030982   \n",
              "1 -0.006592  0.013765  0.033325  0.078423 -0.067712 -0.015063 -0.014386   \n",
              "2  0.128540  0.190918 -0.293457  0.030273 -0.139038  0.134399  0.050659   \n",
              "\n",
              "       t295      t296      t297      t298      t299  \n",
              "0 -0.088679 -0.039396 -0.026700 -0.070457 -0.012906  \n",
              "1 -0.140498  0.078174 -0.022119 -0.026172 -0.024219  \n",
              "2 -0.037354  0.042969 -0.079590  0.059326 -0.248047  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "      <th>128</th>\n",
              "      <th>129</th>\n",
              "      <th>130</th>\n",
              "      <th>131</th>\n",
              "      <th>132</th>\n",
              "      <th>133</th>\n",
              "      <th>134</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "      <th>141</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>144</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "      <th>151</th>\n",
              "      <th>152</th>\n",
              "      <th>153</th>\n",
              "      <th>154</th>\n",
              "      <th>155</th>\n",
              "      <th>156</th>\n",
              "      <th>157</th>\n",
              "      <th>158</th>\n",
              "      <th>159</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "      <th>200</th>\n",
              "      <th>201</th>\n",
              "      <th>202</th>\n",
              "      <th>203</th>\n",
              "      <th>204</th>\n",
              "      <th>205</th>\n",
              "      <th>206</th>\n",
              "      <th>207</th>\n",
              "      <th>208</th>\n",
              "      <th>209</th>\n",
              "      <th>210</th>\n",
              "      <th>211</th>\n",
              "      <th>212</th>\n",
              "      <th>213</th>\n",
              "      <th>214</th>\n",
              "      <th>215</th>\n",
              "      <th>216</th>\n",
              "      <th>217</th>\n",
              "      <th>218</th>\n",
              "      <th>219</th>\n",
              "      <th>220</th>\n",
              "      <th>221</th>\n",
              "      <th>222</th>\n",
              "      <th>223</th>\n",
              "      <th>224</th>\n",
              "      <th>225</th>\n",
              "      <th>226</th>\n",
              "      <th>227</th>\n",
              "      <th>228</th>\n",
              "      <th>229</th>\n",
              "      <th>230</th>\n",
              "      <th>231</th>\n",
              "      <th>232</th>\n",
              "      <th>233</th>\n",
              "      <th>234</th>\n",
              "      <th>235</th>\n",
              "      <th>236</th>\n",
              "      <th>237</th>\n",
              "      <th>238</th>\n",
              "      <th>239</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "      <th>244</th>\n",
              "      <th>245</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "      <th>259</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "      <th>t0</th>\n",
              "      <th>t1</th>\n",
              "      <th>t2</th>\n",
              "      <th>t3</th>\n",
              "      <th>t4</th>\n",
              "      <th>t5</th>\n",
              "      <th>t6</th>\n",
              "      <th>t7</th>\n",
              "      <th>t8</th>\n",
              "      <th>t9</th>\n",
              "      <th>t10</th>\n",
              "      <th>t11</th>\n",
              "      <th>t12</th>\n",
              "      <th>t13</th>\n",
              "      <th>t14</th>\n",
              "      <th>t15</th>\n",
              "      <th>t16</th>\n",
              "      <th>t17</th>\n",
              "      <th>t18</th>\n",
              "      <th>t19</th>\n",
              "      <th>t20</th>\n",
              "      <th>t21</th>\n",
              "      <th>t22</th>\n",
              "      <th>t23</th>\n",
              "      <th>t24</th>\n",
              "      <th>t25</th>\n",
              "      <th>t26</th>\n",
              "      <th>t27</th>\n",
              "      <th>t28</th>\n",
              "      <th>t29</th>\n",
              "      <th>t30</th>\n",
              "      <th>t31</th>\n",
              "      <th>t32</th>\n",
              "      <th>t33</th>\n",
              "      <th>t34</th>\n",
              "      <th>t35</th>\n",
              "      <th>t36</th>\n",
              "      <th>t37</th>\n",
              "      <th>t38</th>\n",
              "      <th>t39</th>\n",
              "      <th>t40</th>\n",
              "      <th>t41</th>\n",
              "      <th>t42</th>\n",
              "      <th>t43</th>\n",
              "      <th>t44</th>\n",
              "      <th>t45</th>\n",
              "      <th>t46</th>\n",
              "      <th>t47</th>\n",
              "      <th>t48</th>\n",
              "      <th>t49</th>\n",
              "      <th>t50</th>\n",
              "      <th>t51</th>\n",
              "      <th>t52</th>\n",
              "      <th>t53</th>\n",
              "      <th>t54</th>\n",
              "      <th>t55</th>\n",
              "      <th>t56</th>\n",
              "      <th>t57</th>\n",
              "      <th>t58</th>\n",
              "      <th>t59</th>\n",
              "      <th>t60</th>\n",
              "      <th>t61</th>\n",
              "      <th>t62</th>\n",
              "      <th>t63</th>\n",
              "      <th>t64</th>\n",
              "      <th>t65</th>\n",
              "      <th>t66</th>\n",
              "      <th>t67</th>\n",
              "      <th>t68</th>\n",
              "      <th>t69</th>\n",
              "      <th>t70</th>\n",
              "      <th>t71</th>\n",
              "      <th>t72</th>\n",
              "      <th>t73</th>\n",
              "      <th>t74</th>\n",
              "      <th>t75</th>\n",
              "      <th>t76</th>\n",
              "      <th>t77</th>\n",
              "      <th>t78</th>\n",
              "      <th>t79</th>\n",
              "      <th>t80</th>\n",
              "      <th>t81</th>\n",
              "      <th>t82</th>\n",
              "      <th>t83</th>\n",
              "      <th>t84</th>\n",
              "      <th>t85</th>\n",
              "      <th>t86</th>\n",
              "      <th>t87</th>\n",
              "      <th>t88</th>\n",
              "      <th>t89</th>\n",
              "      <th>t90</th>\n",
              "      <th>t91</th>\n",
              "      <th>t92</th>\n",
              "      <th>t93</th>\n",
              "      <th>t94</th>\n",
              "      <th>t95</th>\n",
              "      <th>t96</th>\n",
              "      <th>t97</th>\n",
              "      <th>t98</th>\n",
              "      <th>t99</th>\n",
              "      <th>t100</th>\n",
              "      <th>t101</th>\n",
              "      <th>t102</th>\n",
              "      <th>t103</th>\n",
              "      <th>t104</th>\n",
              "      <th>t105</th>\n",
              "      <th>t106</th>\n",
              "      <th>t107</th>\n",
              "      <th>t108</th>\n",
              "      <th>t109</th>\n",
              "      <th>t110</th>\n",
              "      <th>t111</th>\n",
              "      <th>t112</th>\n",
              "      <th>t113</th>\n",
              "      <th>t114</th>\n",
              "      <th>t115</th>\n",
              "      <th>t116</th>\n",
              "      <th>t117</th>\n",
              "      <th>t118</th>\n",
              "      <th>t119</th>\n",
              "      <th>t120</th>\n",
              "      <th>t121</th>\n",
              "      <th>t122</th>\n",
              "      <th>t123</th>\n",
              "      <th>t124</th>\n",
              "      <th>t125</th>\n",
              "      <th>t126</th>\n",
              "      <th>t127</th>\n",
              "      <th>t128</th>\n",
              "      <th>t129</th>\n",
              "      <th>t130</th>\n",
              "      <th>t131</th>\n",
              "      <th>t132</th>\n",
              "      <th>t133</th>\n",
              "      <th>t134</th>\n",
              "      <th>t135</th>\n",
              "      <th>t136</th>\n",
              "      <th>t137</th>\n",
              "      <th>t138</th>\n",
              "      <th>t139</th>\n",
              "      <th>t140</th>\n",
              "      <th>t141</th>\n",
              "      <th>t142</th>\n",
              "      <th>t143</th>\n",
              "      <th>t144</th>\n",
              "      <th>t145</th>\n",
              "      <th>t146</th>\n",
              "      <th>t147</th>\n",
              "      <th>t148</th>\n",
              "      <th>t149</th>\n",
              "      <th>t150</th>\n",
              "      <th>t151</th>\n",
              "      <th>t152</th>\n",
              "      <th>t153</th>\n",
              "      <th>t154</th>\n",
              "      <th>t155</th>\n",
              "      <th>t156</th>\n",
              "      <th>t157</th>\n",
              "      <th>t158</th>\n",
              "      <th>t159</th>\n",
              "      <th>t160</th>\n",
              "      <th>t161</th>\n",
              "      <th>t162</th>\n",
              "      <th>t163</th>\n",
              "      <th>t164</th>\n",
              "      <th>t165</th>\n",
              "      <th>t166</th>\n",
              "      <th>t167</th>\n",
              "      <th>t168</th>\n",
              "      <th>t169</th>\n",
              "      <th>t170</th>\n",
              "      <th>t171</th>\n",
              "      <th>t172</th>\n",
              "      <th>t173</th>\n",
              "      <th>t174</th>\n",
              "      <th>t175</th>\n",
              "      <th>t176</th>\n",
              "      <th>t177</th>\n",
              "      <th>t178</th>\n",
              "      <th>t179</th>\n",
              "      <th>t180</th>\n",
              "      <th>t181</th>\n",
              "      <th>t182</th>\n",
              "      <th>t183</th>\n",
              "      <th>t184</th>\n",
              "      <th>t185</th>\n",
              "      <th>t186</th>\n",
              "      <th>t187</th>\n",
              "      <th>t188</th>\n",
              "      <th>t189</th>\n",
              "      <th>t190</th>\n",
              "      <th>t191</th>\n",
              "      <th>t192</th>\n",
              "      <th>t193</th>\n",
              "      <th>t194</th>\n",
              "      <th>t195</th>\n",
              "      <th>t196</th>\n",
              "      <th>t197</th>\n",
              "      <th>t198</th>\n",
              "      <th>t199</th>\n",
              "      <th>t200</th>\n",
              "      <th>t201</th>\n",
              "      <th>t202</th>\n",
              "      <th>t203</th>\n",
              "      <th>t204</th>\n",
              "      <th>t205</th>\n",
              "      <th>t206</th>\n",
              "      <th>t207</th>\n",
              "      <th>t208</th>\n",
              "      <th>t209</th>\n",
              "      <th>t210</th>\n",
              "      <th>t211</th>\n",
              "      <th>t212</th>\n",
              "      <th>t213</th>\n",
              "      <th>t214</th>\n",
              "      <th>t215</th>\n",
              "      <th>t216</th>\n",
              "      <th>t217</th>\n",
              "      <th>t218</th>\n",
              "      <th>t219</th>\n",
              "      <th>t220</th>\n",
              "      <th>t221</th>\n",
              "      <th>t222</th>\n",
              "      <th>t223</th>\n",
              "      <th>t224</th>\n",
              "      <th>t225</th>\n",
              "      <th>t226</th>\n",
              "      <th>t227</th>\n",
              "      <th>t228</th>\n",
              "      <th>t229</th>\n",
              "      <th>t230</th>\n",
              "      <th>t231</th>\n",
              "      <th>t232</th>\n",
              "      <th>t233</th>\n",
              "      <th>t234</th>\n",
              "      <th>t235</th>\n",
              "      <th>t236</th>\n",
              "      <th>t237</th>\n",
              "      <th>t238</th>\n",
              "      <th>t239</th>\n",
              "      <th>t240</th>\n",
              "      <th>t241</th>\n",
              "      <th>t242</th>\n",
              "      <th>t243</th>\n",
              "      <th>t244</th>\n",
              "      <th>t245</th>\n",
              "      <th>t246</th>\n",
              "      <th>t247</th>\n",
              "      <th>t248</th>\n",
              "      <th>t249</th>\n",
              "      <th>t250</th>\n",
              "      <th>t251</th>\n",
              "      <th>t252</th>\n",
              "      <th>t253</th>\n",
              "      <th>t254</th>\n",
              "      <th>t255</th>\n",
              "      <th>t256</th>\n",
              "      <th>t257</th>\n",
              "      <th>t258</th>\n",
              "      <th>t259</th>\n",
              "      <th>t260</th>\n",
              "      <th>t261</th>\n",
              "      <th>t262</th>\n",
              "      <th>t263</th>\n",
              "      <th>t264</th>\n",
              "      <th>t265</th>\n",
              "      <th>t266</th>\n",
              "      <th>t267</th>\n",
              "      <th>t268</th>\n",
              "      <th>t269</th>\n",
              "      <th>t270</th>\n",
              "      <th>t271</th>\n",
              "      <th>t272</th>\n",
              "      <th>t273</th>\n",
              "      <th>t274</th>\n",
              "      <th>t275</th>\n",
              "      <th>t276</th>\n",
              "      <th>t277</th>\n",
              "      <th>t278</th>\n",
              "      <th>t279</th>\n",
              "      <th>t280</th>\n",
              "      <th>t281</th>\n",
              "      <th>t282</th>\n",
              "      <th>t283</th>\n",
              "      <th>t284</th>\n",
              "      <th>t285</th>\n",
              "      <th>t286</th>\n",
              "      <th>t287</th>\n",
              "      <th>t288</th>\n",
              "      <th>t289</th>\n",
              "      <th>t290</th>\n",
              "      <th>t291</th>\n",
              "      <th>t292</th>\n",
              "      <th>t293</th>\n",
              "      <th>t294</th>\n",
              "      <th>t295</th>\n",
              "      <th>t296</th>\n",
              "      <th>t297</th>\n",
              "      <th>t298</th>\n",
              "      <th>t299</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.051895</td>\n",
              "      <td>0.037328</td>\n",
              "      <td>-0.034177</td>\n",
              "      <td>0.101743</td>\n",
              "      <td>-0.058414</td>\n",
              "      <td>0.054615</td>\n",
              "      <td>-0.041165</td>\n",
              "      <td>-0.046734</td>\n",
              "      <td>0.038159</td>\n",
              "      <td>0.087764</td>\n",
              "      <td>0.003641</td>\n",
              "      <td>-0.067222</td>\n",
              "      <td>-0.049265</td>\n",
              "      <td>0.003996</td>\n",
              "      <td>-0.131833</td>\n",
              "      <td>0.101383</td>\n",
              "      <td>0.028708</td>\n",
              "      <td>0.075796</td>\n",
              "      <td>-0.012462</td>\n",
              "      <td>-0.011240</td>\n",
              "      <td>0.008695</td>\n",
              "      <td>0.041100</td>\n",
              "      <td>0.084000</td>\n",
              "      <td>0.001998</td>\n",
              "      <td>0.057393</td>\n",
              "      <td>-0.026329</td>\n",
              "      <td>-0.040030</td>\n",
              "      <td>0.017978</td>\n",
              "      <td>0.063161</td>\n",
              "      <td>-0.043544</td>\n",
              "      <td>-0.017762</td>\n",
              "      <td>0.047229</td>\n",
              "      <td>0.000327</td>\n",
              "      <td>-0.043785</td>\n",
              "      <td>0.012311</td>\n",
              "      <td>-0.032266</td>\n",
              "      <td>0.040161</td>\n",
              "      <td>0.030547</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>0.101680</td>\n",
              "      <td>0.085920</td>\n",
              "      <td>-0.113623</td>\n",
              "      <td>0.134226</td>\n",
              "      <td>-0.021433</td>\n",
              "      <td>-0.023916</td>\n",
              "      <td>-0.075753</td>\n",
              "      <td>-0.078853</td>\n",
              "      <td>-0.009371</td>\n",
              "      <td>0.002723</td>\n",
              "      <td>0.022405</td>\n",
              "      <td>-0.005610</td>\n",
              "      <td>0.115219</td>\n",
              "      <td>0.019362</td>\n",
              "      <td>-0.019095</td>\n",
              "      <td>0.001305</td>\n",
              "      <td>-0.028062</td>\n",
              "      <td>-0.047248</td>\n",
              "      <td>-0.083741</td>\n",
              "      <td>0.019699</td>\n",
              "      <td>-0.085840</td>\n",
              "      <td>-0.016829</td>\n",
              "      <td>0.037920</td>\n",
              "      <td>-0.076820</td>\n",
              "      <td>-0.002485</td>\n",
              "      <td>0.009050</td>\n",
              "      <td>-0.073896</td>\n",
              "      <td>-0.065966</td>\n",
              "      <td>0.052086</td>\n",
              "      <td>0.028344</td>\n",
              "      <td>0.036428</td>\n",
              "      <td>0.041802</td>\n",
              "      <td>0.028025</td>\n",
              "      <td>0.055240</td>\n",
              "      <td>-0.009629</td>\n",
              "      <td>-0.161032</td>\n",
              "      <td>-0.020440</td>\n",
              "      <td>0.031243</td>\n",
              "      <td>0.071313</td>\n",
              "      <td>-0.021661</td>\n",
              "      <td>-0.021817</td>\n",
              "      <td>-0.086409</td>\n",
              "      <td>-0.039698</td>\n",
              "      <td>0.012998</td>\n",
              "      <td>0.000663</td>\n",
              "      <td>-0.056211</td>\n",
              "      <td>-0.004524</td>\n",
              "      <td>-0.060419</td>\n",
              "      <td>0.085980</td>\n",
              "      <td>0.031487</td>\n",
              "      <td>0.028893</td>\n",
              "      <td>-0.004157</td>\n",
              "      <td>0.061732</td>\n",
              "      <td>-0.068722</td>\n",
              "      <td>-0.113466</td>\n",
              "      <td>-0.067157</td>\n",
              "      <td>-0.034516</td>\n",
              "      <td>0.039504</td>\n",
              "      <td>0.064622</td>\n",
              "      <td>0.074289</td>\n",
              "      <td>-0.057171</td>\n",
              "      <td>-0.075850</td>\n",
              "      <td>-0.036300</td>\n",
              "      <td>0.065043</td>\n",
              "      <td>-0.015186</td>\n",
              "      <td>0.010391</td>\n",
              "      <td>-0.045006</td>\n",
              "      <td>0.004535</td>\n",
              "      <td>-0.006627</td>\n",
              "      <td>0.064333</td>\n",
              "      <td>-0.068010</td>\n",
              "      <td>-0.052673</td>\n",
              "      <td>-0.007005</td>\n",
              "      <td>-0.006933</td>\n",
              "      <td>-0.014125</td>\n",
              "      <td>0.090703</td>\n",
              "      <td>0.041767</td>\n",
              "      <td>0.017151</td>\n",
              "      <td>-0.055764</td>\n",
              "      <td>0.012064</td>\n",
              "      <td>0.042873</td>\n",
              "      <td>0.007832</td>\n",
              "      <td>0.009667</td>\n",
              "      <td>-0.078502</td>\n",
              "      <td>0.091327</td>\n",
              "      <td>0.053238</td>\n",
              "      <td>0.028802</td>\n",
              "      <td>-0.009848</td>\n",
              "      <td>-0.019586</td>\n",
              "      <td>0.058703</td>\n",
              "      <td>0.018897</td>\n",
              "      <td>-0.067695</td>\n",
              "      <td>-0.079420</td>\n",
              "      <td>-0.035956</td>\n",
              "      <td>-0.004591</td>\n",
              "      <td>-0.036042</td>\n",
              "      <td>0.023869</td>\n",
              "      <td>0.018929</td>\n",
              "      <td>0.004610</td>\n",
              "      <td>-0.018475</td>\n",
              "      <td>0.058678</td>\n",
              "      <td>0.056634</td>\n",
              "      <td>-0.106849</td>\n",
              "      <td>0.048059</td>\n",
              "      <td>0.013481</td>\n",
              "      <td>-0.016872</td>\n",
              "      <td>0.022587</td>\n",
              "      <td>-0.035953</td>\n",
              "      <td>-0.055542</td>\n",
              "      <td>-0.026154</td>\n",
              "      <td>-0.079539</td>\n",
              "      <td>0.086903</td>\n",
              "      <td>-0.038297</td>\n",
              "      <td>-0.048888</td>\n",
              "      <td>0.093569</td>\n",
              "      <td>-0.066556</td>\n",
              "      <td>-0.000924</td>\n",
              "      <td>-0.093473</td>\n",
              "      <td>-0.054987</td>\n",
              "      <td>-0.037182</td>\n",
              "      <td>-0.046404</td>\n",
              "      <td>-0.041381</td>\n",
              "      <td>0.066814</td>\n",
              "      <td>0.068162</td>\n",
              "      <td>-0.022368</td>\n",
              "      <td>0.018231</td>\n",
              "      <td>-0.103666</td>\n",
              "      <td>0.056653</td>\n",
              "      <td>-0.108814</td>\n",
              "      <td>0.012367</td>\n",
              "      <td>0.011420</td>\n",
              "      <td>-0.144671</td>\n",
              "      <td>-0.009916</td>\n",
              "      <td>-0.041884</td>\n",
              "      <td>-0.001014</td>\n",
              "      <td>-0.029669</td>\n",
              "      <td>0.011479</td>\n",
              "      <td>0.093996</td>\n",
              "      <td>-0.030344</td>\n",
              "      <td>0.022801</td>\n",
              "      <td>0.045775</td>\n",
              "      <td>-0.044797</td>\n",
              "      <td>-0.080822</td>\n",
              "      <td>0.023369</td>\n",
              "      <td>0.040966</td>\n",
              "      <td>0.005505</td>\n",
              "      <td>-0.038203</td>\n",
              "      <td>-0.042134</td>\n",
              "      <td>0.061333</td>\n",
              "      <td>0.069623</td>\n",
              "      <td>0.003920</td>\n",
              "      <td>0.021034</td>\n",
              "      <td>0.012171</td>\n",
              "      <td>0.004338</td>\n",
              "      <td>0.049119</td>\n",
              "      <td>-0.008002</td>\n",
              "      <td>0.041074</td>\n",
              "      <td>0.011200</td>\n",
              "      <td>0.025526</td>\n",
              "      <td>-0.052760</td>\n",
              "      <td>-0.025777</td>\n",
              "      <td>0.005203</td>\n",
              "      <td>0.095475</td>\n",
              "      <td>-0.096175</td>\n",
              "      <td>0.014684</td>\n",
              "      <td>-0.000366</td>\n",
              "      <td>-0.020392</td>\n",
              "      <td>-0.060521</td>\n",
              "      <td>-0.020938</td>\n",
              "      <td>0.050421</td>\n",
              "      <td>0.001144</td>\n",
              "      <td>0.060202</td>\n",
              "      <td>0.052409</td>\n",
              "      <td>-0.031468</td>\n",
              "      <td>0.037297</td>\n",
              "      <td>-0.044965</td>\n",
              "      <td>0.051063</td>\n",
              "      <td>0.052388</td>\n",
              "      <td>0.006491</td>\n",
              "      <td>-0.129092</td>\n",
              "      <td>-0.052728</td>\n",
              "      <td>-0.050609</td>\n",
              "      <td>0.026036</td>\n",
              "      <td>0.004733</td>\n",
              "      <td>0.042223</td>\n",
              "      <td>0.084900</td>\n",
              "      <td>-0.054232</td>\n",
              "      <td>0.016700</td>\n",
              "      <td>-0.018987</td>\n",
              "      <td>-0.004762</td>\n",
              "      <td>-0.033665</td>\n",
              "      <td>0.027724</td>\n",
              "      <td>-0.022977</td>\n",
              "      <td>-0.009939</td>\n",
              "      <td>-0.017045</td>\n",
              "      <td>0.034892</td>\n",
              "      <td>0.041107</td>\n",
              "      <td>0.000366</td>\n",
              "      <td>-0.053192</td>\n",
              "      <td>0.066096</td>\n",
              "      <td>0.036053</td>\n",
              "      <td>0.103144</td>\n",
              "      <td>0.002409</td>\n",
              "      <td>-0.029148</td>\n",
              "      <td>-0.118922</td>\n",
              "      <td>-0.026632</td>\n",
              "      <td>0.042847</td>\n",
              "      <td>-0.018780</td>\n",
              "      <td>0.022491</td>\n",
              "      <td>0.018803</td>\n",
              "      <td>-0.052682</td>\n",
              "      <td>0.036296</td>\n",
              "      <td>0.058932</td>\n",
              "      <td>0.048758</td>\n",
              "      <td>0.086733</td>\n",
              "      <td>0.015181</td>\n",
              "      <td>-0.020578</td>\n",
              "      <td>-0.002301</td>\n",
              "      <td>0.017927</td>\n",
              "      <td>-0.082257</td>\n",
              "      <td>-0.097707</td>\n",
              "      <td>0.013906</td>\n",
              "      <td>-0.023128</td>\n",
              "      <td>-0.052425</td>\n",
              "      <td>0.012839</td>\n",
              "      <td>0.044951</td>\n",
              "      <td>0.075966</td>\n",
              "      <td>-0.060223</td>\n",
              "      <td>0.018071</td>\n",
              "      <td>-0.014205</td>\n",
              "      <td>-0.000439</td>\n",
              "      <td>-0.046774</td>\n",
              "      <td>0.044653</td>\n",
              "      <td>0.099763</td>\n",
              "      <td>-0.012508</td>\n",
              "      <td>0.080197</td>\n",
              "      <td>-0.018646</td>\n",
              "      <td>-0.061245</td>\n",
              "      <td>-0.081346</td>\n",
              "      <td>-0.074165</td>\n",
              "      <td>0.027213</td>\n",
              "      <td>-0.018275</td>\n",
              "      <td>0.013956</td>\n",
              "      <td>0.067887</td>\n",
              "      <td>0.098910</td>\n",
              "      <td>0.051680</td>\n",
              "      <td>-0.012935</td>\n",
              "      <td>-0.086912</td>\n",
              "      <td>0.039813</td>\n",
              "      <td>0.019609</td>\n",
              "      <td>0.034114</td>\n",
              "      <td>0.016492</td>\n",
              "      <td>0.101426</td>\n",
              "      <td>-0.098417</td>\n",
              "      <td>-0.008000</td>\n",
              "      <td>-0.042937</td>\n",
              "      <td>-0.026282</td>\n",
              "      <td>-0.011188</td>\n",
              "      <td>-0.034331</td>\n",
              "      <td>-0.041593</td>\n",
              "      <td>-0.018652</td>\n",
              "      <td>0.004442</td>\n",
              "      <td>0.110379</td>\n",
              "      <td>0.029680</td>\n",
              "      <td>0.050332</td>\n",
              "      <td>-0.094188</td>\n",
              "      <td>0.014221</td>\n",
              "      <td>-0.066811</td>\n",
              "      <td>-0.130055</td>\n",
              "      <td>0.057018</td>\n",
              "      <td>0.074929</td>\n",
              "      <td>-0.026917</td>\n",
              "      <td>-0.095598</td>\n",
              "      <td>0.011858</td>\n",
              "      <td>-0.073314</td>\n",
              "      <td>-0.091153</td>\n",
              "      <td>0.080200</td>\n",
              "      <td>0.005538</td>\n",
              "      <td>0.060813</td>\n",
              "      <td>-0.000502</td>\n",
              "      <td>-0.062600</td>\n",
              "      <td>0.017908</td>\n",
              "      <td>0.009444</td>\n",
              "      <td>-0.043845</td>\n",
              "      <td>0.023987</td>\n",
              "      <td>-0.011497</td>\n",
              "      <td>0.029896</td>\n",
              "      <td>-0.156672</td>\n",
              "      <td>0.107794</td>\n",
              "      <td>0.030462</td>\n",
              "      <td>-0.059143</td>\n",
              "      <td>-0.018854</td>\n",
              "      <td>0.017811</td>\n",
              "      <td>-0.020064</td>\n",
              "      <td>0.121718</td>\n",
              "      <td>0.027555</td>\n",
              "      <td>-0.016341</td>\n",
              "      <td>0.032487</td>\n",
              "      <td>0.010143</td>\n",
              "      <td>-0.018625</td>\n",
              "      <td>-0.014671</td>\n",
              "      <td>0.024180</td>\n",
              "      <td>-0.052368</td>\n",
              "      <td>0.040882</td>\n",
              "      <td>0.031294</td>\n",
              "      <td>0.026090</td>\n",
              "      <td>-0.125816</td>\n",
              "      <td>-0.054099</td>\n",
              "      <td>0.065482</td>\n",
              "      <td>0.091919</td>\n",
              "      <td>0.048230</td>\n",
              "      <td>-0.022039</td>\n",
              "      <td>0.026163</td>\n",
              "      <td>-0.023338</td>\n",
              "      <td>-0.036591</td>\n",
              "      <td>0.058660</td>\n",
              "      <td>-0.000732</td>\n",
              "      <td>-0.052091</td>\n",
              "      <td>-0.062472</td>\n",
              "      <td>0.063044</td>\n",
              "      <td>-0.113015</td>\n",
              "      <td>-0.019875</td>\n",
              "      <td>0.030806</td>\n",
              "      <td>-0.073398</td>\n",
              "      <td>-0.032360</td>\n",
              "      <td>-0.001298</td>\n",
              "      <td>-0.053411</td>\n",
              "      <td>-0.049039</td>\n",
              "      <td>0.051447</td>\n",
              "      <td>0.011314</td>\n",
              "      <td>0.114367</td>\n",
              "      <td>0.109385</td>\n",
              "      <td>0.035894</td>\n",
              "      <td>0.060073</td>\n",
              "      <td>0.017412</td>\n",
              "      <td>-0.078097</td>\n",
              "      <td>-0.040300</td>\n",
              "      <td>0.095834</td>\n",
              "      <td>0.045030</td>\n",
              "      <td>-0.000794</td>\n",
              "      <td>-0.003540</td>\n",
              "      <td>0.071563</td>\n",
              "      <td>-0.010137</td>\n",
              "      <td>0.014205</td>\n",
              "      <td>-0.020541</td>\n",
              "      <td>-0.052723</td>\n",
              "      <td>-0.005993</td>\n",
              "      <td>-0.005693</td>\n",
              "      <td>0.149625</td>\n",
              "      <td>0.013894</td>\n",
              "      <td>0.013161</td>\n",
              "      <td>0.048676</td>\n",
              "      <td>-0.062112</td>\n",
              "      <td>-0.039655</td>\n",
              "      <td>-0.025914</td>\n",
              "      <td>0.020169</td>\n",
              "      <td>-0.018444</td>\n",
              "      <td>-0.114774</td>\n",
              "      <td>-0.075290</td>\n",
              "      <td>0.052856</td>\n",
              "      <td>0.014759</td>\n",
              "      <td>0.049572</td>\n",
              "      <td>-0.030538</td>\n",
              "      <td>-0.003890</td>\n",
              "      <td>0.036954</td>\n",
              "      <td>-0.063743</td>\n",
              "      <td>-0.041883</td>\n",
              "      <td>0.005612</td>\n",
              "      <td>0.064248</td>\n",
              "      <td>-0.005346</td>\n",
              "      <td>-0.029982</td>\n",
              "      <td>0.033203</td>\n",
              "      <td>0.010628</td>\n",
              "      <td>-0.071422</td>\n",
              "      <td>-0.053151</td>\n",
              "      <td>0.041149</td>\n",
              "      <td>0.052809</td>\n",
              "      <td>-0.017057</td>\n",
              "      <td>-0.040638</td>\n",
              "      <td>0.053520</td>\n",
              "      <td>0.057617</td>\n",
              "      <td>-0.005815</td>\n",
              "      <td>-0.155806</td>\n",
              "      <td>-0.013117</td>\n",
              "      <td>0.117254</td>\n",
              "      <td>0.063010</td>\n",
              "      <td>-0.008609</td>\n",
              "      <td>0.014107</td>\n",
              "      <td>-0.009610</td>\n",
              "      <td>0.123281</td>\n",
              "      <td>0.041504</td>\n",
              "      <td>0.060613</td>\n",
              "      <td>-0.089697</td>\n",
              "      <td>-0.088568</td>\n",
              "      <td>0.032582</td>\n",
              "      <td>-0.011282</td>\n",
              "      <td>0.072585</td>\n",
              "      <td>-0.036643</td>\n",
              "      <td>-0.017361</td>\n",
              "      <td>-0.047641</td>\n",
              "      <td>0.098666</td>\n",
              "      <td>-0.010476</td>\n",
              "      <td>-0.084445</td>\n",
              "      <td>0.045787</td>\n",
              "      <td>-0.000666</td>\n",
              "      <td>0.033580</td>\n",
              "      <td>-0.056388</td>\n",
              "      <td>-0.014804</td>\n",
              "      <td>0.016768</td>\n",
              "      <td>-0.016688</td>\n",
              "      <td>-0.018676</td>\n",
              "      <td>0.074019</td>\n",
              "      <td>-0.084339</td>\n",
              "      <td>-0.042025</td>\n",
              "      <td>0.070657</td>\n",
              "      <td>0.003070</td>\n",
              "      <td>-0.029180</td>\n",
              "      <td>-0.139393</td>\n",
              "      <td>0.022228</td>\n",
              "      <td>-0.044123</td>\n",
              "      <td>0.053151</td>\n",
              "      <td>0.037864</td>\n",
              "      <td>0.021118</td>\n",
              "      <td>0.007016</td>\n",
              "      <td>-0.021040</td>\n",
              "      <td>0.041519</td>\n",
              "      <td>-0.040871</td>\n",
              "      <td>0.107455</td>\n",
              "      <td>0.002960</td>\n",
              "      <td>0.015791</td>\n",
              "      <td>0.007790</td>\n",
              "      <td>-0.100822</td>\n",
              "      <td>-0.049408</td>\n",
              "      <td>-0.032812</td>\n",
              "      <td>-0.038358</td>\n",
              "      <td>-0.032005</td>\n",
              "      <td>-0.004494</td>\n",
              "      <td>0.120062</td>\n",
              "      <td>-0.144465</td>\n",
              "      <td>-0.047530</td>\n",
              "      <td>0.100048</td>\n",
              "      <td>-0.080145</td>\n",
              "      <td>-0.072188</td>\n",
              "      <td>-0.017001</td>\n",
              "      <td>0.082586</td>\n",
              "      <td>0.011271</td>\n",
              "      <td>0.018682</td>\n",
              "      <td>0.073875</td>\n",
              "      <td>0.073836</td>\n",
              "      <td>-0.014970</td>\n",
              "      <td>0.061257</td>\n",
              "      <td>-0.009455</td>\n",
              "      <td>0.022261</td>\n",
              "      <td>0.062506</td>\n",
              "      <td>0.046353</td>\n",
              "      <td>-0.070817</td>\n",
              "      <td>0.006303</td>\n",
              "      <td>-0.011968</td>\n",
              "      <td>-0.014515</td>\n",
              "      <td>0.048345</td>\n",
              "      <td>-0.030784</td>\n",
              "      <td>0.010282</td>\n",
              "      <td>0.109222</td>\n",
              "      <td>-0.087562</td>\n",
              "      <td>0.013050</td>\n",
              "      <td>-0.051114</td>\n",
              "      <td>-0.041482</td>\n",
              "      <td>0.039540</td>\n",
              "      <td>0.010509</td>\n",
              "      <td>0.100701</td>\n",
              "      <td>-0.035606</td>\n",
              "      <td>-0.009999</td>\n",
              "      <td>0.045122</td>\n",
              "      <td>-0.090132</td>\n",
              "      <td>0.114459</td>\n",
              "      <td>-0.012068</td>\n",
              "      <td>0.031195</td>\n",
              "      <td>0.085574</td>\n",
              "      <td>-0.004328</td>\n",
              "      <td>-0.124545</td>\n",
              "      <td>0.039407</td>\n",
              "      <td>-0.036588</td>\n",
              "      <td>0.092751</td>\n",
              "      <td>-0.003435</td>\n",
              "      <td>0.035624</td>\n",
              "      <td>-0.000236</td>\n",
              "      <td>-0.075739</td>\n",
              "      <td>0.037828</td>\n",
              "      <td>-0.000441</td>\n",
              "      <td>0.011838</td>\n",
              "      <td>-0.084488</td>\n",
              "      <td>0.043875</td>\n",
              "      <td>-0.021368</td>\n",
              "      <td>0.056835</td>\n",
              "      <td>-0.029683</td>\n",
              "      <td>-0.003407</td>\n",
              "      <td>-0.010703</td>\n",
              "      <td>-0.022872</td>\n",
              "      <td>0.041926</td>\n",
              "      <td>0.078635</td>\n",
              "      <td>0.017168</td>\n",
              "      <td>0.037098</td>\n",
              "      <td>-0.049500</td>\n",
              "      <td>0.028617</td>\n",
              "      <td>-0.109264</td>\n",
              "      <td>0.083523</td>\n",
              "      <td>0.012756</td>\n",
              "      <td>-0.093439</td>\n",
              "      <td>0.070401</td>\n",
              "      <td>0.075828</td>\n",
              "      <td>-0.022039</td>\n",
              "      <td>0.031403</td>\n",
              "      <td>0.054291</td>\n",
              "      <td>0.063883</td>\n",
              "      <td>0.105386</td>\n",
              "      <td>0.028265</td>\n",
              "      <td>0.002674</td>\n",
              "      <td>-0.023482</td>\n",
              "      <td>0.040991</td>\n",
              "      <td>-0.042487</td>\n",
              "      <td>-0.066567</td>\n",
              "      <td>0.092205</td>\n",
              "      <td>0.009408</td>\n",
              "      <td>-0.083651</td>\n",
              "      <td>-0.018915</td>\n",
              "      <td>0.055505</td>\n",
              "      <td>0.037384</td>\n",
              "      <td>-0.007301</td>\n",
              "      <td>-0.051247</td>\n",
              "      <td>-0.115146</td>\n",
              "      <td>0.037015</td>\n",
              "      <td>-0.025568</td>\n",
              "      <td>-0.056647</td>\n",
              "      <td>-0.019456</td>\n",
              "      <td>0.019936</td>\n",
              "      <td>0.030862</td>\n",
              "      <td>-0.036383</td>\n",
              "      <td>-0.009282</td>\n",
              "      <td>-0.062328</td>\n",
              "      <td>-0.072415</td>\n",
              "      <td>0.034452</td>\n",
              "      <td>-0.083957</td>\n",
              "      <td>0.016255</td>\n",
              "      <td>0.060856</td>\n",
              "      <td>0.006026</td>\n",
              "      <td>0.028193</td>\n",
              "      <td>-0.031367</td>\n",
              "      <td>-0.000033</td>\n",
              "      <td>-0.047677</td>\n",
              "      <td>-0.020241</td>\n",
              "      <td>-0.028700</td>\n",
              "      <td>-0.130510</td>\n",
              "      <td>0.013438</td>\n",
              "      <td>-0.092540</td>\n",
              "      <td>0.014499</td>\n",
              "      <td>-0.030982</td>\n",
              "      <td>-0.088679</td>\n",
              "      <td>-0.039396</td>\n",
              "      <td>-0.026700</td>\n",
              "      <td>-0.070457</td>\n",
              "      <td>-0.012906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.001054</td>\n",
              "      <td>0.001782</td>\n",
              "      <td>-0.044656</td>\n",
              "      <td>0.097634</td>\n",
              "      <td>0.048234</td>\n",
              "      <td>-0.050914</td>\n",
              "      <td>0.082231</td>\n",
              "      <td>-0.041166</td>\n",
              "      <td>-0.003163</td>\n",
              "      <td>0.062847</td>\n",
              "      <td>-0.014182</td>\n",
              "      <td>-0.113599</td>\n",
              "      <td>-0.078436</td>\n",
              "      <td>0.016851</td>\n",
              "      <td>-0.009305</td>\n",
              "      <td>0.150368</td>\n",
              "      <td>0.109996</td>\n",
              "      <td>0.136547</td>\n",
              "      <td>0.062544</td>\n",
              "      <td>-0.062195</td>\n",
              "      <td>0.077737</td>\n",
              "      <td>0.091286</td>\n",
              "      <td>0.108676</td>\n",
              "      <td>0.037065</td>\n",
              "      <td>0.091064</td>\n",
              "      <td>-0.012357</td>\n",
              "      <td>-0.018322</td>\n",
              "      <td>-0.042345</td>\n",
              "      <td>0.017696</td>\n",
              "      <td>0.016801</td>\n",
              "      <td>-0.080247</td>\n",
              "      <td>0.076333</td>\n",
              "      <td>0.005660</td>\n",
              "      <td>0.046167</td>\n",
              "      <td>0.002042</td>\n",
              "      <td>-0.020963</td>\n",
              "      <td>0.099282</td>\n",
              "      <td>-0.033167</td>\n",
              "      <td>0.010920</td>\n",
              "      <td>0.044533</td>\n",
              "      <td>0.136308</td>\n",
              "      <td>-0.053655</td>\n",
              "      <td>0.111411</td>\n",
              "      <td>0.032424</td>\n",
              "      <td>0.003096</td>\n",
              "      <td>-0.047646</td>\n",
              "      <td>-0.064209</td>\n",
              "      <td>-0.025413</td>\n",
              "      <td>0.044914</td>\n",
              "      <td>0.020874</td>\n",
              "      <td>-0.037770</td>\n",
              "      <td>0.050557</td>\n",
              "      <td>0.132291</td>\n",
              "      <td>0.012962</td>\n",
              "      <td>-0.030063</td>\n",
              "      <td>0.012551</td>\n",
              "      <td>0.009452</td>\n",
              "      <td>-0.055475</td>\n",
              "      <td>0.059297</td>\n",
              "      <td>-0.098962</td>\n",
              "      <td>-0.093600</td>\n",
              "      <td>0.058651</td>\n",
              "      <td>-0.100142</td>\n",
              "      <td>-0.019221</td>\n",
              "      <td>0.004052</td>\n",
              "      <td>0.002907</td>\n",
              "      <td>-0.093119</td>\n",
              "      <td>-0.041920</td>\n",
              "      <td>-0.036821</td>\n",
              "      <td>0.150257</td>\n",
              "      <td>0.087511</td>\n",
              "      <td>0.024636</td>\n",
              "      <td>0.076089</td>\n",
              "      <td>-0.032618</td>\n",
              "      <td>-0.154652</td>\n",
              "      <td>-0.200550</td>\n",
              "      <td>0.007850</td>\n",
              "      <td>-0.001337</td>\n",
              "      <td>0.086673</td>\n",
              "      <td>0.124856</td>\n",
              "      <td>-0.044778</td>\n",
              "      <td>-0.033749</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>-0.018230</td>\n",
              "      <td>-0.025102</td>\n",
              "      <td>0.023553</td>\n",
              "      <td>-0.034768</td>\n",
              "      <td>0.091353</td>\n",
              "      <td>-0.007187</td>\n",
              "      <td>0.023593</td>\n",
              "      <td>0.047476</td>\n",
              "      <td>-0.016369</td>\n",
              "      <td>-0.056674</td>\n",
              "      <td>-0.039155</td>\n",
              "      <td>-0.047608</td>\n",
              "      <td>-0.053237</td>\n",
              "      <td>0.067560</td>\n",
              "      <td>0.085790</td>\n",
              "      <td>0.016147</td>\n",
              "      <td>-0.067441</td>\n",
              "      <td>-0.155218</td>\n",
              "      <td>0.024892</td>\n",
              "      <td>0.038164</td>\n",
              "      <td>0.054416</td>\n",
              "      <td>-0.100134</td>\n",
              "      <td>0.029646</td>\n",
              "      <td>0.016868</td>\n",
              "      <td>-0.048662</td>\n",
              "      <td>0.065652</td>\n",
              "      <td>-0.068615</td>\n",
              "      <td>-0.085283</td>\n",
              "      <td>-0.027765</td>\n",
              "      <td>-0.035756</td>\n",
              "      <td>0.022450</td>\n",
              "      <td>0.147131</td>\n",
              "      <td>-0.050526</td>\n",
              "      <td>-0.010859</td>\n",
              "      <td>-0.034463</td>\n",
              "      <td>0.010476</td>\n",
              "      <td>0.017160</td>\n",
              "      <td>-0.062181</td>\n",
              "      <td>-0.104420</td>\n",
              "      <td>-0.009566</td>\n",
              "      <td>0.121113</td>\n",
              "      <td>0.024969</td>\n",
              "      <td>-0.044083</td>\n",
              "      <td>-0.045151</td>\n",
              "      <td>-0.086857</td>\n",
              "      <td>0.021973</td>\n",
              "      <td>0.126792</td>\n",
              "      <td>-0.129838</td>\n",
              "      <td>-0.059061</td>\n",
              "      <td>-0.094316</td>\n",
              "      <td>0.066262</td>\n",
              "      <td>0.027890</td>\n",
              "      <td>-0.049882</td>\n",
              "      <td>0.070534</td>\n",
              "      <td>-0.001731</td>\n",
              "      <td>-0.020641</td>\n",
              "      <td>0.165394</td>\n",
              "      <td>-0.021368</td>\n",
              "      <td>-0.098885</td>\n",
              "      <td>-0.019942</td>\n",
              "      <td>-0.116599</td>\n",
              "      <td>-0.016441</td>\n",
              "      <td>0.042259</td>\n",
              "      <td>-0.010454</td>\n",
              "      <td>0.015103</td>\n",
              "      <td>0.048406</td>\n",
              "      <td>-0.084101</td>\n",
              "      <td>0.112654</td>\n",
              "      <td>0.028453</td>\n",
              "      <td>-0.158001</td>\n",
              "      <td>0.120938</td>\n",
              "      <td>-0.092618</td>\n",
              "      <td>0.054968</td>\n",
              "      <td>0.025969</td>\n",
              "      <td>-0.108920</td>\n",
              "      <td>-0.116943</td>\n",
              "      <td>-0.086725</td>\n",
              "      <td>-0.051371</td>\n",
              "      <td>0.060797</td>\n",
              "      <td>0.005488</td>\n",
              "      <td>0.038386</td>\n",
              "      <td>0.061250</td>\n",
              "      <td>-0.090021</td>\n",
              "      <td>0.061210</td>\n",
              "      <td>-0.054016</td>\n",
              "      <td>-0.086337</td>\n",
              "      <td>-0.005388</td>\n",
              "      <td>-0.056616</td>\n",
              "      <td>-0.017226</td>\n",
              "      <td>0.003385</td>\n",
              "      <td>-0.046697</td>\n",
              "      <td>-0.043990</td>\n",
              "      <td>0.109053</td>\n",
              "      <td>0.091625</td>\n",
              "      <td>-0.089866</td>\n",
              "      <td>-0.094371</td>\n",
              "      <td>0.019520</td>\n",
              "      <td>-0.188721</td>\n",
              "      <td>-0.099576</td>\n",
              "      <td>0.086737</td>\n",
              "      <td>0.030562</td>\n",
              "      <td>-0.136009</td>\n",
              "      <td>-0.055500</td>\n",
              "      <td>-0.027821</td>\n",
              "      <td>-0.016632</td>\n",
              "      <td>0.050981</td>\n",
              "      <td>-0.043346</td>\n",
              "      <td>-0.034557</td>\n",
              "      <td>0.016025</td>\n",
              "      <td>0.020308</td>\n",
              "      <td>-0.000954</td>\n",
              "      <td>0.032720</td>\n",
              "      <td>0.007446</td>\n",
              "      <td>0.001737</td>\n",
              "      <td>-0.049458</td>\n",
              "      <td>-0.075029</td>\n",
              "      <td>-0.108776</td>\n",
              "      <td>0.014321</td>\n",
              "      <td>0.067483</td>\n",
              "      <td>-0.102644</td>\n",
              "      <td>-0.045013</td>\n",
              "      <td>0.059350</td>\n",
              "      <td>-0.079948</td>\n",
              "      <td>-0.127491</td>\n",
              "      <td>-0.062223</td>\n",
              "      <td>-0.021512</td>\n",
              "      <td>-0.022272</td>\n",
              "      <td>-0.036333</td>\n",
              "      <td>0.100087</td>\n",
              "      <td>-0.145974</td>\n",
              "      <td>0.080578</td>\n",
              "      <td>-0.044201</td>\n",
              "      <td>-0.043307</td>\n",
              "      <td>0.200195</td>\n",
              "      <td>0.051958</td>\n",
              "      <td>-0.067055</td>\n",
              "      <td>-0.066084</td>\n",
              "      <td>-0.009458</td>\n",
              "      <td>-0.026900</td>\n",
              "      <td>-0.055853</td>\n",
              "      <td>-0.072121</td>\n",
              "      <td>0.065344</td>\n",
              "      <td>0.010720</td>\n",
              "      <td>0.046392</td>\n",
              "      <td>-0.038976</td>\n",
              "      <td>-0.023837</td>\n",
              "      <td>0.045377</td>\n",
              "      <td>-0.039108</td>\n",
              "      <td>-0.038517</td>\n",
              "      <td>-0.052901</td>\n",
              "      <td>-0.063382</td>\n",
              "      <td>0.150236</td>\n",
              "      <td>0.035785</td>\n",
              "      <td>-0.074485</td>\n",
              "      <td>0.024268</td>\n",
              "      <td>0.087624</td>\n",
              "      <td>0.011253</td>\n",
              "      <td>0.043257</td>\n",
              "      <td>-0.000627</td>\n",
              "      <td>-0.057000</td>\n",
              "      <td>-0.041005</td>\n",
              "      <td>-0.043680</td>\n",
              "      <td>0.006481</td>\n",
              "      <td>-0.001105</td>\n",
              "      <td>0.036183</td>\n",
              "      <td>-0.063053</td>\n",
              "      <td>-0.164828</td>\n",
              "      <td>0.014438</td>\n",
              "      <td>0.016227</td>\n",
              "      <td>0.043427</td>\n",
              "      <td>0.136253</td>\n",
              "      <td>0.009216</td>\n",
              "      <td>-0.032669</td>\n",
              "      <td>0.009327</td>\n",
              "      <td>-0.013017</td>\n",
              "      <td>-0.080031</td>\n",
              "      <td>-0.058527</td>\n",
              "      <td>-0.009344</td>\n",
              "      <td>-0.001243</td>\n",
              "      <td>-0.122148</td>\n",
              "      <td>-0.008071</td>\n",
              "      <td>0.050593</td>\n",
              "      <td>0.087290</td>\n",
              "      <td>0.012525</td>\n",
              "      <td>0.007815</td>\n",
              "      <td>-0.061798</td>\n",
              "      <td>-0.074912</td>\n",
              "      <td>-0.085494</td>\n",
              "      <td>0.111972</td>\n",
              "      <td>0.042913</td>\n",
              "      <td>0.087025</td>\n",
              "      <td>0.076061</td>\n",
              "      <td>-0.126753</td>\n",
              "      <td>-0.084853</td>\n",
              "      <td>-0.076638</td>\n",
              "      <td>-0.056785</td>\n",
              "      <td>0.031760</td>\n",
              "      <td>0.072879</td>\n",
              "      <td>0.000888</td>\n",
              "      <td>0.066967</td>\n",
              "      <td>0.078114</td>\n",
              "      <td>0.010620</td>\n",
              "      <td>-0.008073</td>\n",
              "      <td>-0.042034</td>\n",
              "      <td>-0.009710</td>\n",
              "      <td>-0.010537</td>\n",
              "      <td>0.006836</td>\n",
              "      <td>0.003540</td>\n",
              "      <td>0.039720</td>\n",
              "      <td>-0.179488</td>\n",
              "      <td>-0.036050</td>\n",
              "      <td>-0.042403</td>\n",
              "      <td>-0.041787</td>\n",
              "      <td>0.031367</td>\n",
              "      <td>-0.023621</td>\n",
              "      <td>-0.063424</td>\n",
              "      <td>-0.004039</td>\n",
              "      <td>-0.033264</td>\n",
              "      <td>0.085437</td>\n",
              "      <td>-0.028619</td>\n",
              "      <td>0.065173</td>\n",
              "      <td>-0.053516</td>\n",
              "      <td>-0.038281</td>\n",
              "      <td>-0.106885</td>\n",
              "      <td>-0.088867</td>\n",
              "      <td>0.093604</td>\n",
              "      <td>0.062354</td>\n",
              "      <td>-0.040540</td>\n",
              "      <td>-0.104346</td>\n",
              "      <td>-0.052124</td>\n",
              "      <td>0.038077</td>\n",
              "      <td>-0.081763</td>\n",
              "      <td>-0.001077</td>\n",
              "      <td>-0.019141</td>\n",
              "      <td>0.104065</td>\n",
              "      <td>-0.029080</td>\n",
              "      <td>-0.124295</td>\n",
              "      <td>0.002368</td>\n",
              "      <td>0.095801</td>\n",
              "      <td>-0.003790</td>\n",
              "      <td>-0.008057</td>\n",
              "      <td>-0.046313</td>\n",
              "      <td>-0.045959</td>\n",
              "      <td>-0.128809</td>\n",
              "      <td>0.118750</td>\n",
              "      <td>0.118701</td>\n",
              "      <td>-0.046130</td>\n",
              "      <td>-0.036485</td>\n",
              "      <td>0.059882</td>\n",
              "      <td>-0.050143</td>\n",
              "      <td>-0.152246</td>\n",
              "      <td>0.006885</td>\n",
              "      <td>0.003784</td>\n",
              "      <td>-0.014929</td>\n",
              "      <td>-0.036832</td>\n",
              "      <td>-0.050330</td>\n",
              "      <td>0.001270</td>\n",
              "      <td>0.100003</td>\n",
              "      <td>0.002271</td>\n",
              "      <td>0.149582</td>\n",
              "      <td>0.002316</td>\n",
              "      <td>-0.112860</td>\n",
              "      <td>-0.042432</td>\n",
              "      <td>0.049930</td>\n",
              "      <td>-0.052710</td>\n",
              "      <td>0.099289</td>\n",
              "      <td>0.060638</td>\n",
              "      <td>-0.024561</td>\n",
              "      <td>0.006726</td>\n",
              "      <td>0.098896</td>\n",
              "      <td>0.022693</td>\n",
              "      <td>-0.032422</td>\n",
              "      <td>-0.089270</td>\n",
              "      <td>0.036096</td>\n",
              "      <td>-0.060977</td>\n",
              "      <td>-0.017202</td>\n",
              "      <td>-0.059467</td>\n",
              "      <td>-0.009930</td>\n",
              "      <td>-0.016235</td>\n",
              "      <td>-0.104175</td>\n",
              "      <td>-0.093433</td>\n",
              "      <td>0.033162</td>\n",
              "      <td>-0.010995</td>\n",
              "      <td>-0.167554</td>\n",
              "      <td>0.103186</td>\n",
              "      <td>0.088501</td>\n",
              "      <td>0.055981</td>\n",
              "      <td>0.029272</td>\n",
              "      <td>0.052771</td>\n",
              "      <td>0.106641</td>\n",
              "      <td>-0.058228</td>\n",
              "      <td>-0.086279</td>\n",
              "      <td>-0.101135</td>\n",
              "      <td>0.057367</td>\n",
              "      <td>0.037744</td>\n",
              "      <td>-0.004025</td>\n",
              "      <td>0.002179</td>\n",
              "      <td>0.033357</td>\n",
              "      <td>-0.038501</td>\n",
              "      <td>0.025476</td>\n",
              "      <td>-0.002698</td>\n",
              "      <td>0.011501</td>\n",
              "      <td>0.005823</td>\n",
              "      <td>-0.060974</td>\n",
              "      <td>0.082837</td>\n",
              "      <td>0.022705</td>\n",
              "      <td>0.123291</td>\n",
              "      <td>-0.133838</td>\n",
              "      <td>0.059204</td>\n",
              "      <td>-0.039185</td>\n",
              "      <td>0.015979</td>\n",
              "      <td>0.065918</td>\n",
              "      <td>-0.045572</td>\n",
              "      <td>0.010422</td>\n",
              "      <td>0.018085</td>\n",
              "      <td>0.073129</td>\n",
              "      <td>-0.027090</td>\n",
              "      <td>-0.011981</td>\n",
              "      <td>-0.053564</td>\n",
              "      <td>0.034216</td>\n",
              "      <td>0.018085</td>\n",
              "      <td>-0.069226</td>\n",
              "      <td>-0.099809</td>\n",
              "      <td>0.077966</td>\n",
              "      <td>-0.002281</td>\n",
              "      <td>0.070276</td>\n",
              "      <td>0.000894</td>\n",
              "      <td>0.077640</td>\n",
              "      <td>0.049038</td>\n",
              "      <td>-0.118469</td>\n",
              "      <td>0.065125</td>\n",
              "      <td>0.063452</td>\n",
              "      <td>-0.029886</td>\n",
              "      <td>-0.003882</td>\n",
              "      <td>-0.062134</td>\n",
              "      <td>0.018213</td>\n",
              "      <td>0.045096</td>\n",
              "      <td>-0.015625</td>\n",
              "      <td>0.030215</td>\n",
              "      <td>-0.087355</td>\n",
              "      <td>0.077466</td>\n",
              "      <td>0.061343</td>\n",
              "      <td>-0.010736</td>\n",
              "      <td>-0.087750</td>\n",
              "      <td>0.076697</td>\n",
              "      <td>0.081256</td>\n",
              "      <td>0.171240</td>\n",
              "      <td>-0.050885</td>\n",
              "      <td>-0.024017</td>\n",
              "      <td>-0.029041</td>\n",
              "      <td>0.035010</td>\n",
              "      <td>0.058936</td>\n",
              "      <td>0.016943</td>\n",
              "      <td>-0.040715</td>\n",
              "      <td>0.009045</td>\n",
              "      <td>0.016602</td>\n",
              "      <td>-0.058224</td>\n",
              "      <td>0.068182</td>\n",
              "      <td>-0.169360</td>\n",
              "      <td>0.022827</td>\n",
              "      <td>-0.022589</td>\n",
              "      <td>0.033661</td>\n",
              "      <td>0.156598</td>\n",
              "      <td>-0.039621</td>\n",
              "      <td>0.064331</td>\n",
              "      <td>-0.044409</td>\n",
              "      <td>-0.040804</td>\n",
              "      <td>0.158966</td>\n",
              "      <td>-0.116748</td>\n",
              "      <td>-0.051065</td>\n",
              "      <td>0.058643</td>\n",
              "      <td>-0.012624</td>\n",
              "      <td>-0.040546</td>\n",
              "      <td>-0.169482</td>\n",
              "      <td>-0.011092</td>\n",
              "      <td>-0.053809</td>\n",
              "      <td>-0.047314</td>\n",
              "      <td>-0.028320</td>\n",
              "      <td>0.026489</td>\n",
              "      <td>0.026306</td>\n",
              "      <td>-0.054993</td>\n",
              "      <td>-0.009918</td>\n",
              "      <td>-0.058942</td>\n",
              "      <td>0.050195</td>\n",
              "      <td>-0.069815</td>\n",
              "      <td>0.002750</td>\n",
              "      <td>-0.050122</td>\n",
              "      <td>-0.099222</td>\n",
              "      <td>-0.023523</td>\n",
              "      <td>-0.041675</td>\n",
              "      <td>0.008414</td>\n",
              "      <td>-0.020383</td>\n",
              "      <td>-0.005309</td>\n",
              "      <td>0.111230</td>\n",
              "      <td>-0.168311</td>\n",
              "      <td>-0.051233</td>\n",
              "      <td>-0.051663</td>\n",
              "      <td>0.013602</td>\n",
              "      <td>-0.118787</td>\n",
              "      <td>-0.032056</td>\n",
              "      <td>0.018774</td>\n",
              "      <td>-0.006619</td>\n",
              "      <td>-0.094467</td>\n",
              "      <td>0.095776</td>\n",
              "      <td>-0.018164</td>\n",
              "      <td>0.129248</td>\n",
              "      <td>0.021240</td>\n",
              "      <td>0.077496</td>\n",
              "      <td>-0.008240</td>\n",
              "      <td>-0.059033</td>\n",
              "      <td>0.081934</td>\n",
              "      <td>-0.011026</td>\n",
              "      <td>0.147864</td>\n",
              "      <td>-0.005255</td>\n",
              "      <td>0.020117</td>\n",
              "      <td>-0.066284</td>\n",
              "      <td>0.045508</td>\n",
              "      <td>0.026172</td>\n",
              "      <td>0.059106</td>\n",
              "      <td>-0.099658</td>\n",
              "      <td>-0.079663</td>\n",
              "      <td>0.026416</td>\n",
              "      <td>-0.098218</td>\n",
              "      <td>0.016309</td>\n",
              "      <td>-0.154184</td>\n",
              "      <td>0.012402</td>\n",
              "      <td>0.021631</td>\n",
              "      <td>-0.039868</td>\n",
              "      <td>0.141357</td>\n",
              "      <td>-0.123218</td>\n",
              "      <td>0.136963</td>\n",
              "      <td>-0.082266</td>\n",
              "      <td>0.017908</td>\n",
              "      <td>0.053600</td>\n",
              "      <td>-0.036914</td>\n",
              "      <td>-0.163770</td>\n",
              "      <td>-0.067383</td>\n",
              "      <td>-0.138672</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>-0.082422</td>\n",
              "      <td>0.083191</td>\n",
              "      <td>0.005103</td>\n",
              "      <td>-0.042999</td>\n",
              "      <td>0.059521</td>\n",
              "      <td>0.004687</td>\n",
              "      <td>0.050659</td>\n",
              "      <td>-0.035229</td>\n",
              "      <td>0.029028</td>\n",
              "      <td>-0.068005</td>\n",
              "      <td>-0.040979</td>\n",
              "      <td>-0.025415</td>\n",
              "      <td>-0.007959</td>\n",
              "      <td>-0.064954</td>\n",
              "      <td>-0.044617</td>\n",
              "      <td>-0.038359</td>\n",
              "      <td>0.076807</td>\n",
              "      <td>0.003052</td>\n",
              "      <td>0.116927</td>\n",
              "      <td>-0.067413</td>\n",
              "      <td>-0.004865</td>\n",
              "      <td>-0.072168</td>\n",
              "      <td>-0.003876</td>\n",
              "      <td>-0.025293</td>\n",
              "      <td>0.021417</td>\n",
              "      <td>0.040814</td>\n",
              "      <td>-0.048975</td>\n",
              "      <td>-0.102832</td>\n",
              "      <td>0.047266</td>\n",
              "      <td>0.080347</td>\n",
              "      <td>0.051373</td>\n",
              "      <td>0.001697</td>\n",
              "      <td>0.014594</td>\n",
              "      <td>-0.040234</td>\n",
              "      <td>0.085754</td>\n",
              "      <td>-0.050488</td>\n",
              "      <td>-0.041284</td>\n",
              "      <td>-0.001678</td>\n",
              "      <td>0.027881</td>\n",
              "      <td>0.009937</td>\n",
              "      <td>-0.009497</td>\n",
              "      <td>0.053183</td>\n",
              "      <td>0.066504</td>\n",
              "      <td>0.137244</td>\n",
              "      <td>-0.044849</td>\n",
              "      <td>0.004687</td>\n",
              "      <td>0.023503</td>\n",
              "      <td>-0.062231</td>\n",
              "      <td>-0.044312</td>\n",
              "      <td>0.129553</td>\n",
              "      <td>0.003674</td>\n",
              "      <td>0.051208</td>\n",
              "      <td>0.112500</td>\n",
              "      <td>-0.041443</td>\n",
              "      <td>-0.101257</td>\n",
              "      <td>-0.134863</td>\n",
              "      <td>-0.084351</td>\n",
              "      <td>0.068066</td>\n",
              "      <td>-0.039642</td>\n",
              "      <td>-0.040778</td>\n",
              "      <td>0.043164</td>\n",
              "      <td>-0.095618</td>\n",
              "      <td>-0.041675</td>\n",
              "      <td>-0.105194</td>\n",
              "      <td>-0.090332</td>\n",
              "      <td>0.007379</td>\n",
              "      <td>-0.006592</td>\n",
              "      <td>0.013765</td>\n",
              "      <td>0.033325</td>\n",
              "      <td>0.078423</td>\n",
              "      <td>-0.067712</td>\n",
              "      <td>-0.015063</td>\n",
              "      <td>-0.014386</td>\n",
              "      <td>-0.140498</td>\n",
              "      <td>0.078174</td>\n",
              "      <td>-0.022119</td>\n",
              "      <td>-0.026172</td>\n",
              "      <td>-0.024219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.024440</td>\n",
              "      <td>-0.019189</td>\n",
              "      <td>-0.017484</td>\n",
              "      <td>0.115548</td>\n",
              "      <td>-0.080122</td>\n",
              "      <td>0.034342</td>\n",
              "      <td>0.108956</td>\n",
              "      <td>-0.102426</td>\n",
              "      <td>0.056501</td>\n",
              "      <td>-0.008850</td>\n",
              "      <td>0.019420</td>\n",
              "      <td>-0.131958</td>\n",
              "      <td>-0.004109</td>\n",
              "      <td>-0.035932</td>\n",
              "      <td>-0.126325</td>\n",
              "      <td>0.074991</td>\n",
              "      <td>0.056996</td>\n",
              "      <td>0.084442</td>\n",
              "      <td>0.071490</td>\n",
              "      <td>-0.065927</td>\n",
              "      <td>-0.008837</td>\n",
              "      <td>0.061724</td>\n",
              "      <td>-0.032928</td>\n",
              "      <td>-0.031252</td>\n",
              "      <td>-0.006649</td>\n",
              "      <td>0.035767</td>\n",
              "      <td>-0.019636</td>\n",
              "      <td>0.040798</td>\n",
              "      <td>0.040303</td>\n",
              "      <td>-0.078404</td>\n",
              "      <td>-0.046112</td>\n",
              "      <td>0.047523</td>\n",
              "      <td>-0.149671</td>\n",
              "      <td>0.095799</td>\n",
              "      <td>0.048139</td>\n",
              "      <td>-0.054740</td>\n",
              "      <td>0.067553</td>\n",
              "      <td>-0.046378</td>\n",
              "      <td>0.034215</td>\n",
              "      <td>0.069903</td>\n",
              "      <td>0.109706</td>\n",
              "      <td>0.011852</td>\n",
              "      <td>0.059383</td>\n",
              "      <td>0.017539</td>\n",
              "      <td>-0.078974</td>\n",
              "      <td>-0.042868</td>\n",
              "      <td>0.042350</td>\n",
              "      <td>-0.000314</td>\n",
              "      <td>-0.039991</td>\n",
              "      <td>0.045389</td>\n",
              "      <td>-0.011892</td>\n",
              "      <td>0.062012</td>\n",
              "      <td>-0.078090</td>\n",
              "      <td>-0.098218</td>\n",
              "      <td>-0.065909</td>\n",
              "      <td>0.094793</td>\n",
              "      <td>0.010773</td>\n",
              "      <td>-0.072719</td>\n",
              "      <td>0.074815</td>\n",
              "      <td>-0.128605</td>\n",
              "      <td>0.090681</td>\n",
              "      <td>-0.012864</td>\n",
              "      <td>-0.062618</td>\n",
              "      <td>-0.123455</td>\n",
              "      <td>-0.032645</td>\n",
              "      <td>-0.101922</td>\n",
              "      <td>-0.142417</td>\n",
              "      <td>0.045845</td>\n",
              "      <td>-0.068571</td>\n",
              "      <td>0.103021</td>\n",
              "      <td>-0.014577</td>\n",
              "      <td>-0.007952</td>\n",
              "      <td>0.058071</td>\n",
              "      <td>0.100691</td>\n",
              "      <td>-0.170471</td>\n",
              "      <td>0.007307</td>\n",
              "      <td>-0.009129</td>\n",
              "      <td>0.103352</td>\n",
              "      <td>0.034467</td>\n",
              "      <td>0.031198</td>\n",
              "      <td>-0.080663</td>\n",
              "      <td>-0.023499</td>\n",
              "      <td>0.079476</td>\n",
              "      <td>-0.030576</td>\n",
              "      <td>0.003067</td>\n",
              "      <td>-0.023821</td>\n",
              "      <td>-0.172573</td>\n",
              "      <td>0.077101</td>\n",
              "      <td>0.011588</td>\n",
              "      <td>0.063492</td>\n",
              "      <td>0.074267</td>\n",
              "      <td>-0.038339</td>\n",
              "      <td>-0.079266</td>\n",
              "      <td>-0.059419</td>\n",
              "      <td>-0.059952</td>\n",
              "      <td>-0.051793</td>\n",
              "      <td>0.070010</td>\n",
              "      <td>0.023330</td>\n",
              "      <td>0.031503</td>\n",
              "      <td>-0.030300</td>\n",
              "      <td>0.007483</td>\n",
              "      <td>0.045401</td>\n",
              "      <td>-0.026986</td>\n",
              "      <td>-0.099906</td>\n",
              "      <td>0.045763</td>\n",
              "      <td>-0.030796</td>\n",
              "      <td>-0.004225</td>\n",
              "      <td>-0.035932</td>\n",
              "      <td>-0.023579</td>\n",
              "      <td>-0.132747</td>\n",
              "      <td>-0.046112</td>\n",
              "      <td>0.047355</td>\n",
              "      <td>-0.054749</td>\n",
              "      <td>0.012486</td>\n",
              "      <td>0.141724</td>\n",
              "      <td>0.037678</td>\n",
              "      <td>0.002816</td>\n",
              "      <td>-0.040149</td>\n",
              "      <td>0.109887</td>\n",
              "      <td>0.011034</td>\n",
              "      <td>0.089085</td>\n",
              "      <td>0.024684</td>\n",
              "      <td>-0.011967</td>\n",
              "      <td>0.115417</td>\n",
              "      <td>-0.007860</td>\n",
              "      <td>-0.014169</td>\n",
              "      <td>-0.061820</td>\n",
              "      <td>-0.011394</td>\n",
              "      <td>-0.033591</td>\n",
              "      <td>0.067644</td>\n",
              "      <td>-0.060865</td>\n",
              "      <td>-0.071586</td>\n",
              "      <td>-0.051348</td>\n",
              "      <td>0.007917</td>\n",
              "      <td>0.036743</td>\n",
              "      <td>-0.021198</td>\n",
              "      <td>-0.025108</td>\n",
              "      <td>0.004341</td>\n",
              "      <td>0.031023</td>\n",
              "      <td>0.057602</td>\n",
              "      <td>0.087646</td>\n",
              "      <td>-0.025502</td>\n",
              "      <td>-0.079930</td>\n",
              "      <td>0.031259</td>\n",
              "      <td>-0.027605</td>\n",
              "      <td>0.006768</td>\n",
              "      <td>0.062465</td>\n",
              "      <td>0.049685</td>\n",
              "      <td>-0.126223</td>\n",
              "      <td>-0.054025</td>\n",
              "      <td>0.115180</td>\n",
              "      <td>0.085100</td>\n",
              "      <td>0.014936</td>\n",
              "      <td>0.189784</td>\n",
              "      <td>-0.022626</td>\n",
              "      <td>-0.074969</td>\n",
              "      <td>-0.064679</td>\n",
              "      <td>-0.084045</td>\n",
              "      <td>-0.161970</td>\n",
              "      <td>-0.080627</td>\n",
              "      <td>0.008013</td>\n",
              "      <td>0.074486</td>\n",
              "      <td>-0.006906</td>\n",
              "      <td>0.057783</td>\n",
              "      <td>0.096531</td>\n",
              "      <td>0.040353</td>\n",
              "      <td>0.073356</td>\n",
              "      <td>-0.149391</td>\n",
              "      <td>-0.024833</td>\n",
              "      <td>-0.039019</td>\n",
              "      <td>0.001670</td>\n",
              "      <td>0.033116</td>\n",
              "      <td>0.015350</td>\n",
              "      <td>-0.045323</td>\n",
              "      <td>0.041508</td>\n",
              "      <td>-0.040880</td>\n",
              "      <td>0.100784</td>\n",
              "      <td>-0.088475</td>\n",
              "      <td>0.016064</td>\n",
              "      <td>-0.029297</td>\n",
              "      <td>-0.064985</td>\n",
              "      <td>-0.125746</td>\n",
              "      <td>-0.007875</td>\n",
              "      <td>-0.107812</td>\n",
              "      <td>-0.001500</td>\n",
              "      <td>-0.023482</td>\n",
              "      <td>-0.051274</td>\n",
              "      <td>-0.044964</td>\n",
              "      <td>0.039952</td>\n",
              "      <td>0.038707</td>\n",
              "      <td>-0.012763</td>\n",
              "      <td>-0.004634</td>\n",
              "      <td>0.050814</td>\n",
              "      <td>0.004141</td>\n",
              "      <td>-0.034511</td>\n",
              "      <td>-0.064514</td>\n",
              "      <td>0.064621</td>\n",
              "      <td>0.046927</td>\n",
              "      <td>-0.044859</td>\n",
              "      <td>-0.089714</td>\n",
              "      <td>-0.038112</td>\n",
              "      <td>0.060093</td>\n",
              "      <td>-0.065798</td>\n",
              "      <td>0.001641</td>\n",
              "      <td>-0.082546</td>\n",
              "      <td>-0.022811</td>\n",
              "      <td>-0.114556</td>\n",
              "      <td>0.004238</td>\n",
              "      <td>-0.000314</td>\n",
              "      <td>-0.019163</td>\n",
              "      <td>-0.065134</td>\n",
              "      <td>0.079867</td>\n",
              "      <td>0.002223</td>\n",
              "      <td>0.001635</td>\n",
              "      <td>-0.125506</td>\n",
              "      <td>0.053493</td>\n",
              "      <td>0.053502</td>\n",
              "      <td>0.074639</td>\n",
              "      <td>-0.036534</td>\n",
              "      <td>-0.071846</td>\n",
              "      <td>0.010553</td>\n",
              "      <td>0.004024</td>\n",
              "      <td>-0.102129</td>\n",
              "      <td>0.040251</td>\n",
              "      <td>0.057155</td>\n",
              "      <td>-0.013085</td>\n",
              "      <td>-0.066462</td>\n",
              "      <td>0.017840</td>\n",
              "      <td>0.003993</td>\n",
              "      <td>-0.068726</td>\n",
              "      <td>-0.008889</td>\n",
              "      <td>0.019064</td>\n",
              "      <td>-0.115060</td>\n",
              "      <td>0.095494</td>\n",
              "      <td>0.089475</td>\n",
              "      <td>-0.020464</td>\n",
              "      <td>0.131217</td>\n",
              "      <td>0.027825</td>\n",
              "      <td>-0.012373</td>\n",
              "      <td>-0.047389</td>\n",
              "      <td>0.017735</td>\n",
              "      <td>-0.120793</td>\n",
              "      <td>0.057539</td>\n",
              "      <td>0.007649</td>\n",
              "      <td>-0.095213</td>\n",
              "      <td>0.007350</td>\n",
              "      <td>-0.019235</td>\n",
              "      <td>0.078650</td>\n",
              "      <td>0.017717</td>\n",
              "      <td>-0.126347</td>\n",
              "      <td>0.028739</td>\n",
              "      <td>0.049500</td>\n",
              "      <td>0.092585</td>\n",
              "      <td>0.047727</td>\n",
              "      <td>0.023660</td>\n",
              "      <td>-0.024222</td>\n",
              "      <td>-0.046077</td>\n",
              "      <td>0.028678</td>\n",
              "      <td>-0.098001</td>\n",
              "      <td>0.088161</td>\n",
              "      <td>0.001770</td>\n",
              "      <td>0.007455</td>\n",
              "      <td>-0.078526</td>\n",
              "      <td>-0.034435</td>\n",
              "      <td>-0.001112</td>\n",
              "      <td>0.133676</td>\n",
              "      <td>0.042603</td>\n",
              "      <td>-0.041260</td>\n",
              "      <td>-0.071237</td>\n",
              "      <td>-0.069057</td>\n",
              "      <td>0.077026</td>\n",
              "      <td>0.066044</td>\n",
              "      <td>0.042341</td>\n",
              "      <td>0.066668</td>\n",
              "      <td>0.139125</td>\n",
              "      <td>-0.000296</td>\n",
              "      <td>-0.076050</td>\n",
              "      <td>-0.061872</td>\n",
              "      <td>-0.095869</td>\n",
              "      <td>0.049816</td>\n",
              "      <td>0.075640</td>\n",
              "      <td>0.036830</td>\n",
              "      <td>0.089923</td>\n",
              "      <td>0.048976</td>\n",
              "      <td>0.042324</td>\n",
              "      <td>-0.022932</td>\n",
              "      <td>-0.080719</td>\n",
              "      <td>0.088243</td>\n",
              "      <td>0.112810</td>\n",
              "      <td>0.063232</td>\n",
              "      <td>-0.151803</td>\n",
              "      <td>-0.008364</td>\n",
              "      <td>-0.070173</td>\n",
              "      <td>-0.054688</td>\n",
              "      <td>-0.014561</td>\n",
              "      <td>-0.038535</td>\n",
              "      <td>0.032900</td>\n",
              "      <td>-0.009990</td>\n",
              "      <td>-0.029566</td>\n",
              "      <td>-0.058221</td>\n",
              "      <td>0.020142</td>\n",
              "      <td>-0.023315</td>\n",
              "      <td>-0.018112</td>\n",
              "      <td>0.170898</td>\n",
              "      <td>-0.043823</td>\n",
              "      <td>0.111816</td>\n",
              "      <td>0.061768</td>\n",
              "      <td>-0.097961</td>\n",
              "      <td>-0.069885</td>\n",
              "      <td>0.069214</td>\n",
              "      <td>-0.038818</td>\n",
              "      <td>-0.198730</td>\n",
              "      <td>-0.166992</td>\n",
              "      <td>-0.044495</td>\n",
              "      <td>-0.044556</td>\n",
              "      <td>-0.010925</td>\n",
              "      <td>0.099228</td>\n",
              "      <td>0.183594</td>\n",
              "      <td>0.048340</td>\n",
              "      <td>-0.018127</td>\n",
              "      <td>0.037231</td>\n",
              "      <td>0.106995</td>\n",
              "      <td>-0.042603</td>\n",
              "      <td>0.107605</td>\n",
              "      <td>-0.010254</td>\n",
              "      <td>-0.001953</td>\n",
              "      <td>-0.103027</td>\n",
              "      <td>0.075500</td>\n",
              "      <td>0.021439</td>\n",
              "      <td>-0.293945</td>\n",
              "      <td>-0.117188</td>\n",
              "      <td>0.165283</td>\n",
              "      <td>-0.291992</td>\n",
              "      <td>0.013184</td>\n",
              "      <td>0.040649</td>\n",
              "      <td>-0.080200</td>\n",
              "      <td>0.046021</td>\n",
              "      <td>-0.172363</td>\n",
              "      <td>-0.062500</td>\n",
              "      <td>0.078125</td>\n",
              "      <td>0.063354</td>\n",
              "      <td>0.150391</td>\n",
              "      <td>-0.054863</td>\n",
              "      <td>0.097900</td>\n",
              "      <td>0.015577</td>\n",
              "      <td>-0.058136</td>\n",
              "      <td>0.042847</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>-0.114075</td>\n",
              "      <td>-0.046387</td>\n",
              "      <td>-0.024063</td>\n",
              "      <td>0.112823</td>\n",
              "      <td>-0.002441</td>\n",
              "      <td>-0.162109</td>\n",
              "      <td>-0.026611</td>\n",
              "      <td>-0.104004</td>\n",
              "      <td>-0.076782</td>\n",
              "      <td>-0.095459</td>\n",
              "      <td>0.079102</td>\n",
              "      <td>-0.143799</td>\n",
              "      <td>0.159668</td>\n",
              "      <td>-0.074707</td>\n",
              "      <td>-0.109375</td>\n",
              "      <td>-0.111328</td>\n",
              "      <td>-0.073914</td>\n",
              "      <td>-0.115723</td>\n",
              "      <td>-0.083740</td>\n",
              "      <td>0.087158</td>\n",
              "      <td>-0.043213</td>\n",
              "      <td>0.164062</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.012589</td>\n",
              "      <td>0.099854</td>\n",
              "      <td>0.062378</td>\n",
              "      <td>-0.020996</td>\n",
              "      <td>0.054443</td>\n",
              "      <td>-0.071289</td>\n",
              "      <td>-0.110107</td>\n",
              "      <td>-0.084473</td>\n",
              "      <td>-0.099365</td>\n",
              "      <td>-0.145020</td>\n",
              "      <td>-0.063660</td>\n",
              "      <td>0.044922</td>\n",
              "      <td>-0.036560</td>\n",
              "      <td>-0.040405</td>\n",
              "      <td>0.163330</td>\n",
              "      <td>-0.159180</td>\n",
              "      <td>0.124023</td>\n",
              "      <td>0.036621</td>\n",
              "      <td>0.011902</td>\n",
              "      <td>0.114861</td>\n",
              "      <td>-0.062744</td>\n",
              "      <td>-0.095947</td>\n",
              "      <td>-0.102409</td>\n",
              "      <td>-0.173828</td>\n",
              "      <td>0.145508</td>\n",
              "      <td>0.128906</td>\n",
              "      <td>0.028809</td>\n",
              "      <td>0.097656</td>\n",
              "      <td>-0.014893</td>\n",
              "      <td>-0.064327</td>\n",
              "      <td>0.128662</td>\n",
              "      <td>-0.043701</td>\n",
              "      <td>-0.081055</td>\n",
              "      <td>-0.163086</td>\n",
              "      <td>-0.093872</td>\n",
              "      <td>0.079346</td>\n",
              "      <td>-0.146484</td>\n",
              "      <td>-0.009277</td>\n",
              "      <td>-0.171875</td>\n",
              "      <td>-0.063354</td>\n",
              "      <td>0.150452</td>\n",
              "      <td>-0.002014</td>\n",
              "      <td>0.073608</td>\n",
              "      <td>0.039551</td>\n",
              "      <td>0.136047</td>\n",
              "      <td>0.005371</td>\n",
              "      <td>-0.077393</td>\n",
              "      <td>0.069702</td>\n",
              "      <td>-0.057861</td>\n",
              "      <td>0.102417</td>\n",
              "      <td>-0.024414</td>\n",
              "      <td>0.004883</td>\n",
              "      <td>0.147949</td>\n",
              "      <td>0.036926</td>\n",
              "      <td>0.034515</td>\n",
              "      <td>-0.160645</td>\n",
              "      <td>-0.042542</td>\n",
              "      <td>-0.030029</td>\n",
              "      <td>0.050842</td>\n",
              "      <td>-0.023865</td>\n",
              "      <td>-0.055786</td>\n",
              "      <td>0.014465</td>\n",
              "      <td>0.172852</td>\n",
              "      <td>-0.030029</td>\n",
              "      <td>-0.024834</td>\n",
              "      <td>0.042267</td>\n",
              "      <td>-0.071960</td>\n",
              "      <td>0.045410</td>\n",
              "      <td>0.116699</td>\n",
              "      <td>0.080688</td>\n",
              "      <td>-0.013000</td>\n",
              "      <td>0.002930</td>\n",
              "      <td>0.097229</td>\n",
              "      <td>-0.057220</td>\n",
              "      <td>0.001183</td>\n",
              "      <td>0.213379</td>\n",
              "      <td>-0.051025</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.074097</td>\n",
              "      <td>0.170166</td>\n",
              "      <td>0.270508</td>\n",
              "      <td>0.095215</td>\n",
              "      <td>0.156250</td>\n",
              "      <td>-0.141113</td>\n",
              "      <td>-0.086914</td>\n",
              "      <td>-0.057343</td>\n",
              "      <td>-0.138672</td>\n",
              "      <td>-0.213379</td>\n",
              "      <td>-0.110718</td>\n",
              "      <td>-0.095947</td>\n",
              "      <td>0.111931</td>\n",
              "      <td>-0.145020</td>\n",
              "      <td>0.164551</td>\n",
              "      <td>0.125732</td>\n",
              "      <td>0.085510</td>\n",
              "      <td>0.158203</td>\n",
              "      <td>-0.153698</td>\n",
              "      <td>-0.055450</td>\n",
              "      <td>-0.113770</td>\n",
              "      <td>-0.181152</td>\n",
              "      <td>-0.057129</td>\n",
              "      <td>0.109619</td>\n",
              "      <td>0.022339</td>\n",
              "      <td>0.107910</td>\n",
              "      <td>0.018066</td>\n",
              "      <td>-0.043991</td>\n",
              "      <td>0.035156</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.053833</td>\n",
              "      <td>-0.011658</td>\n",
              "      <td>-0.073486</td>\n",
              "      <td>-0.044067</td>\n",
              "      <td>-0.149902</td>\n",
              "      <td>0.105225</td>\n",
              "      <td>0.080811</td>\n",
              "      <td>0.019453</td>\n",
              "      <td>-0.033661</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.093994</td>\n",
              "      <td>0.016632</td>\n",
              "      <td>0.080444</td>\n",
              "      <td>0.095947</td>\n",
              "      <td>-0.082764</td>\n",
              "      <td>-0.134521</td>\n",
              "      <td>-0.146240</td>\n",
              "      <td>0.080627</td>\n",
              "      <td>0.054321</td>\n",
              "      <td>-0.035339</td>\n",
              "      <td>0.030151</td>\n",
              "      <td>-0.088013</td>\n",
              "      <td>0.047974</td>\n",
              "      <td>-0.082550</td>\n",
              "      <td>-0.039017</td>\n",
              "      <td>-0.114624</td>\n",
              "      <td>-0.129761</td>\n",
              "      <td>-0.079712</td>\n",
              "      <td>0.056763</td>\n",
              "      <td>0.007324</td>\n",
              "      <td>-0.096161</td>\n",
              "      <td>-0.220215</td>\n",
              "      <td>0.095779</td>\n",
              "      <td>-0.027405</td>\n",
              "      <td>-0.070923</td>\n",
              "      <td>-0.092651</td>\n",
              "      <td>0.024414</td>\n",
              "      <td>-0.001221</td>\n",
              "      <td>0.164307</td>\n",
              "      <td>-0.140137</td>\n",
              "      <td>-0.175537</td>\n",
              "      <td>-0.060989</td>\n",
              "      <td>-0.053406</td>\n",
              "      <td>-0.094727</td>\n",
              "      <td>-0.038574</td>\n",
              "      <td>0.072205</td>\n",
              "      <td>-0.035034</td>\n",
              "      <td>-0.010254</td>\n",
              "      <td>0.182495</td>\n",
              "      <td>-0.174316</td>\n",
              "      <td>-0.098022</td>\n",
              "      <td>0.059021</td>\n",
              "      <td>-0.150078</td>\n",
              "      <td>-0.042969</td>\n",
              "      <td>0.100098</td>\n",
              "      <td>0.290527</td>\n",
              "      <td>-0.060059</td>\n",
              "      <td>0.078125</td>\n",
              "      <td>0.027832</td>\n",
              "      <td>0.082520</td>\n",
              "      <td>0.032104</td>\n",
              "      <td>0.170410</td>\n",
              "      <td>-0.290039</td>\n",
              "      <td>0.037109</td>\n",
              "      <td>-0.034912</td>\n",
              "      <td>-0.016357</td>\n",
              "      <td>0.014282</td>\n",
              "      <td>0.015381</td>\n",
              "      <td>0.083374</td>\n",
              "      <td>-0.090042</td>\n",
              "      <td>-0.278809</td>\n",
              "      <td>0.020599</td>\n",
              "      <td>0.031494</td>\n",
              "      <td>-0.003906</td>\n",
              "      <td>0.082520</td>\n",
              "      <td>0.089966</td>\n",
              "      <td>-0.026062</td>\n",
              "      <td>-0.048218</td>\n",
              "      <td>0.049713</td>\n",
              "      <td>-0.064209</td>\n",
              "      <td>0.149414</td>\n",
              "      <td>-0.089371</td>\n",
              "      <td>0.040771</td>\n",
              "      <td>0.086914</td>\n",
              "      <td>0.093750</td>\n",
              "      <td>0.142578</td>\n",
              "      <td>0.348633</td>\n",
              "      <td>0.104248</td>\n",
              "      <td>-0.182373</td>\n",
              "      <td>-0.118774</td>\n",
              "      <td>-0.038208</td>\n",
              "      <td>-0.090698</td>\n",
              "      <td>0.082123</td>\n",
              "      <td>-0.055328</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.247070</td>\n",
              "      <td>-0.165283</td>\n",
              "      <td>-0.127075</td>\n",
              "      <td>0.090698</td>\n",
              "      <td>-0.168457</td>\n",
              "      <td>0.077637</td>\n",
              "      <td>0.144043</td>\n",
              "      <td>0.117676</td>\n",
              "      <td>0.150391</td>\n",
              "      <td>0.022766</td>\n",
              "      <td>-0.079834</td>\n",
              "      <td>-0.053833</td>\n",
              "      <td>0.167969</td>\n",
              "      <td>-0.052795</td>\n",
              "      <td>0.128540</td>\n",
              "      <td>0.190918</td>\n",
              "      <td>-0.293457</td>\n",
              "      <td>0.030273</td>\n",
              "      <td>-0.139038</td>\n",
              "      <td>0.134399</td>\n",
              "      <td>0.050659</td>\n",
              "      <td>-0.037354</td>\n",
              "      <td>0.042969</td>\n",
              "      <td>-0.079590</td>\n",
              "      <td>0.059326</td>\n",
              "      <td>-0.248047</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "algos = {\n",
        "  'RF' : RandomForestRegressor(n_estimators=50,random_state=1,n_jobs=-1),\n",
        "  'KNN' : KNeighborsRegressor(n_neighbors=5,n_jobs=-1,metric='cosine'),\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(20,10),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models_with_binary_tresholding_score(rev_a_title_train_wv_google, y_train, rev_a_title_test_wv_google, y_test.to_numpy(), algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "id": "_cR7WCwdbZik",
        "outputId": "c7bdff35-1065-4264-fcca-377c3cf073e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## RF #############\n",
            "MAE = 2.712, Accuracy = 0.104, Customized Accuracy = 0.511, Binary Thresholding Score = 0.610\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,    0,    4,   68,  558, 1213,  942,  306,   21,    0,    0],\n",
              "       [   0,    0,    1,    3,   47,  151,  114,   45,    6,    0,    0],\n",
              "       [   0,    0,    2,    6,  103,  269,  288,  120,   15,    0,    0],\n",
              "       [   0,    0,    0,    6,   94,  345,  378,  185,   19,    0,    0],\n",
              "       [   0,    0,    0,    5,   46,  157,  232,  105,   14,    0,    0],\n",
              "       [   0,    0,    1,   30,  300,  944, 1085,  531,   69,    0,    0],\n",
              "       [   0,    0,    0,    6,   43,  225,  395,  237,   33,    0,    0],\n",
              "       [   0,    0,    0,   10,  165,  705,  935,  596,   86,    0,    0],\n",
              "       [   0,    0,    0,    4,  158,  638, 1186,  906,  140,    0,    0],\n",
              "       [   0,    0,    0,    2,   63,  357,  837,  796,  195,    1,    0],\n",
              "       [   0,    0,    0,   13,  258,  896, 1068,  643,  123,    0,    0]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## KNN #############\n",
            "MAE = 2.897, Accuracy = 0.103, Customized Accuracy = 0.500, Binary Thresholding Score = 0.558\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  0,  33,  58, 154, 362, 565, 747, 646, 410, 132,   5],\n",
              "       [  0,   1,   3,   7,  34,  69,  89,  86,  65,  11,   2],\n",
              "       [  0,   2,   9,  31,  67, 143, 207, 219,  89,  33,   3],\n",
              "       [  0,   3,   7,  38,  85, 167, 272, 248, 166,  39,   2],\n",
              "       [  0,   2,   3,  15,  45,  92, 152, 152,  76,  22,   0],\n",
              "       [  0,   5,  25, 110, 269, 507, 740, 730, 453, 119,   2],\n",
              "       [  0,   1,  12,  24,  85, 149, 233, 228, 162,  44,   1],\n",
              "       [  0,   7,  21,  60, 187, 384, 673, 659, 396, 107,   3],\n",
              "       [  0,   4,  24,  83, 212, 495, 723, 849, 486, 153,   3],\n",
              "       [  0,   3,  16,  59, 140, 329, 541, 601, 427, 133,   2],\n",
              "       [  0,   6,  39,  97, 244, 481, 723, 764, 491, 149,   7]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## MLP #############\n",
            "MAE = 2.927, Accuracy = 0.097, Customized Accuracy = 0.503, Binary Thresholding Score = 0.573\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  0, 237, 171, 291, 386, 487, 554, 497, 308, 123,  58],\n",
              "       [  0,  19,  12,  27,  32,  81,  84,  77,  21,  10,   4],\n",
              "       [  0,  36,  28,  51, 103, 159, 167, 163,  68,  20,   8],\n",
              "       [  0,  31,  29,  55, 123, 186, 248, 200, 123,  27,   5],\n",
              "       [  0,  13,  16,  25,  68, 104, 144, 116,  47,  22,   4],\n",
              "       [  0, 105, 116, 206, 309, 468, 628, 642, 353,  93,  40],\n",
              "       [  0,  18,  18,  57,  73, 146, 221, 246, 113,  45,   2],\n",
              "       [  0,  60,  71, 137, 236, 408, 586, 559, 309,  96,  35],\n",
              "       [  0,  74,  68, 128, 266, 412, 726, 784, 422, 122,  30],\n",
              "       [  0,  34,  43,  85, 149, 295, 477, 615, 425, 107,  21],\n",
              "       [  0, 105,  97, 186, 318, 431, 581, 682, 423, 124,  54]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les performances sont inférieurs par rapport à son équivalent dans la partie précédente."
      ],
      "metadata": {
        "id": "BCcsjd8nbc5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Courbe d'apprentissage du meilleur modèle"
      ],
      "metadata": {
        "id": "ndf7k4gNbj0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# utiliser la fonction learning_curve\n",
        "train_sizes, train_scores, test_scores = learning_curve(\\\n",
        "  algos[\"MLP\"], pd.concat([rev_a_title_train_wv_google, rev_a_title_test_wv_google]),\\\n",
        "  pd.concat([y_train, y_test]), verbose=1)\n",
        "\n",
        "# tracer les courbes d'apprentissage\n",
        "plt.figure()\n",
        "plt.title(\"Courbe d'apprentissage\")\n",
        "plt.xlabel(\"Taille de l'échantillon d'entraînement\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.plot(train_sizes, train_scores.mean(axis=1), 'o-', color=\"r\", label=\"Score d'entraînement\")\n",
        "plt.plot(train_sizes, test_scores.mean(axis=1), 'o-', color=\"g\", label=\"Score de validation croisée\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "z0ilnbV1bksz",
        "outputId": "e749868e-b6dc-4e37-f42e-4b1a25df0484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[learning_curve] Training set sizes: [ 8218 26711 45203 63696 82189]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed: 113.0min finished\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Amélioration du meilleur modèle"
      ],
      "metadata": {
        "id": "2Kid5GVxbrgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Moins de neurones\n",
        "algos = {\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(10, 5),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models_with_binary_tresholding_score(rev_a_title_train_wv_google, y_train, rev_a_title_test_wv_google, y_test.to_numpy(), algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "RLJIjb98b0wM",
        "outputId": "1d94edd8-54ca-41d3-d7a5-582187e28511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## MLP #############\n",
            "MAE = 2.775, Accuracy = 0.102, Customized Accuracy = 0.519, Binary Thresholding Score = 0.592\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,   96,  111,  191,  380,  684,  860,  599,  174,   15,    2],\n",
              "       [   0,   10,    8,   12,   43,   80,  109,   71,   30,    4,    0],\n",
              "       [   0,   16,   17,   30,   79,  155,  267,  178,   52,    7,    2],\n",
              "       [   0,   11,   21,   37,   81,  195,  345,  265,   65,    6,    1],\n",
              "       [   0,    7,   13,   14,   43,  100,  188,  150,   37,    7,    0],\n",
              "       [   0,   48,   60,  101,  272,  501,  948,  800,  205,   23,    2],\n",
              "       [   0,    8,   12,   27,   51,  154,  307,  284,   84,   10,    2],\n",
              "       [   0,   16,   23,   67,  188,  390,  792,  770,  225,   21,    5],\n",
              "       [   0,   22,   30,   59,  175,  405,  924, 1008,  370,   37,    2],\n",
              "       [   0,   11,    9,   26,   90,  277,  622,  833,  345,   37,    1],\n",
              "       [   0,   36,   66,  118,  220,  488,  861,  878,  297,   37,    0]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Petite amélioration"
      ],
      "metadata": {
        "id": "-NgQMnGDcGcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plus de neurones\n",
        "algos = {\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(40, 20),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models_with_binary_tresholding_score(rev_a_title_train_wv_google, y_train, rev_a_title_test_wv_google, y_test.to_numpy(), algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "QBCBV2glcAt2",
        "outputId": "0fe8b479-4017-4abe-f9e4-aef79403309a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## MLP #############\n",
            "MAE = 3.068, Accuracy = 0.110, Customized Accuracy = 0.506, Binary Thresholding Score = 0.571\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  0, 418, 147, 187, 253, 322, 394, 504, 435, 257, 195],\n",
              "       [  0,  27,  22,  20,  35,  52,  57,  75,  50,  19,  10],\n",
              "       [  0,  68,  28,  50,  75, 114, 140, 141, 110,  47,  30],\n",
              "       [  0,  84,  41,  53,  93, 126, 160, 195, 156,  77,  42],\n",
              "       [  0,  28,  20,  31,  45,  71, 101, 114,  90,  36,  23],\n",
              "       [  0, 245, 113, 172, 191, 309, 462, 571, 487, 266, 144],\n",
              "       [  0,  52,  24,  52,  63,  88, 159, 208, 175,  87,  31],\n",
              "       [  0, 148,  69, 123, 161, 271, 405, 544, 420, 230, 126],\n",
              "       [  0, 152,  86, 118, 199, 285, 459, 638, 615, 331, 149],\n",
              "       [  0,  85,  42,  66, 117, 190, 343, 523, 481, 281, 123],\n",
              "       [  0, 264, 108, 137, 222, 295, 407, 558, 500, 318, 192]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Petite amélioration aussi"
      ],
      "metadata": {
        "id": "bVuoPzzkcJpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### II. Prise en compte de la longueur, de la spécificité et de la subjectivité du texte"
      ],
      "metadata": {
        "id": "rqkGmEul31Tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul de la longueur, la spécificité et la subjectivité\n",
        "df_meta_train=pd.DataFrame([])\n",
        "df_meta_train['length_text'] = [len(reviews_train_tokens.iloc[i]) for i in range(len(reviews_train_tokens))]\n",
        "df_meta_train['polarity'] = [polarity_txt(X_train[\"reviewText\"].iloc[i]) for i in range(len(X_train))]\n",
        "df_meta_train['subjectivity'] = [subj_txt(X_train[\"reviewText\"].iloc[i]) for i in range(len(X_train))]\n",
        "\n",
        "df_meta_test=pd.DataFrame([])\n",
        "df_meta_test['length_text'] = [len(reviews_test_tokens.iloc[i]) for i in range(len(reviews_test_tokens))]\n",
        "df_meta_test['polarity'] = [polarity_txt(X_test[\"reviewText\"].iloc[i]) for i in range(len(X_test))]\n",
        "df_meta_test['subjectivity'] = [subj_txt(X_test[\"reviewText\"].iloc[i]) for i in range(len(X_test))]"
      ],
      "metadata": {
        "id": "eBENWQe6dW85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concaténation\n",
        "corpus_train_wv_google_meta=pd.concat([reviews_train_wv_google,title_train_wv_google,df_meta_train],axis=1)\n",
        "corpus_test_wv_google_meta=pd.concat([reviews_test_wv_google,title_test_wv_google,df_meta_test],axis=1)"
      ],
      "metadata": {
        "id": "gdtSIz6BdY7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entraînement et évaluation\n",
        "algos = {\n",
        "  'RF' : RandomForestRegressor(n_estimators=50,random_state=1,n_jobs=-1),\n",
        "  'KNN' : KNeighborsRegressor(n_neighbors=5,n_jobs=-1,metric='cosine'),\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(20,10),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models_with_binary_tresholding_score(corpus_train_wv_google_meta,y_train,corpus_test_wv_google_meta,y_test.to_numpy(),algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "id": "WlOezIGGdavl",
        "outputId": "c12aa53c-9bce-4af4-c983-7cd6cb4c638b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## RF #############\n",
            "MAE = 2.686, Accuracy = 0.100, Customized Accuracy = 0.514, Binary Thresholding Score = 0.623\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,   20,    9,  110,  698, 1050,  840,  340,   45,    0,    0],\n",
              "       [   0,    3,    1,    5,   59,  133,  128,   34,    4,    0,    0],\n",
              "       [   0,    1,    1,    9,  111,  264,  282,  128,    7,    0,    0],\n",
              "       [   0,    0,    1,   14,  117,  331,  367,  176,   21,    0,    0],\n",
              "       [   0,    0,    1,    5,   53,  167,  198,  126,    9,    0,    0],\n",
              "       [   0,    3,    1,   44,  388,  839, 1049,  542,   94,    0,    0],\n",
              "       [   0,    0,    0,    8,   63,  201,  367,  263,   37,    0,    0],\n",
              "       [   0,    0,    2,   14,  218,  590,  976,  599,   98,    0,    0],\n",
              "       [   0,    0,    0,   23,  191,  572, 1094,  968,  181,    3,    0],\n",
              "       [   0,    0,    0,    5,   59,  283,  829,  834,  240,    1,    0],\n",
              "       [   0,    1,    0,   36,  328,  753, 1043,  707,  133,    0,    0]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## KNN #############\n",
            "MAE = 2.805, Accuracy = 0.112, Customized Accuracy = 0.520, Binary Thresholding Score = 0.587\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  0,  68, 131, 236, 417, 586, 640, 591, 326, 112,   5],\n",
              "       [  0,   7,   7,  23,  37,  58,  86,  84,  50,  14,   1],\n",
              "       [  0,   2,  15,  38, 102, 163, 171, 186, 102,  23,   1],\n",
              "       [  0,   4,  14,  37, 104, 187, 241, 227, 169,  44,   0],\n",
              "       [  0,   0,   8,  15,  50, 103, 137, 147,  73,  24,   2],\n",
              "       [  0,  22,  54, 129, 264, 524, 694, 660, 479, 127,   7],\n",
              "       [  0,   4,  15,  26,  77, 151, 217, 231, 171,  45,   2],\n",
              "       [  0,   6,  41,  74, 188, 384, 575, 635, 483, 109,   2],\n",
              "       [  0,  12,  25,  87, 201, 409, 687, 803, 626, 178,   4],\n",
              "       [  0,   3,  10,  35, 115, 251, 493, 634, 531, 173,   6],\n",
              "       [  0,  22,  53, 128, 277, 476, 641, 711, 526, 158,   9]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## MLP #############\n",
            "MAE = 2.808, Accuracy = 0.102, Customized Accuracy = 0.525, Binary Thresholding Score = 0.605\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  0, 320, 201, 246, 359, 580, 607, 477, 200,  73,  49],\n",
              "       [  0,  19,  13,  31,  48,  81,  86,  64,  13,   7,   5],\n",
              "       [  0,  33,  36,  61,  94, 164, 196, 161,  41,   9,   8],\n",
              "       [  0,  56,  32,  67, 115, 187, 248, 223,  73,  14,  12],\n",
              "       [  0,  17,  14,  21,  55, 124, 151, 134,  32,   7,   4],\n",
              "       [  0, 141, 127, 174, 314, 541, 736, 644, 208,  42,  33],\n",
              "       [  0,  29,  15,  39,  75, 156, 272, 270,  70,  11,   2],\n",
              "       [  0,  71,  59, 113, 215, 403, 674, 685, 214,  40,  23],\n",
              "       [  0,  62,  52,  97, 219, 428, 788, 995, 322,  42,  27],\n",
              "       [  0,  30,  23,  41, 113, 252, 529, 850, 344,  52,  17],\n",
              "       [  0, 142,  84, 127, 282, 427, 708, 778, 338,  64,  51]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Petite amélioration du random forest et du KNN par rapport à leurs équivalent dans la partie précédente."
      ],
      "metadata": {
        "id": "eKy8q4dsszDF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### III. Prise en compte de l'ordre des mots (LSTM)"
      ],
      "metadata": {
        "id": "sIwyP7dnccrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concaténation des reviews et des titres\n",
        "title_reviews_train_tokens = pd.Series([title_train_tokens[i] + reviews_train_tokens[i] for i in reviews_train_tokens.index])\n",
        "title_reviews_test_tokens = pd.Series([title_test_tokens[i] + reviews_test_tokens[i] for i in reviews_test_tokens.index])"
      ],
      "metadata": {
        "id": "oVDG0KVveEZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_sequences = [to_sequence(word2idx, x) for x in title_reviews_train_tokens]\n",
        "X_test_sequences = [to_sequence(word2idx, x) for x in  title_reviews_test_tokens]"
      ],
      "metadata": {
        "id": "P8Hy_yzYeG1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGHT=75\n",
        "N_FEATURES = len(model_google_vec_neg_300.index_to_key)\n",
        "X_train_sequences = pad_sequences(X_train_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)\n",
        "X_test_sequences = pad_sequences(X_test_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)"
      ],
      "metadata": {
        "id": "GrKjeoDYeJ1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDINGS_LEN = model_google_vec_neg_300.vector_size\n",
        "embeddings_index = np.zeros((len(model_google_vec_neg_300.index_to_key)+1, EMBEDDINGS_LEN))\n",
        "embeddings_index.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNr3LlQyeMFq",
        "outputId": "bd410113-53ae-4159-ffd0-acf5ee8ed41a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000001, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word, idx in word2idx.items():\n",
        "    try:\n",
        "        embedding = model_google_vec_neg_300[word]\n",
        "        embeddings_index[idx] = embedding\n",
        "    except:\n",
        "        pass\n",
        "      \n",
        "print(\"EMBEDDINGS_LEN=\", EMBEDDINGS_LEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnXDnw-AeO0G",
        "outputId": "b4697d9a-c38e-4b3e-d9ad-988ea545093c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EMBEDDINGS_LEN= 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(len(model_google_vec_neg_300.key_to_index)+1,\n",
        "                    EMBEDDINGS_LEN,  # Embedding size\n",
        "                    weights=[embeddings_index],\n",
        "                    trainable=False))\n",
        "\n",
        "model_lstm.add(LSTM(300, dropout=0.2))\n",
        "\n",
        "model_lstm.add(Dense(1))\n",
        "model_lstm.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "model_lstm.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zCvRvEAeS-T",
        "outputId": "f7d4384a-110b-4f3f-ae44-52e88d3fd333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, None, 300)         900000300 \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 300)               721200    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 301       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 900,721,801\n",
            "Trainable params: 721,501\n",
            "Non-trainable params: 900,000,300\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_lstm.fit(X_train_sequences, y_train, epochs=10, batch_size=128, verbose=1,validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtmL6OF4eWC6",
        "outputId": "ed153e78-1200-4637-fe61-30f9a52eb9eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "578/578 [==============================] - 125s 210ms/step - loss: 2.7818 - mean_absolute_error: 2.7818 - val_loss: 2.7625 - val_mean_absolute_error: 2.7625\n",
            "Epoch 2/10\n",
            "578/578 [==============================] - 117s 203ms/step - loss: 2.6755 - mean_absolute_error: 2.6755 - val_loss: 2.6016 - val_mean_absolute_error: 2.6016\n",
            "Epoch 3/10\n",
            "578/578 [==============================] - 115s 199ms/step - loss: 2.6006 - mean_absolute_error: 2.6006 - val_loss: 2.5851 - val_mean_absolute_error: 2.5851\n",
            "Epoch 4/10\n",
            "578/578 [==============================] - 116s 200ms/step - loss: 2.5862 - mean_absolute_error: 2.5862 - val_loss: 2.5968 - val_mean_absolute_error: 2.5968\n",
            "Epoch 5/10\n",
            "578/578 [==============================] - 116s 200ms/step - loss: 2.5769 - mean_absolute_error: 2.5769 - val_loss: 2.5898 - val_mean_absolute_error: 2.5898\n",
            "Epoch 6/10\n",
            "578/578 [==============================] - 113s 196ms/step - loss: 2.5700 - mean_absolute_error: 2.5700 - val_loss: 2.5715 - val_mean_absolute_error: 2.5715\n",
            "Epoch 7/10\n",
            "578/578 [==============================] - 113s 196ms/step - loss: 2.5640 - mean_absolute_error: 2.5640 - val_loss: 2.5806 - val_mean_absolute_error: 2.5806\n",
            "Epoch 8/10\n",
            "578/578 [==============================] - 114s 197ms/step - loss: 2.5527 - mean_absolute_error: 2.5527 - val_loss: 2.5754 - val_mean_absolute_error: 2.5754\n",
            "Epoch 9/10\n",
            "578/578 [==============================] - 114s 197ms/step - loss: 2.5409 - mean_absolute_error: 2.5409 - val_loss: 2.5723 - val_mean_absolute_error: 2.5723\n",
            "Epoch 10/10\n",
            "578/578 [==============================] - 116s 200ms/step - loss: 2.5309 - mean_absolute_error: 2.5309 - val_loss: 2.5748 - val_mean_absolute_error: 2.5748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores_lstm = model_lstm.evaluate(X_test_sequences, y_test, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StaRt-SseY6L",
        "outputId": "4fd0a67f-d449-423a-ebf4-774279794b69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "643/643 [==============================] - 23s 36ms/step - loss: 2.6201 - mean_absolute_error: 2.6201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction=model_lstm.predict(X_test_sequences)\n",
        "prediction[prediction<1]=1\n",
        "prediction[prediction>10]=10\n",
        "ACC=accuracy_score(y_test,np.round(prediction))\n",
        "MAE=mean_absolute_error(y_test,prediction)\n",
        "CACC=customized_accuracy(y_test.to_numpy(), np.round(prediction))\n",
        "BTS=binary_tresholding_score(y_test.to_numpy(),np.round(prediction))\n",
        "print('For LSTM MAE = {0:.3f}, Accuracy = {1:.3f}, Customized Accuracy = {2:.3f}, Binary Thresholding Score = {3:.3f}'.format(MAE,ACC,CACC,BTS))\n",
        "display(confusion_matrix(y_test,np.round(prediction)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "yAtiu6-Reb6b",
        "outputId": "6826e0ae-f726-4f90-d1b2-a6d5830cbf01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "643/643 [==============================] - 23s 36ms/step\n",
            "For LSTM MAE = 2.620, Accuracy = 0.122, Customized Accuracy = 0.348, Binary Thresholding Score = 0.629\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,   21,   72,  184,  478,  840,  693,  609,  215,    0,    0],\n",
              "       [   0,    0,    5,   10,   48,  110,  114,   62,   18,    0,    0],\n",
              "       [   0,    2,    9,   29,   89,  202,  206,  192,   74,    0,    0],\n",
              "       [   0,    1,    7,   29,   94,  252,  292,  247,  105,    0,    0],\n",
              "       [   0,    3,    1,   10,   42,  116,  164,  162,   61,    0,    0],\n",
              "       [   0,   11,   32,   85,  276,  642,  672,  862,  377,    3,    0],\n",
              "       [   0,    1,    5,    8,   45,  140,  229,  340,  170,    1,    0],\n",
              "       [   0,    4,    9,   39,  160,  449,  570,  820,  442,    4,    0],\n",
              "       [   0,    2,    9,   42,  130,  389,  616, 1113,  726,    5,    0],\n",
              "       [   0,    1,    4,   12,   56,  149,  374,  852,  786,   17,    0],\n",
              "       [   0,    5,   20,   59,  231,  532,  645,  872,  623,   14,    0]])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pas d'amélioration par rapport à son équivalent dans la partie précédente."
      ],
      "metadata": {
        "id": "3crR6ahgDtsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### IV. Utilisation de la sortie du LSTM dans d'autres modèles seupervisés"
      ],
      "metadata": {
        "id": "7v2SP1P1hwDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "Model_rf = Model(inputs=model_lstm.inputs, outputs=model_lstm.layers[1].output)\n",
        "#Model_rf.save('../models/model_lstm_features.h5')\n",
        "corpus_train_lstm = Model_rf.predict(X_train_sequences)\n",
        "corpus_test_lstm = Model_rf.predict(X_test_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDVfqlPOhy6A",
        "outputId": "945e7c10-fc21-44a7-cce4-09f8d9c33efd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2569/2569 [==============================] - 93s 36ms/step\n",
            "643/643 [==============================] - 23s 36ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "algos = {\n",
        "  'RF' : RandomForestRegressor(n_estimators=50,random_state=1,n_jobs=-1),\n",
        "  'KNN' : KNeighborsRegressor(n_neighbors=5,n_jobs=-1,metric='cosine'),\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(20,10),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models_with_binary_tresholding_score(corpus_train_lstm,y_train,corpus_test_lstm,y_test.to_numpy(),algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "id": "ynu_aXujh1AL",
        "outputId": "457601f8-1f09-44db-ef09-4ae985b351da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## RF #############\n",
            "MAE = 2.704, Accuracy = 0.101, Customized Accuracy = 0.517, Binary Thresholding Score = 0.614\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,    7,   50,  212,  607,  917,  870,  377,   71,    1,    0],\n",
              "       [   0,    0,    2,   14,   63,  121,  120,   40,    7,    0,    0],\n",
              "       [   0,    0,    3,   28,  128,  224,  262,  140,   17,    1,    0],\n",
              "       [   0,    1,    6,   40,  125,  297,  365,  160,   33,    0,    0],\n",
              "       [   0,    3,    2,    8,   58,  148,  216,  104,   20,    0,    0],\n",
              "       [   0,    6,   23,  103,  393,  762,  965,  584,  121,    3,    0],\n",
              "       [   0,    1,    3,   16,   75,  195,  331,  267,   51,    0,    0],\n",
              "       [   0,    1,    8,   62,  218,  566,  887,  598,  154,    3,    0],\n",
              "       [   0,    1,    6,   42,  198,  593, 1055,  862,  268,    7,    0],\n",
              "       [   0,    1,    3,    9,  104,  291,  762,  759,  316,    6,    0],\n",
              "       [   0,    2,   18,   89,  309,  640,  987,  682,  270,    4,    0]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## KNN #############\n",
            "MAE = 2.841, Accuracy = 0.106, Customized Accuracy = 0.507, Binary Thresholding Score = 0.580\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  0,  91, 180, 294, 466, 561, 633, 520, 286,  74,   7],\n",
              "       [  0,   5,  16,  31,  56,  81,  74,  59,  35,  10,   0],\n",
              "       [  0,  16,  28,  64,  97, 165, 181, 147,  84,  20,   1],\n",
              "       [  0,   9,  33,  70, 136, 188, 227, 196, 138,  28,   2],\n",
              "       [  0,   4,  10,  31,  68, 109, 129, 121,  67,  20,   0],\n",
              "       [  0,  39,  98, 218, 386, 542, 634, 570, 345, 125,   3],\n",
              "       [  0,   3,  20,  41,  97, 177, 207, 207, 127,  58,   2],\n",
              "       [  0,  19,  61, 122, 282, 419, 530, 548, 409, 103,   4],\n",
              "       [  0,  22,  54, 140, 242, 492, 635, 717, 520, 206,   4],\n",
              "       [  0,   4,  27,  66, 159, 315, 482, 553, 453, 182,  10],\n",
              "       [  0,  32,  87, 198, 293, 519, 613, 603, 468, 178,  10]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## MLP #############\n",
            "MAE = 2.659, Accuracy = 0.106, Customized Accuracy = 0.541, Binary Thresholding Score = 0.626\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,   25,   85,  132,  424,  924,  962,  499,   56,    5,    0],\n",
              "       [   0,    5,    2,    7,   47,  122,  130,   49,    5,    0,    0],\n",
              "       [   0,    2,    9,   18,   92,  223,  268,  169,   21,    1,    0],\n",
              "       [   0,    1,    5,   27,   88,  272,  362,  237,   34,    0,    1],\n",
              "       [   0,    1,    5,   15,   32,  130,  195,  162,   19,    0,    0],\n",
              "       [   0,   13,   37,   64,  239,  692, 1018,  784,  111,    2,    0],\n",
              "       [   0,    0,    4,    8,   42,  162,  324,  344,   55,    0,    0],\n",
              "       [   0,   11,   10,   29,  137,  466,  867,  834,  140,    2,    1],\n",
              "       [   0,    4,   11,   37,  115,  425,  981, 1207,  251,    1,    0],\n",
              "       [   0,    3,    7,    5,   53,  180,  612, 1071,  320,    0,    0],\n",
              "       [   0,    3,   26,   53,  203,  605,  942,  938,  228,    3,    0]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pas d'amélioration par rapport à leurs équivalents de la partie précédente."
      ],
      "metadata": {
        "id": "_vGlO37XD_Vf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Modèle qui prédit et la note et l'utilité"
      ],
      "metadata": {
        "id": "tVBzFKMeZHZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A. Préparation du corpus et des labels"
      ],
      "metadata": {
        "id": "WEvGMc_oK3_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I. Sélection d'un sous ensemble des données mieux équilibrer pour les deux labels"
      ],
      "metadata": {
        "id": "jzU_w0a2LBgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_sample_by_helpful_and_balanced_note(data, n, helpful):\n",
        "  tmp_data = data[data[\"helpful\"] == helpful]\n",
        "  nb_note = df['overall'].unique().shape[0]\n",
        "  tmp_data = pd.concat([data_sample_by_note(tmp_data, n // nb_note, i) for i in df['overall'].unique()])\n",
        "  return tmp_data"
      ],
      "metadata": {
        "id": "jgeS98PhS2i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_line_per_category = 20000"
      ],
      "metadata": {
        "id": "g4GSkneTLMp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset utilisé pour la prédiction de l'utilité\n",
        "ho_df = pd.concat([data_sample_by_helpful_and_balanced_note(df, nb_line_per_category, i) for i in df['helpful'].unique()])\n",
        "# Vérification de la taille\n",
        "ho_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUOYDqCgLXER",
        "outputId": "f95c4ef4-b4c7-484b-e2cf-76508b39b76b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(104576, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 269
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisation de la distribution des notes\n",
        "plt.hist(ho_df['overall'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "dVpvQeo_OoAs",
        "outputId": "690bd97a-1f6e-4e16-e110-90720d568d89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ho_df['overall'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Te4OT-2oP4fH",
        "outputId": "ef14161c-f5c5-4827-fb35-9c5d70d4e0e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.0    29853\n",
              "1.0    22598\n",
              "4.0    20111\n",
              "3.0    16631\n",
              "2.0    15383\n",
              "Name: overall, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 271
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisation de la distribution de l'utilité\n",
        "plt.hist(ho_df['helpful'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "E_tyr9J6Oya3",
        "outputId": "8ca9c4a7-42d5-457b-c4bb-34149563276f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ho_df['helpful'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1naicYdVo7o",
        "outputId": "a5bfd3c4-c513-4250-80a9-b2d669b5ca5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     20000\n",
              "10    20000\n",
              "5     14384\n",
              "8     11719\n",
              "7     10701\n",
              "9      8999\n",
              "3      5323\n",
              "6      4891\n",
              "2      3976\n",
              "4      2907\n",
              "1      1676\n",
              "Name: helpful, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 273
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### II. Split du dataset"
      ],
      "metadata": {
        "id": "cOZRpZPcOKGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Affectation et transformation\n",
        "Y = ho_df[['overall', 'helpful']]\n",
        "X = ho_df[['title', 'reviewText']]"
      ],
      "metadata": {
        "id": "yKaBRrpKOSf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,shuffle=True,random_state=1)"
      ],
      "metadata": {
        "id": "KieAF9lGgYWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_train_tokens = X_train['reviewText'].apply(lambda line : preprocess_text(line))\n",
        "reviews_test_tokens = X_test['reviewText'].apply(lambda line : preprocess_text(line))"
      ],
      "metadata": {
        "id": "fCNPmixVgaGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title_train_tokens = X_train['title'].apply(lambda line : preprocess_text(line))\n",
        "title_test_tokens = X_test['title'].apply(lambda line : preprocess_text(line))"
      ],
      "metadata": {
        "id": "8gdfVzvFgb8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B. Utilisation que du commentaire"
      ],
      "metadata": {
        "id": "hGKmGcy1bSb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I. Moyenne des embeddings"
      ],
      "metadata": {
        "id": "xDbqDVejbXH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def customized_accuracy_score_double(y1_true, y1_pred, y2_true, y2_pred, d1=0, d2=0):\n",
        "  return mean([(1 if abs(y1_true[i] - y1_pred[i]) <= d1 and abs(y2_true[i] - y2_pred[i]) <= d2 else 0) for i in range(len(y1_true))])"
      ],
      "metadata": {
        "id": "SxdutvqjcduT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exécuter les algos en paramètre sur les données en utilisant en plus le score basé sur le seuillage binaire\n",
        "def run_models_multioutput(X_train,Y_train,X_test,Y_test,algos):\n",
        "    for algo_name in algos:\n",
        "        model=algos[algo_name]\n",
        "        model.fit(X_train,Y_train)\n",
        "        prediction=model.predict(X_test)\n",
        "\n",
        "        ytest_overall = Y_test[:,0]\n",
        "        pred_overall = prediction[:,0]\n",
        "        pred_overall[pred_overall < 1] = 1\n",
        "        pred_overall[pred_overall > 5] = 5\n",
        "        \n",
        "        ytest_helpful = Y_test[:,1]\n",
        "        pred_helpful = prediction[:,1]\n",
        "        pred_helpful[pred_helpful < 1] = 1\n",
        "        pred_helpful[pred_helpful > 10] = 10\n",
        "        \n",
        "        OMAE=mean_absolute_error(ytest_overall,pred_overall)\n",
        "        OACC=accuracy_score(ytest_overall,np.round(pred_overall))\n",
        "        OCACC=customized_accuracy(ytest_overall,np.round(pred_overall),1)\n",
        "\n",
        "        HMAE=mean_absolute_error(ytest_helpful,pred_helpful)\n",
        "        HACC=accuracy_score(ytest_helpful,np.round(pred_helpful))\n",
        "        HCACC=customized_accuracy(ytest_helpful,np.round(pred_helpful),2)\n",
        "        HBTS=binary_tresholding_score(ytest_helpful,np.round(pred_helpful))\n",
        "\n",
        "        ACC=customized_accuracy_score_double(ytest_overall,np.round(pred_overall),ytest_helpful,np.round(pred_helpful))\n",
        "        CACC=customized_accuracy_score_double(ytest_overall,np.round(pred_overall),ytest_helpful,np.round(pred_helpful),1,2)\n",
        "\n",
        "        print('################## {0} #############'.format(algo_name))\n",
        "        print('Note: MAE = {0:.3f}, Accuracy = {1:.3f}, Customized Accuracy = {2:.3f}'.format(OMAE,OACC,OCACC))\n",
        "        print('Utilité: MAE = {0:.3f}, Accuracy = {1:.3f}, Customized Accuracy = {2:.3f}, Binary Thresholding Score = {3:.3f}'.format(HMAE,HACC,HCACC,HBTS))\n",
        "        print('Les deux: Accuracy = {0:.3f}, Customized Accuracy = {1:.3f}'.format(ACC,CACC))\n",
        "        display(confusion_matrix(ytest_overall,np.round(pred_overall)))\n",
        "        display(confusion_matrix(ytest_helpful,np.round(pred_helpful)))\n",
        "        print()"
      ],
      "metadata": {
        "id": "-2fDUkiqWmVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul de de l'embedding des phrases\n",
        "vector_size=model_google_vec_neg_300.vector_size\n",
        "corpus_train_wv_google=word2vec_generator(reviews_train_tokens,model_google_vec_neg_300,vector_size)\n",
        "corpus_test_wv_google=word2vec_generator(reviews_test_tokens,model_google_vec_neg_300,vector_size)"
      ],
      "metadata": {
        "id": "5qwkOAmpfBTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modèles\n",
        "algos = {\n",
        "  'RF' : RandomForestRegressor(n_estimators=50,random_state=1,n_jobs=-1),\n",
        "  'KNN' : KNeighborsRegressor(n_neighbors=5,n_jobs=-1,metric='cosine'),\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(20,10),max_iter=800,random_state=1,alpha=0.001)\n",
        "}"
      ],
      "metadata": {
        "id": "4YtZv6HdfDwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_models_multioutput(corpus_train_wv_google, y_train, corpus_test_wv_google, y_test.to_numpy(), algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OnUHITRofG1I",
        "outputId": "013739d4-c147-47a8-a1bd-e69c29e80d7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## RF #############\n",
            "Note: MAE = 1.228, Accuracy = 0.192, Customized Accuracy = 0.615\n",
            "Utilité: MAE = 2.961, Accuracy = 0.096, Customized Accuracy = 0.440, Binary Thresholding Score = 0.602\n",
            "Les deux: Accuracy = 0.016, Customized Accuracy = 0.261\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   1,  714, 3539,  331,    1],\n",
              "       [   0,  263, 2439,  356,    0],\n",
              "       [   0,  134, 2541,  631,    0],\n",
              "       [   0,   86, 2699, 1217,    1],\n",
              "       [   0,   77, 3664, 2222,    0]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,    4,    2,  120, 1017, 1682,  964,  325,   32,    0,    0],\n",
              "       [   0,    0,    0,    2,   73,  125,   89,   32,    3,    0,    0],\n",
              "       [   0,    0,    1,    8,  123,  309,  251,   67,    9,    0,    0],\n",
              "       [   0,    1,    0,   12,  156,  365,  350,  152,   20,    0,    0],\n",
              "       [   0,    0,    0,    6,   58,  178,  191,   74,   13,    0,    0],\n",
              "       [   0,    0,    0,   31,  410, 1001, 1014,  427,   66,    0,    0],\n",
              "       [   0,    1,    0,    4,   73,  262,  378,  222,   32,    0,    0],\n",
              "       [   0,    0,    0,   15,  216,  615,  768,  431,   56,    2,    0],\n",
              "       [   0,    0,    0,   11,  192,  616,  862,  580,  123,    2,    0],\n",
              "       [   0,    0,    0,   10,  105,  335,  637,  544,  138,    0,    0],\n",
              "       [   0,    0,    2,   42,  489, 1266, 1258,  737,  128,    1,    0]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## KNN #############\n",
            "Note: MAE = 1.192, Accuracy = 0.236, Customized Accuracy = 0.670\n",
            "Utilité: MAE = 3.112, Accuracy = 0.103, Customized Accuracy = 0.480, Binary Thresholding Score = 0.563\n",
            "Les deux: Accuracy = 0.025, Customized Accuracy = 0.316\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[ 298, 1311, 1971,  938,   68],\n",
              "       [  60,  670, 1448,  819,   61],\n",
              "       [  45,  473, 1537, 1146,  105],\n",
              "       [  19,  353, 1563, 1803,  265],\n",
              "       [  28,  439, 2058, 2817,  621]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,   30,   75,  171,  376,  635,  875,  964,  683,  316,   21],\n",
              "       [   0,    1,    9,   17,   27,   50,   72,   72,   55,   20,    1],\n",
              "       [   0,    2,    9,   28,   71,  112,  168,  193,  131,   51,    3],\n",
              "       [   0,    5,    8,   28,   70,  142,  233,  268,  221,   77,    4],\n",
              "       [   0,    2,    2,    6,   34,   69,  121,  146,  103,   35,    2],\n",
              "       [   0,    9,   30,   77,  189,  399,  664,  726,  599,  242,   14],\n",
              "       [   0,    2,    4,   18,   46,  110,  230,  258,  222,   79,    3],\n",
              "       [   0,    6,    7,   36,  111,  254,  413,  640,  465,  161,   10],\n",
              "       [   0,    3,    7,   26,  100,  239,  490,  697,  599,  209,   16],\n",
              "       [   0,    2,    4,   12,   54,  178,  345,  522,  457,  181,   14],\n",
              "       [   0,    7,   40,   84,  240,  462,  819, 1000,  846,  384,   41]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## MLP #############\n",
            "Note: MAE = 1.013, Accuracy = 0.283, Customized Accuracy = 0.763\n",
            "Utilité: MAE = 2.981, Accuracy = 0.093, Customized Accuracy = 0.463, Binary Thresholding Score = 0.593\n",
            "Les deux: Accuracy = 0.026, Customized Accuracy = 0.347\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[ 662, 1956, 1514,  439,   15],\n",
              "       [ 123,  981, 1370,  561,   23],\n",
              "       [  40,  648, 1539, 1004,   75],\n",
              "       [  13,  317, 1516, 1871,  286],\n",
              "       [  11,  333, 1614, 3139,  866]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  0,  92, 158, 413, 797, 963, 912, 590, 190,  30,   1],\n",
              "       [  0,   5,  11,  22,  75,  65,  91,  47,   7,   1,   0],\n",
              "       [  0,  10,  18,  44, 149, 207, 181, 127,  30,   2,   0],\n",
              "       [  0,   8,  25,  75, 150, 233, 288, 213,  61,   3,   0],\n",
              "       [  0,   2,   9,  28,  65, 122, 156, 115,  19,   3,   1],\n",
              "       [  0,  25,  55, 178, 437, 648, 760, 632, 192,  20,   2],\n",
              "       [  0,   3,   9,  31,  91, 194, 292, 261,  82,   9,   0],\n",
              "       [  0,  10,  28,  85, 248, 385, 579, 560, 194,  14,   0],\n",
              "       [  0,  12,  17,  79, 206, 422, 670, 689, 260,  31,   0],\n",
              "       [  0,   3,  12,  35, 106, 246, 453, 594, 288,  31,   1],\n",
              "       [  0,  20,  93, 211, 481, 806, 947, 924, 401,  40,   0]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Pseudo courbe d'apprentissage du meilleur modèle"
      ],
      "metadata": {
        "id": "le1Zo67qkeKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_train_wv_google.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNIh8g31lMtA",
        "outputId": "b7264df5-50a3-497e-a37d-9eb63556eb3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(83660, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 291
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_train_wv_google.iloc[:int(y_train.shape[0] * 0.8)].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTa13wq-lYdw",
        "outputId": "0bdab0d9-fb12-459d-b09e-17cd9e1b3cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(66928, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 294
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "algos = {\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(20,10),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models_multioutput(corpus_train_wv_google.iloc[:int(y_train.shape[0] * 0.8)], y_train.iloc[:int(y_train.shape[0] * 0.8)], corpus_test_wv_google, y_test.to_numpy(), algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "SePIOq7iktWO",
        "outputId": "69438c4c-5462-4f94-d49a-3f0509e862c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## MLP #############\n",
            "Note: MAE = 1.016, Accuracy = 0.271, Customized Accuracy = 0.772\n",
            "Utilité: MAE = 2.963, Accuracy = 0.093, Customized Accuracy = 0.471, Binary Thresholding Score = 0.598\n",
            "Les deux: Accuracy = 0.024, Customized Accuracy = 0.360\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[ 458, 2166, 1402,  537,   23],\n",
              "       [  78, 1001, 1295,  669,   15],\n",
              "       [  20,  609, 1407, 1235,   35],\n",
              "       [   8,  341, 1228, 2261,  165],\n",
              "       [   8,  306, 1397, 3716,  536]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,  145,  188,  397,  692,  874,  963,  659,  193,   27,    8],\n",
              "       [   0,    6,   13,   28,   60,   90,   74,   41,   10,    2,    0],\n",
              "       [   0,   16,   18,   43,  111,  201,  202,  139,   36,    2,    0],\n",
              "       [   0,   13,   28,   70,  139,  194,  318,  227,   65,    2,    0],\n",
              "       [   0,    5,    7,   30,   55,  109,  161,  121,   29,    3,    0],\n",
              "       [   0,   41,   64,  166,  347,  594,  819,  724,  181,   12,    1],\n",
              "       [   0,    3,   14,   27,   71,  169,  309,  306,   61,   11,    1],\n",
              "       [   0,   11,   29,   73,  201,  363,  622,  605,  190,    9,    0],\n",
              "       [   0,   12,   29,   68,  163,  363,  649,  802,  276,   24,    0],\n",
              "       [   0,    7,   12,   42,   78,  210,  464,  645,  292,   19,    0],\n",
              "       [   0,   47,   92,  196,  428,  672, 1045, 1020,  388,   32,    3]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il n'est pas nécessaire de rajouter d'autres données pour améliorer l'apprentissage: Il n'y a pas de différence entre les deux trainings pour plus ou moins plus de données."
      ],
      "metadata": {
        "id": "w6_BJwHdm5Hk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Amélioration du meilleur modèle"
      ],
      "metadata": {
        "id": "W8IVAgtcmOih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plus de neurones\n",
        "algos = {\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(40, 20),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models_multioutput(corpus_train_wv_google, y_train, corpus_test_wv_google, y_test.to_numpy(), algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "4PNw8eyLmSti",
        "outputId": "81fdbbf6-704f-480e-860b-0e704f4f5d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## MLP #############\n",
            "Note: MAE = 1.009, Accuracy = 0.292, Customized Accuracy = 0.766\n",
            "Utilité: MAE = 3.073, Accuracy = 0.092, Customized Accuracy = 0.473, Binary Thresholding Score = 0.586\n",
            "Les deux: Accuracy = 0.025, Customized Accuracy = 0.358\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[ 469, 1838, 1597,  646,   36],\n",
              "       [  76,  810, 1404,  740,   28],\n",
              "       [  20,  466, 1480, 1247,   93],\n",
              "       [  10,  225, 1200, 2181,  387],\n",
              "       [   6,  217, 1272, 3309, 1159]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  0, 383, 235, 366, 554, 724, 723, 682, 311,  90,  78],\n",
              "       [  0,  25,  16,  27,  41,  75,  80,  42,  10,   3,   5],\n",
              "       [  0,  47,  34,  56,  90, 138, 190, 131,  52,  22,   8],\n",
              "       [  0,  44,  39,  70, 125, 207, 244, 194, 101,  23,   9],\n",
              "       [  0,  12,  17,  31,  57,  90, 123, 115,  54,  14,   7],\n",
              "       [  0, 152,  94, 203, 307, 506, 675, 636, 267,  76,  33],\n",
              "       [  0,  31,  27,  34,  78, 151, 270, 258,  97,  18,   8],\n",
              "       [  0,  64,  52, 104, 174, 349, 542, 528, 224,  49,  17],\n",
              "       [  0,  43,  43, 105, 180, 348, 597, 669, 300,  67,  34],\n",
              "       [  0,  25,  28,  46,  98, 228, 411, 535, 316,  65,  17],\n",
              "       [  0, 172, 133, 213, 353, 573, 857, 924, 491, 141,  66]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Legere amélioration dans la prédiction de la note et de la prédiction global."
      ],
      "metadata": {
        "id": "KcP99_nCpvS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Moins de neurones\n",
        "algos = {\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(10, 5),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models_multioutput(corpus_train_wv_google, y_train, corpus_test_wv_google, y_test.to_numpy(), algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "tYav5WrAmc8s",
        "outputId": "9224d4b8-3175-4068-c46b-ecdf770fc523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## MLP #############\n",
            "Note: MAE = 1.019, Accuracy = 0.278, Customized Accuracy = 0.764\n",
            "Utilité: MAE = 2.948, Accuracy = 0.093, Customized Accuracy = 0.459, Binary Thresholding Score = 0.603\n",
            "Les deux: Accuracy = 0.025, Customized Accuracy = 0.348\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[ 408, 1996, 1579,  580,   23],\n",
              "       [  66,  883, 1402,  687,   20],\n",
              "       [  21,  472, 1529, 1229,   55],\n",
              "       [   3,  265, 1340, 2163,  232],\n",
              "       [   9,  243, 1448, 3430,  833]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,   42,   74,  298,  678, 1174, 1235,  572,   70,    2,    1],\n",
              "       [   0,    2,    7,   17,   63,   90,   99,   42,    3,    1,    0],\n",
              "       [   0,    3,    8,   39,  107,  212,  277,  111,   11,    0,    0],\n",
              "       [   0,    5,   10,   47,  121,  256,  362,  233,   22,    0,    0],\n",
              "       [   0,    2,    6,   14,   50,  106,  220,  106,   16,    0,    0],\n",
              "       [   0,    5,   29,   85,  339,  703, 1043,  661,   83,    1,    0],\n",
              "       [   0,    1,    4,   17,   63,  177,  389,  283,   37,    1,    0],\n",
              "       [   0,    1,   10,   38,  150,  431,  770,  619,   81,    3,    0],\n",
              "       [   0,    2,   15,   34,  137,  404,  890,  784,  117,    3,    0],\n",
              "       [   0,    1,    4,   16,   76,  226,  588,  695,  159,    4,    0],\n",
              "       [   0,    8,   26,  118,  379,  797, 1407, 1049,  138,    1,    0]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pas d'amélioration."
      ],
      "metadata": {
        "id": "CnXUR9kSnONI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### II. Prise en compte de la longueur, de la spécificité et de la subjectivité du texte"
      ],
      "metadata": {
        "id": "2QL5zG1y34ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul de la longueur, la spécificité et la subjectivité\n",
        "df_meta_train=pd.DataFrame([])\n",
        "df_meta_train['length_text'] = [len(reviews_train_tokens.iloc[i]) for i in range(len(reviews_train_tokens))]\n",
        "df_meta_train['polarity'] = [polarity_txt(X_train[\"reviewText\"].iloc[i]) for i in range(len(X_train))]\n",
        "df_meta_train['subjectivity'] = [subj_txt(X_train[\"reviewText\"].iloc[i]) for i in range(len(X_train))]\n",
        "\n",
        "df_meta_test=pd.DataFrame([])\n",
        "df_meta_test['length_text'] = [len(reviews_test_tokens.iloc[i]) for i in range(len(reviews_test_tokens))]\n",
        "df_meta_test['polarity'] = [polarity_txt(X_test[\"reviewText\"].iloc[i]) for i in range(len(X_test))]\n",
        "df_meta_test['subjectivity'] = [subj_txt(X_test[\"reviewText\"].iloc[i]) for i in range(len(X_test))]"
      ],
      "metadata": {
        "id": "NnnM-aNVnRjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concaténation\n",
        "corpus_train_wv_google_meta=pd.concat([corpus_train_wv_google,df_meta_train],axis=1)\n",
        "corpus_test_wv_google_meta=pd.concat([corpus_test_wv_google,df_meta_test],axis=1)"
      ],
      "metadata": {
        "id": "CQOiK8i7nUwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entraînement\n",
        "algos = {\n",
        "  'RF' : RandomForestRegressor(n_estimators=50,random_state=1,n_jobs=-1),\n",
        "  'KNN' : KNeighborsRegressor(n_neighbors=5,n_jobs=-1,metric='cosine'),\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(20,10),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models_multioutput(corpus_train_wv_google_meta,y_train,corpus_test_wv_google_meta,y_test.to_numpy(),algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HTCxfnYInaJp",
        "outputId": "d5a2afc1-8d7d-4d63-9fec-f2f05059214c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## RF #############\n",
            "Note: MAE = 1.090, Accuracy = 0.227, Customized Accuracy = 0.738\n",
            "Utilité: MAE = 2.944, Accuracy = 0.092, Customized Accuracy = 0.445, Binary Thresholding Score = 0.607\n",
            "Les deux: Accuracy = 0.018, Customized Accuracy = 0.322\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  24, 2234, 1802,  526,    0],\n",
              "       [   2,  937, 1530,  589,    0],\n",
              "       [   4,  559, 1659, 1083,    1],\n",
              "       [   0,  327, 1590, 2071,   15],\n",
              "       [   1,  350, 1876, 3680,   56]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,    3,   16,  262, 1043, 1442,  992,  347,   41,    0,    0],\n",
              "       [   0,    0,    0,    9,   66,  128,   92,   27,    2,    0,    0],\n",
              "       [   0,    0,    3,   22,  146,  268,  227,   90,   12,    0,    0],\n",
              "       [   0,    1,    0,   27,  163,  348,  338,  154,   24,    1,    0],\n",
              "       [   0,    0,    1,    9,   64,  159,  199,   75,   13,    0,    0],\n",
              "       [   0,    0,    4,   86,  441,  851, 1023,  468,   76,    0,    0],\n",
              "       [   0,    1,    0,    9,   86,  221,  384,  240,   31,    0,    0],\n",
              "       [   0,    0,    1,   31,  239,  535,  762,  451,   84,    0,    0],\n",
              "       [   0,    0,    0,   28,  193,  580,  817,  629,  137,    2,    0],\n",
              "       [   0,    0,    0,   12,   79,  294,  635,  596,  151,    2,    0],\n",
              "       [   0,    0,    4,   73,  536, 1101, 1308,  762,  135,    4,    0]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## KNN #############\n",
            "Note: MAE = 1.068, Accuracy = 0.292, Customized Accuracy = 0.733\n",
            "Utilité: MAE = 3.092, Accuracy = 0.091, Customized Accuracy = 0.459, Binary Thresholding Score = 0.571\n",
            "Les deux: Accuracy = 0.024, Customized Accuracy = 0.334\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[ 709, 1626, 1451,  693,  107],\n",
              "       [ 179,  824, 1206,  752,   97],\n",
              "       [  78,  566, 1289, 1146,  227],\n",
              "       [  33,  327, 1218, 1839,  586],\n",
              "       [  46,  385, 1381, 2709, 1442]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  0, 109, 247, 422, 648, 846, 817, 604, 325, 117,  11],\n",
              "       [  0,   4,   5,  36,  43,  87,  81,  36,  24,   8,   0],\n",
              "       [  0,  13,  28,  62, 127, 145, 165, 127,  76,  24,   1],\n",
              "       [  0,  18,  37,  70, 126, 208, 252, 206, 109,  30,   0],\n",
              "       [  0,   6,  18,  29,  66, 110, 111, 108,  50,  22,   0],\n",
              "       [  0,  42,  85, 203, 387, 555, 656, 600, 309, 105,   7],\n",
              "       [  0,  14,  19,  51,  94, 165, 233, 216, 131,  47,   2],\n",
              "       [  0,  17,  57, 109, 221, 380, 506, 455, 269,  81,   8],\n",
              "       [  0,  16,  50,  98, 234, 378, 549, 546, 360, 148,   7],\n",
              "       [  0,   9,  29,  59, 154, 228, 402, 442, 333, 108,   5],\n",
              "       [  0,  40, 126, 280, 465, 702, 782, 773, 522, 219,  14]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## MLP #############\n",
            "Note: MAE = 0.974, Accuracy = 0.293, Customized Accuracy = 0.789\n",
            "Utilité: MAE = 2.971, Accuracy = 0.086, Customized Accuracy = 0.444, Binary Thresholding Score = 0.609\n",
            "Les deux: Accuracy = 0.023, Customized Accuracy = 0.343\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[ 651, 2380, 1208,  331,   16],\n",
              "       [ 123, 1203, 1262,  456,   14],\n",
              "       [  38,  728, 1578,  923,   39],\n",
              "       [  11,  374, 1546, 1828,  244],\n",
              "       [   8,  349, 1561, 3172,  873]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,  110,  126,  303,  734, 1301, 1269,  288,   14,    1,    0],\n",
              "       [   0,    5,   10,   20,   50,  132,   92,   15,    0,    0,    0],\n",
              "       [   0,    8,   13,   39,  125,  284,  247,   51,    0,    1,    0],\n",
              "       [   0,   12,   13,   45,  132,  346,  432,   71,    5,    0,    0],\n",
              "       [   0,    4,    9,   18,   46,  176,  221,   43,    3,    0,    0],\n",
              "       [   0,   38,   47,  101,  358,  884, 1246,  263,   12,    0,    0],\n",
              "       [   0,    4,    5,   21,   78,  252,  481,  125,    6,    0,    0],\n",
              "       [   0,    9,   13,   55,  159,  561,  990,  306,   10,    0,    0],\n",
              "       [   0,   16,   17,   35,  151,  529, 1212,  404,   22,    0,    0],\n",
              "       [   0,    5,    8,   22,   68,  299,  887,  453,   27,    0,    0],\n",
              "       [   0,   40,   55,  115,  368, 1025, 1681,  602,   35,    2,    0]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Petite amélioration dans la prédiction de la note"
      ],
      "metadata": {
        "id": "e3ilQMjTnohq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### III. Prise en compte de l'ordre des mots (LSTM)"
      ],
      "metadata": {
        "id": "AdX1yIxWbr9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = {word: idx for idx, word in enumerate(model_google_vec_neg_300.index_to_key)}\n",
        "X_train_sequences = [to_sequence(word2idx, x) for x in reviews_train_tokens]\n",
        "X_test_sequences = [to_sequence(word2idx, x) for x in  reviews_test_tokens]\n",
        "\n",
        "print(reviews_train_tokens.values[0])\n",
        "print(X_train_sequences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2S8REggnv6-",
        "outputId": "2b4ad874-750f-4e3d-e4ed-a4ab839c73f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['if', 'amazon', 'com', 'zero', 'star', 'rate', 'that', 'product', 'machin', 'month', 'enjoi', 'work', 'meticul', 'maintain', 'clean', 'us', 'recent', 'detect', 'burn', 'odor', 'machin', 'got', 'loud', 'realiz', 'go', 'close', 'examin', 'part', 'the', 'blade', 'basket', 'cheap', 'gotta', 'cut', 'cost', 'somewher', 'the', 'teeth', 'blade', 'grind', 'liquifi', 'eventu', 'start', 'bend', 'upward', 'actual', 'start', 'eat', 'plastic', 'food', 'chute', 'perhap', 'defect', 'blade', 'doubt', 'work', 'fine', 'got', 'it', 'in', 'event', 'unbeknownst', 'me', 'probabl', 'ingest', 'small', 'particl', 'plastic', 'yummi', 'fruit', 'drink', 'needless', 'sai', 'extrem', 'unhappi', 'product', 'perhap', 'want', 'bui', 'new', 'basket', 'month', 'my', 'advic', 'save', 'monei', 'heavi', 'duti', 'machin', 'ebai']\n",
            "[88, 183690, 18117, 4200, 948, 511, 3, 684, 1701433, 212, 141, 1907, 1749, 164, 386, 7624, 5566, 15860, 1701433, 193, 6092, 152, 398, 1954453, 186, 11, 16693, 4490, 4240, 12944, 529, 370, 11, 6969, 16693, 13381, 284, 14120, 10271, 2769, 284, 2785, 4471, 560, 39543, 17046, 16693, 1962, 141, 1389, 193, 15, 1, 424, 68886, 170, 47121, 428, 4471, 5169, 3554, 25417, 229984, 1560228, 684, 189, 972514, 65, 4490, 212, 126, 1114, 1701433]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import pad_sequences\n",
        "\n",
        "MAX_SEQ_LENGHT=50\n",
        "N_FEATURES = len(model_google_vec_neg_300.index_to_key)\n",
        "X_train_sequences = pad_sequences(X_train_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)\n",
        "X_test_sequences = pad_sequences(X_test_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)\n",
        "print(X_train_sequences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXdu1C88oAiB",
        "outputId": "a96c9865-dd71-4556-f97c-294c55726ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1954453     186      11   16693    4490    4240   12944     529     370\n",
            "      11    6969   16693   13381     284   14120   10271    2769     284\n",
            "    2785    4471     560   39543   17046   16693    1962     141    1389\n",
            "     193      15       1     424   68886     170   47121     428    4471\n",
            "    5169    3554   25417  229984 1560228     684     189  972514      65\n",
            "    4490     212     126    1114 1701433]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDINGS_LEN = model_google_vec_neg_300.vector_size\n",
        "embeddings_index = np.zeros((len(model_google_vec_neg_300.index_to_key)+1, EMBEDDINGS_LEN))\n",
        "embeddings_index.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdZUwxxToKIj",
        "outputId": "acb553e7-ef3a-42dc-edb2-de3805630d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000001, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 306
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word, idx in word2idx.items():\n",
        "    try:\n",
        "        embedding = model_google_vec_neg_300[word]\n",
        "        embeddings_index[idx] = embedding\n",
        "    except:\n",
        "        pass\n",
        "      \n",
        "print(\"EMBEDDINGS_LEN=\", EMBEDDINGS_LEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Chl23TvgoPm-",
        "outputId": "85489d0b-dd60-4365-d24c-42116a8c16c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EMBEDDINGS_LEN= 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense,LSTM\n",
        "\n",
        "\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(len(model_google_vec_neg_300.key_to_index)+1,\n",
        "                    EMBEDDINGS_LEN,  # Embedding size\n",
        "                    weights=[embeddings_index],\n",
        "                    trainable=False))\n",
        "\n",
        "model_lstm.add(LSTM(300, dropout=0.2))\n",
        "\n",
        "model_lstm.add(Dense(2))\n",
        "model_lstm.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "model_lstm.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoAX0yTDoS2x",
        "outputId": "161122d3-4d24-4bd4-fe3b-41599504618b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (None, None, 300)         900000300 \n",
            "                                                                 \n",
            " lstm_6 (LSTM)               (None, 300)               721200    \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 2)                 602       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 900,722,102\n",
            "Trainable params: 721,802\n",
            "Non-trainable params: 900,000,300\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_lstm.fit(X_train_sequences, y_train, epochs=10, batch_size=128, verbose=1,validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r957sYgAogc3",
        "outputId": "13b00341-ace7-45ed-c2a1-c5caa181c201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "589/589 [==============================] - 84s 139ms/step - loss: 2.0405 - mean_absolute_error: 2.0405 - val_loss: 1.9452 - val_mean_absolute_error: 1.9452\n",
            "Epoch 2/10\n",
            "589/589 [==============================] - 82s 139ms/step - loss: 1.9348 - mean_absolute_error: 1.9348 - val_loss: 1.9450 - val_mean_absolute_error: 1.9450\n",
            "Epoch 3/10\n",
            "589/589 [==============================] - 82s 139ms/step - loss: 1.9011 - mean_absolute_error: 1.9011 - val_loss: 1.8902 - val_mean_absolute_error: 1.8902\n",
            "Epoch 4/10\n",
            "589/589 [==============================] - 82s 140ms/step - loss: 1.8798 - mean_absolute_error: 1.8798 - val_loss: 1.9125 - val_mean_absolute_error: 1.9125\n",
            "Epoch 5/10\n",
            "589/589 [==============================] - 80s 136ms/step - loss: 1.8636 - mean_absolute_error: 1.8636 - val_loss: 1.8823 - val_mean_absolute_error: 1.8823\n",
            "Epoch 6/10\n",
            "589/589 [==============================] - 80s 136ms/step - loss: 1.8446 - mean_absolute_error: 1.8446 - val_loss: 1.9011 - val_mean_absolute_error: 1.9011\n",
            "Epoch 7/10\n",
            "589/589 [==============================] - 80s 136ms/step - loss: 1.8291 - mean_absolute_error: 1.8291 - val_loss: 1.8741 - val_mean_absolute_error: 1.8741\n",
            "Epoch 8/10\n",
            "589/589 [==============================] - 80s 135ms/step - loss: 1.8105 - mean_absolute_error: 1.8105 - val_loss: 1.8674 - val_mean_absolute_error: 1.8674\n",
            "Epoch 9/10\n",
            "589/589 [==============================] - 81s 137ms/step - loss: 1.7888 - mean_absolute_error: 1.7888 - val_loss: 1.8747 - val_mean_absolute_error: 1.8747\n",
            "Epoch 10/10\n",
            "589/589 [==============================] - 80s 136ms/step - loss: 1.7645 - mean_absolute_error: 1.7645 - val_loss: 1.8765 - val_mean_absolute_error: 1.8765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores_lstm = model_lstm.evaluate(X_test_sequences, y_test, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zVQwe8QojJo",
        "outputId": "6fb26383-7ab4-4e99-d1a0-f536011e0d5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "654/654 [==============================] - 16s 25ms/step - loss: 1.8689 - mean_absolute_error: 1.8689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction=model_lstm.predict(X_test_sequences)\n",
        "\n",
        "ytest_overall = y_test.iloc[:,0].to_numpy()\n",
        "pred_overall = prediction[:,0]\n",
        "pred_overall[pred_overall < 1] = 1\n",
        "pred_overall[pred_overall > 5] = 5\n",
        "        \n",
        "ytest_helpful = y_test.iloc[:,1].to_numpy()\n",
        "pred_helpful = prediction[:,1]\n",
        "pred_helpful[pred_helpful < 1] = 1\n",
        "pred_helpful[pred_helpful > 10] = 10\n",
        "        \n",
        "OMAE=mean_absolute_error(ytest_overall,pred_overall)\n",
        "OACC=accuracy_score(ytest_overall,np.round(pred_overall))\n",
        "OCACC=customized_accuracy(ytest_overall,np.round(pred_overall),1)\n",
        "\n",
        "HMAE=mean_absolute_error(ytest_helpful,pred_helpful)\n",
        "HACC=accuracy_score(ytest_helpful,np.round(pred_helpful))\n",
        "HCACC=customized_accuracy(ytest_helpful,np.round(pred_helpful),2)\n",
        "HBTS=binary_tresholding_score(ytest_helpful,np.round(pred_helpful))\n",
        "\n",
        "ACC=customized_accuracy_score_double(ytest_overall,np.round(pred_overall),ytest_helpful,np.round(pred_helpful))\n",
        "CACC=customized_accuracy_score_double(ytest_overall,np.round(pred_overall),ytest_helpful,np.round(pred_helpful),1,2)\n",
        "\n",
        "print('Note: MAE = {0:.3f}, Accuracy = {1:.3f}, Customized Accuracy = {2:.3f}'.format(OMAE,OACC,OCACC))\n",
        "print('Utilité: MAE = {0:.3f}, Accuracy = {1:.3f}, Customized Accuracy = {2:.3f}, Binary Thresholding Score = {3:.3f}'.format(HMAE,HACC,HCACC,HBTS))\n",
        "print('Les deux: Accuracy = {0:.3f}, Customized Accuracy = {1:.3f}'.format(ACC,CACC))\n",
        "display(confusion_matrix(ytest_overall,np.round(pred_overall)))\n",
        "display(confusion_matrix(ytest_helpful,np.round(pred_helpful)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "LabpO6Acoo_N",
        "outputId": "15c1c565-8d44-42fe-c2ad-6967bb381f2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "654/654 [==============================] - 16s 25ms/step\n",
            "Note: MAE = 0.832, Accuracy = 0.439, Customized Accuracy = 0.820\n",
            "Utilité: MAE = 2.898, Accuracy = 0.102, Customized Accuracy = 0.500, Binary Thresholding Score = 0.614\n",
            "Les deux: Accuracy = 0.047, Customized Accuracy = 0.410\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[2275, 1079,  655,  439,  138],\n",
              "       [ 658,  913,  813,  554,  120],\n",
              "       [ 272,  617, 1073, 1051,  293],\n",
              "       [  93,  244,  768, 1785, 1113],\n",
              "       [  91,  240,  627, 1868, 3137]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,  203,  179,  352,  582,  830,  868,  773,  321,   37,    1],\n",
              "       [   0,    9,   15,   34,   51,   82,   66,   53,   14,    0,    0],\n",
              "       [   0,   13,   22,   68,  122,  160,  160,  156,   61,    6,    0],\n",
              "       [   0,   12,   24,   65,  128,  234,  231,  258,   93,   11,    0],\n",
              "       [   0,    3,   14,   23,   56,  100,  125,  136,   58,    5,    0],\n",
              "       [   0,   37,   68,  135,  354,  541,  654,  764,  369,   25,    2],\n",
              "       [   0,    9,   10,   20,   77,  148,  233,  322,  147,    6,    0],\n",
              "       [   0,    7,   28,   69,  188,  301,  450,  677,  352,   31,    0],\n",
              "       [   0,   11,   15,   56,  153,  312,  491,  810,  490,   48,    0],\n",
              "       [   0,    7,   10,   25,   63,  131,  320,  665,  502,   45,    1],\n",
              "       [   0,   56,   70,  137,  331,  603,  801, 1154,  686,   84,    1]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Meilleur jusqu'à présent."
      ],
      "metadata": {
        "id": "tWxyq-K3wm5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### IV. Utilisation de la sortie du LSTM dans d'autres modèles seupervisés"
      ],
      "metadata": {
        "id": "htE_CKvKvaZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "Model_rf = Model(inputs=model_lstm.inputs, outputs=model_lstm.layers[1].output)\n",
        "#Model_rf.save('../models/model_lstm_features.h5')\n",
        "corpus_train_lstm = Model_rf.predict(X_train_sequences)\n",
        "corpus_test_lstm = Model_rf.predict(X_test_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrS1LDtPqJ4v",
        "outputId": "7e0cc85d-7c68-4504-b664-986b2c6b6a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2615/2615 [==============================] - 64s 25ms/step\n",
            "654/654 [==============================] - 16s 25ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "algos = {\n",
        "  'RF' : RandomForestRegressor(n_estimators=50,random_state=1,n_jobs=-1),\n",
        "  'KNN' : KNeighborsRegressor(n_neighbors=5,n_jobs=-1,metric='cosine'),\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(20,10),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models_multioutput(corpus_train_lstm,y_train,corpus_test_lstm,y_test.to_numpy(),algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YadZhdwIqMwp",
        "outputId": "5690d24b-c02c-4b0d-ad31-2fd7fd97b493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## RF #############\n",
            "Note: MAE = 0.899, Accuracy = 0.346, Customized Accuracy = 0.828\n",
            "Utilité: MAE = 2.952, Accuracy = 0.092, Customized Accuracy = 0.458, Binary Thresholding Score = 0.602\n",
            "Les deux: Accuracy = 0.031, Customized Accuracy = 0.377\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[1085, 2187,  829,  459,   26],\n",
              "       [ 204, 1285,  993,  549,   27],\n",
              "       [  78,  755, 1278, 1124,   71],\n",
              "       [  25,  313, 1002, 2310,  353],\n",
              "       [  25,  340,  853, 3476, 1269]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,   47,  174,  451,  764, 1067, 1015,  498,  121,    9,    0],\n",
              "       [   0,    3,    5,   28,   70,  101,   75,   36,    6,    0,    0],\n",
              "       [   0,    1,   15,   70,  146,  197,  208,  110,   20,    1,    0],\n",
              "       [   0,    5,   13,   54,  184,  267,  296,  197,   38,    2,    0],\n",
              "       [   0,    0,    6,   26,   76,  126,  156,  112,   17,    1,    0],\n",
              "       [   0,   10,   43,  174,  426,  732,  872,  550,  138,    4,    0],\n",
              "       [   0,    1,    6,   26,   93,  221,  345,  232,   48,    0,    0],\n",
              "       [   0,    1,   15,   76,  218,  484,  630,  528,  146,    5,    0],\n",
              "       [   0,    2,    8,   64,  192,  485,  781,  676,  175,    3,    0],\n",
              "       [   0,    0,   16,   25,   88,  241,  586,  592,  215,    6,    0],\n",
              "       [   0,   16,   64,  188,  468,  847, 1178,  914,  238,   10,    0]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## KNN #############\n",
            "Note: MAE = 0.905, Accuracy = 0.386, Customized Accuracy = 0.803\n",
            "Utilité: MAE = 3.085, Accuracy = 0.094, Customized Accuracy = 0.468, Binary Thresholding Score = 0.577\n",
            "Les deux: Accuracy = 0.037, Customized Accuracy = 0.373\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[1660, 1554,  794,  434,  144],\n",
              "       [ 447, 1015,  904,  555,  137],\n",
              "       [ 187,  676, 1123, 1074,  246],\n",
              "       [  70,  307,  910, 1846,  870],\n",
              "       [  70,  293,  893, 2277, 2430]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  0, 173, 257, 402, 620, 690, 742, 680, 389, 169,  24],\n",
              "       [  0,   9,  16,  35,  60,  52,  62,  40,  36,  13,   1],\n",
              "       [  0,  21,  29,  72, 115, 141, 131, 141,  78,  37,   3],\n",
              "       [  0,  16,  41,  73, 153, 197, 232, 160, 130,  48,   6],\n",
              "       [  0,   5,  18,  30,  60,  89, 112, 115,  61,  27,   3],\n",
              "       [  0,  74,  92, 229, 354, 501, 606, 544, 376, 157,  16],\n",
              "       [  0,   9,  27,  34,  98, 148, 235, 203, 159,  56,   3],\n",
              "       [  0,  20,  53, 105, 240, 326, 454, 473, 310, 110,  12],\n",
              "       [  0,  20,  48, 113, 218, 373, 521, 514, 396, 164,  19],\n",
              "       [  0,  10,  35,  67,  99, 239, 361, 414, 379, 155,  10],\n",
              "       [  0,  59, 138, 228, 392, 607, 810, 746, 604, 306,  33]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## MLP #############\n",
            "Note: MAE = 0.876, Accuracy = 0.362, Customized Accuracy = 0.832\n",
            "Utilité: MAE = 2.986, Accuracy = 0.089, Customized Accuracy = 0.487, Binary Thresholding Score = 0.598\n",
            "Les deux: Accuracy = 0.032, Customized Accuracy = 0.404\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[1526, 1818,  816,  405,   21],\n",
              "       [ 340, 1217, 1004,  482,   15],\n",
              "       [ 119,  773, 1316, 1045,   53],\n",
              "       [  42,  288, 1102, 2219,  352],\n",
              "       [  28,  298,  946, 3391, 1300]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,  399,  291,  359,  397,  718,  926,  791,  213,   34,   18],\n",
              "       [   0,   23,   20,   28,   36,   65,   81,   58,   11,    1,    1],\n",
              "       [   0,   33,   29,   59,   76,  168,  213,  146,   38,    5,    1],\n",
              "       [   0,   26,   52,   62,  109,  199,  306,  244,   50,    6,    2],\n",
              "       [   0,   13,   21,   24,   29,   95,  172,  133,   29,    4,    0],\n",
              "       [   0,  102,  114,  167,  255,  469,  831,  805,  180,   22,    4],\n",
              "       [   0,   13,   18,   33,   85,  135,  294,  310,   82,    2,    0],\n",
              "       [   0,   60,   61,   87,  127,  307,  561,  694,  192,   13,    1],\n",
              "       [   0,   39,   38,   87,  147,  282,  671,  876,  234,   10,    2],\n",
              "       [   0,   16,   27,   42,   57,  148,  438,  761,  256,   22,    2],\n",
              "       [   0,  159,  154,  209,  273,  486, 1008, 1228,  369,   29,    8]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Amélioration du MLP par rapport au précédent."
      ],
      "metadata": {
        "id": "NZ7LKmgxFWk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C. Utilisation du commentaire et du titre du produit"
      ],
      "metadata": {
        "id": "F_CPtmt-cBh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I. Moyenne des embeddings"
      ],
      "metadata": {
        "id": "cOEmcANlcFd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size=model_google_vec_neg_300.vector_size\n",
        "\n",
        "reviews_train_wv_google=word2vec_generator(reviews_train_tokens,model_google_vec_neg_300,vector_size)\n",
        "reviews_test_wv_google=word2vec_generator(reviews_test_tokens,model_google_vec_neg_300,vector_size)\n",
        "\n",
        "title_train_wv_google=word2vec_generator(title_train_tokens,model_google_vec_neg_300,vector_size)\n",
        "title_test_wv_google=word2vec_generator(title_test_tokens,model_google_vec_neg_300,vector_size)\n",
        "\n",
        "# renommage des colonnes de titles...\n",
        "title_train_wv_google.columns = [\"t\" + str(c) for c in title_train_wv_google.columns]\n",
        "title_test_wv_google.columns = [\"t\" + str(c) for c in title_test_wv_google.columns]"
      ],
      "metadata": {
        "id": "XdsfP038xCZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rev_a_title_train_wv_google = pd.concat([reviews_train_wv_google, title_train_wv_google], axis=1)\n",
        "rev_a_title_test_wv_google = pd.concat([reviews_test_wv_google, title_test_wv_google], axis=1)\n",
        "\n",
        "rev_a_title_train_wv_google.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "aEl8_EsqxFzA",
        "outputId": "6497a946-60fb-4c2a-ea1d-1b171e3f4979"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3         4         5         6  \\\n",
              "0 -0.002900  0.044868  0.006784  0.074842 -0.102040  0.020503  0.050016   \n",
              "1  0.088189  0.007243  0.091960  0.115451  0.055745  0.077894  0.029948   \n",
              "2  0.043005  0.080643 -0.008636  0.118901 -0.073688 -0.004681 -0.044107   \n",
              "\n",
              "          7         8         9        10        11        12        13  \\\n",
              "0 -0.074015  0.055866  0.099108 -0.051928 -0.119348 -0.016559  0.020685   \n",
              "1  0.123128  0.025262  0.071333 -0.032091 -0.036662 -0.032471  0.015869   \n",
              "2 -0.062599 -0.042023  0.148212 -0.116117 -0.137677 -0.033790 -0.012560   \n",
              "\n",
              "         14        15        16        17        18        19        20  \\\n",
              "0 -0.092850  0.080830  0.025311  0.084151  0.000409 -0.052561  0.004491   \n",
              "1 -0.173570  0.156345  0.020345  0.013679  0.100316 -0.107720  0.081896   \n",
              "2 -0.103225  0.168814  0.104610  0.053401  0.050854 -0.094546 -0.066885   \n",
              "\n",
              "         21        22        23        24        25        26        27  \\\n",
              "0  0.021908  0.052043  0.023320  0.002230 -0.010714 -0.032790  0.056665   \n",
              "1 -0.051080  0.146905 -0.009928  0.041124  0.041829 -0.162733  0.026671   \n",
              "2  0.037579  0.166987 -0.031743  0.044471 -0.005512  0.011256  0.014382   \n",
              "\n",
              "         28        29        30        31        32        33        34  \\\n",
              "0  0.015640 -0.052884 -0.026823  0.003278  0.002231 -0.074988  0.006963   \n",
              "1  0.042638 -0.090773 -0.083252  0.021064 -0.049778  0.002726 -0.023702   \n",
              "2  0.026179  0.030738 -0.015289  0.043077 -0.023405 -0.104455 -0.036922   \n",
              "\n",
              "         35        36        37        38        39        40        41  \\\n",
              "0 -0.052575  0.012789 -0.016799  0.009686  0.029267  0.069609 -0.065814   \n",
              "1  0.010422  0.062147  0.020616  0.104736  0.098009  0.094523 -0.028428   \n",
              "2  0.011700  0.032654  0.012219  0.040889  0.148212  0.111298 -0.147668   \n",
              "\n",
              "         42        43        44        45        46        47        48  \\\n",
              "0  0.124028 -0.042836 -0.058742 -0.068833 -0.031450  0.005493  0.023426   \n",
              "1  0.071235 -0.000644 -0.053440 -0.078857  0.009440 -0.148098  0.000678   \n",
              "2  0.147837 -0.006129 -0.021409 -0.090104 -0.050051  0.003947 -0.124154   \n",
              "\n",
              "         49        50        51        52        53        54        55  \\\n",
              "0  0.019639  0.000967 -0.004218  0.007726 -0.028178  0.038489 -0.003355   \n",
              "1 -0.025160 -0.076857  0.082316  0.076850 -0.025648 -0.000427 -0.024943   \n",
              "2  0.063026 -0.070064  0.065117  0.075721  0.072885 -0.016714  0.014907   \n",
              "\n",
              "         56        57        58        59        60        61        62  \\\n",
              "0 -0.008886 -0.075110  0.016318 -0.051416 -0.022196  0.044364 -0.096261   \n",
              "1 -0.070236 -0.039971  0.045383 -0.059306  0.059955 -0.036699 -0.088555   \n",
              "2 -0.034901 -0.029010  0.147529 -0.138851 -0.016465  0.125352 -0.054331   \n",
              "\n",
              "         63        64        65        66        67        68        69  \\\n",
              "0 -0.030301  0.026029 -0.037417 -0.031374  0.060877 -0.031473  0.085079   \n",
              "1 -0.090515  0.018077 -0.005819 -0.165371  0.106106 -0.090712  0.024540   \n",
              "2  0.015553 -0.029710 -0.074407 -0.053964  0.032507 -0.048323  0.136024   \n",
              "\n",
              "         70        71        72        73        74        75        76  \\\n",
              "0  0.040173  0.011696  0.057092 -0.027526 -0.138667 -0.075401  0.046112   \n",
              "1  0.114041  0.063192  0.141602 -0.020124 -0.158393 -0.080902  0.029217   \n",
              "2  0.086731  0.045422  0.112488 -0.074895 -0.175565 -0.083909  0.031743   \n",
              "\n",
              "         77        78        79        80        81        82        83  \\\n",
              "0  0.053008  0.020549  0.050109  0.013487 -0.042455  0.023044 -0.003710   \n",
              "1  0.126831 -0.054226  0.048733 -0.120768 -0.070357 -0.056939  0.054755   \n",
              "2  0.075970  0.139123  0.112333 -0.012921 -0.015076  0.106023 -0.015627   \n",
              "\n",
              "         84        85        86        87        88        89        90  \\\n",
              "0 -0.010401 -0.020149 -0.040324  0.155952  0.052127  0.030129  0.015279   \n",
              "1 -0.079047 -0.013960  0.009250  0.108541 -0.018390  0.055203  0.018053   \n",
              "2 -0.035288  0.027006 -0.076641  0.162177 -0.032856  0.012010  0.109571   \n",
              "\n",
              "         91        92        93        94        95        96        97  \\\n",
              "0  0.052953 -0.046857 -0.059532 -0.039087 -0.065372  0.030723  0.017287   \n",
              "1  0.041463 -0.055366 -0.059380 -0.035563 -0.087375  0.048774  0.025336   \n",
              "2  0.032290 -0.092302 -0.001883 -0.030702 -0.053429  0.075749  0.064798   \n",
              "\n",
              "         98        99       100       101       102       103       104  \\\n",
              "0  0.055403 -0.021920 -0.035279 -0.049337  0.020826  0.003350 -0.022710   \n",
              "1  0.036654 -0.128845 -0.159532 -0.011040  0.007541  0.044406 -0.052097   \n",
              "2  0.024825 -0.078530 -0.073106  0.014198  0.073829  0.036020  0.044260   \n",
              "\n",
              "        105       106       107       108       109       110       111  \\\n",
              "0 -0.081744 -0.005923 -0.009235  0.019858 -0.083950 -0.062847 -0.085897   \n",
              "1 -0.063707 -0.018809  0.011041  0.024272 -0.115682 -0.093608  0.041821   \n",
              "2 -0.055021  0.017936  0.029503  0.033809 -0.105929 -0.066378  0.035889   \n",
              "\n",
              "        112       113       114       115       116       117       118  \\\n",
              "0 -0.008505 -0.015938  0.051282  0.033426  0.035956 -0.023264  0.025825   \n",
              "1  0.011315 -0.015245  0.106283 -0.003045  0.026747 -0.081940  0.001587   \n",
              "2  0.045113 -0.006066  0.203942 -0.041438 -0.035950 -0.041255 -0.100652   \n",
              "\n",
              "        119       120       121       122       123       124       125  \\\n",
              "0  0.023855 -0.057292 -0.003651 -0.070389  0.046999  0.015469  0.007323   \n",
              "1  0.032471 -0.006375 -0.035712 -0.098985  0.056286  0.048313  0.069048   \n",
              "2  0.056502 -0.059148  0.019005 -0.048950  0.081881 -0.092745 -0.017757   \n",
              "\n",
              "        126       127       128       129       130       131       132  \\\n",
              "0 -0.039538 -0.058791  0.057725  0.033074 -0.055515 -0.012679 -0.080372   \n",
              "1 -0.108344  0.015734 -0.016466  0.035224 -0.154012 -0.037082  0.001926   \n",
              "2 -0.078012 -0.114848  0.044063  0.009083 -0.190080  0.030019 -0.025480   \n",
              "\n",
              "        133       134       135       136       137       138       139  \\\n",
              "0  0.009174 -0.017444  0.025905  0.044887  0.009169  0.002169  0.073048   \n",
              "1 -0.003788  0.037543 -0.000081 -0.063341  0.036931  0.139506  0.049364   \n",
              "2  0.025395 -0.028757 -0.041654  0.034903  0.006423  0.063589  0.078501   \n",
              "\n",
              "        140       141       142       143       144       145       146  \\\n",
              "0  0.057993 -0.088614 -0.030991 -0.003501  0.008017  0.036120 -0.020569   \n",
              "1  0.073107 -0.073931 -0.018853  0.004435  0.024431 -0.064128 -0.012729   \n",
              "2  0.048037 -0.107132 -0.051101 -0.058331  0.043679  0.001258 -0.023165   \n",
              "\n",
              "        147       148       149       150       151      152       153  \\\n",
              "0 -0.031011 -0.071039 -0.043187  0.089305 -0.029168 -0.05180  0.088863   \n",
              "1 -0.050157 -0.081604 -0.089179  0.075236 -0.038113  0.02592  0.024251   \n",
              "2 -0.077486 -0.109290 -0.106445  0.074270  0.055007 -0.08729  0.031982   \n",
              "\n",
              "        154       155       156       157       158       159       160  \\\n",
              "0 -0.029610 -0.004764 -0.061786 -0.049806 -0.075134 -0.019008 -0.028310   \n",
              "1  0.008892 -0.063938 -0.103651 -0.000882 -0.147678 -0.062283  0.058755   \n",
              "2 -0.050725  0.028879 -0.066673 -0.089524 -0.135968 -0.046869  0.021794   \n",
              "\n",
              "        161       162       163       164       165       166       167  \\\n",
              "0  0.051731  0.003766  0.011950  0.015680 -0.065050  0.068574 -0.075329   \n",
              "1  0.024222 -0.022732  0.054565  0.031067 -0.192247  0.135914 -0.055759   \n",
              "2  0.039537 -0.057767  0.042302  0.170560 -0.158105  0.064904 -0.048697   \n",
              "\n",
              "        168       169       170       171       172       173       174  \\\n",
              "0 -0.001712 -0.063603 -0.127667 -0.050622 -0.050118 -0.036819 -0.061596   \n",
              "1 -0.048231  0.050334 -0.034105 -0.070502 -0.079102 -0.086263 -0.033569   \n",
              "2  0.033945  0.061843 -0.113262  0.036602 -0.092680 -0.114246 -0.168442   \n",
              "\n",
              "        175       176       177       178       179       180       181  \\\n",
              "0 -0.042022  0.124270 -0.081843 -0.014566  0.034287 -0.095007 -0.067428   \n",
              "1  0.015654  0.104824 -0.118137 -0.014459 -0.088637 -0.166287 -0.079264   \n",
              "2  0.039652  0.162274 -0.128981 -0.000559  0.005426 -0.015400 -0.026832   \n",
              "\n",
              "        182       183       184       185       186       187       188  \\\n",
              "0  0.051999  0.006993 -0.026237 -0.021041 -0.030848  0.000314  0.041433   \n",
              "1  0.025662 -0.048726 -0.107530  0.027649 -0.039341 -0.104696  0.084323   \n",
              "2  0.091830 -0.010052 -0.053317 -0.091036 -0.040952  0.012902  0.042415   \n",
              "\n",
              "        189       190       191       192       193       194       195  \\\n",
              "0  0.034017  0.033520 -0.000682 -0.021132  0.004080 -0.007918  0.044946   \n",
              "1  0.087362  0.067410 -0.009169  0.035862 -0.016137 -0.099555  0.168660   \n",
              "2  0.113347 -0.046035 -0.010019  0.014141  0.032536  0.066636  0.083890   \n",
              "\n",
              "        196       197       198       199       200       201       202  \\\n",
              "0  0.003345 -0.040761 -0.099482 -0.108041  0.019720  0.047250 -0.056443   \n",
              "1  0.015625  0.010123 -0.016927 -0.088026 -0.014004  0.048435 -0.102322   \n",
              "2 -0.027381  0.045119 -0.077209 -0.010075  0.073702  0.041072 -0.184284   \n",
              "\n",
              "        203       204       205       206       207       208       209  \\\n",
              "0  0.004647  0.056614 -0.030238 -0.018902 -0.030592  0.018074 -0.024106   \n",
              "1 -0.095350  0.042611  0.020806 -0.204029 -0.005995 -0.033474  0.072361   \n",
              "2 -0.052185  0.009926 -0.037278 -0.025531  0.058959  0.009179  0.040227   \n",
              "\n",
              "        210       211       212       213       214       215       216  \\\n",
              "0 -0.017929  0.056056 -0.072432  0.067731 -0.079447  0.030726  0.042337   \n",
              "1  0.045763  0.079590 -0.020847 -0.043789 -0.088976  0.052572  0.167426   \n",
              "2 -0.064819  0.062933 -0.122352  0.019409 -0.031184  0.105999  0.099628   \n",
              "\n",
              "        217       218       219       220       221       222       223  \\\n",
              "0  0.009965 -0.089965  0.038176 -0.024134  0.006570  0.001954  0.003252   \n",
              "1 -0.003418 -0.101766 -0.042155  0.002700  0.010810  0.028130 -0.028756   \n",
              "2  0.005991 -0.067148 -0.031210 -0.126131  0.008939 -0.035856  0.001810   \n",
              "\n",
              "        224       225       226       227       228       229       230  \\\n",
              "0  0.050383 -0.029478  0.029882  0.036708  0.053066 -0.000418  0.015018   \n",
              "1  0.113525  0.004313  0.040853  0.029036  0.010722 -0.091092  0.100193   \n",
              "2  0.108070 -0.000657  0.015057  0.009775  0.001850  0.138165  0.029924   \n",
              "\n",
              "        231       232       233       234       235       236       237  \\\n",
              "0 -0.003766  0.021896 -0.020837  0.019659 -0.021408 -0.008348 -0.004686   \n",
              "1  0.027866 -0.001383  0.084839  0.055108 -0.004367  0.001600  0.050103   \n",
              "2  0.066923  0.016872  0.059786  0.060115  0.111214 -0.113790 -0.052147   \n",
              "\n",
              "        238       239       240       241       242       243       244  \\\n",
              "0  0.070430 -0.005205  0.056064 -0.000275 -0.024886 -0.114259  0.023371   \n",
              "1  0.041341  0.002348 -0.033411  0.013624  0.009910 -0.121636 -0.035078   \n",
              "2  0.109937  0.007986  0.197378 -0.021268 -0.004765  0.035345 -0.013240   \n",
              "\n",
              "        245       246       247       248       249       250       251  \\\n",
              "0 -0.028838 -0.029380  0.035168  0.021669 -0.065137 -0.008155  0.004890   \n",
              "1  0.046644 -0.006586  0.068244 -0.047831  0.003977  0.019701  0.033976   \n",
              "2  0.022134  0.034292 -0.005920 -0.023910 -0.041485 -0.066064  0.063242   \n",
              "\n",
              "        252       253       254       255       256       257       258  \\\n",
              "0  0.051684  0.016112  0.020894 -0.070980  0.004604  0.001860 -0.044157   \n",
              "1 -0.007785  0.115845  0.013068 -0.099718 -0.084975 -0.003499 -0.083266   \n",
              "2  0.147987  0.088125  0.090971 -0.162335  0.030649  0.023902 -0.030180   \n",
              "\n",
              "        259       260       261       262       263       264       265  \\\n",
              "0 -0.061054  0.020566  0.006596 -0.074984  0.005134  0.037322  0.043370   \n",
              "1  0.000732 -0.097168  0.020101 -0.108222  0.006131 -0.065145  0.106622   \n",
              "2 -0.109826 -0.007136  0.010615 -0.130222  0.060246  0.018799  0.041171   \n",
              "\n",
              "        266       267       268       269       270       271       272  \\\n",
              "0 -0.060567 -0.057459 -0.031847  0.012542  0.015441  0.077615  0.062218   \n",
              "1 -0.011868  0.007395  0.011353 -0.112196  0.030952  0.154975  0.161947   \n",
              "2 -0.042978 -0.100417  0.002826 -0.022114  0.091905  0.104999  0.053176   \n",
              "\n",
              "        273       274       275       276       277       278       279  \\\n",
              "0  0.025076  0.090393 -0.063815 -0.084251 -0.105852 -0.039521  0.019388   \n",
              "1  0.067444  0.139296 -0.049983 -0.045176 -0.134861 -0.026910  0.028887   \n",
              "2  0.103732  0.007597 -0.075083 -0.075533 -0.038828 -0.067167 -0.031973   \n",
              "\n",
              "        280       281       282       283       284       285       286  \\\n",
              "0 -0.009751  0.003045  0.068890  0.048047 -0.024688 -0.012525 -0.082527   \n",
              "1  0.027313  0.029446  0.051520  0.112657  0.126980  0.008865 -0.073242   \n",
              "2 -0.017768 -0.016386  0.076248  0.097592 -0.021288 -0.032921 -0.080318   \n",
              "\n",
              "        287       288       289       290       291       292       293  \\\n",
              "0  0.024600 -0.002151  0.063706 -0.051720  0.060278 -0.099974  0.049301   \n",
              "1  0.019864 -0.032162  0.135525 -0.020562  0.030989 -0.087918 -0.067471   \n",
              "2 -0.045194 -0.029705  0.070133 -0.042634  0.110220 -0.058350 -0.008043   \n",
              "\n",
              "        294       295       296       297       298       299        t0  \\\n",
              "0 -0.063635 -0.036664  0.010074  0.010265 -0.036405 -0.023421 -0.018311   \n",
              "1 -0.021484 -0.055718 -0.062086 -0.081441 -0.021539  0.057465  0.089098   \n",
              "2 -0.026741 -0.097187  0.008345 -0.056997  0.042084  0.002141  0.113180   \n",
              "\n",
              "         t1        t2        t3        t4        t5        t6        t7  \\\n",
              "0  0.038574  0.121716  0.000391 -0.012106  0.092407  0.001611 -0.098041   \n",
              "1  0.020556  0.023010  0.100045 -0.119978  0.056863  0.058434 -0.004328   \n",
              "2  0.039530  0.011556  0.017370 -0.044596  0.148844  0.005778 -0.109985   \n",
              "\n",
              "         t8        t9       t10       t11       t12       t13       t14  \\\n",
              "0 -0.056152  0.056384 -0.119556 -0.098828 -0.129980  0.148145 -0.032080   \n",
              "1  0.027222  0.072852 -0.029977 -0.114386 -0.047398  0.012390 -0.087734   \n",
              "2 -0.055013  0.192383 -0.185018 -0.281576  0.035360  0.036621 -0.051921   \n",
              "\n",
              "        t15       t16       t17       t18       t19       t20       t21  \\\n",
              "0  0.192139  0.074927  0.231836 -0.143042 -0.218604 -0.009082  0.102295   \n",
              "1  0.115348  0.073425  0.091221  0.132725 -0.111607  0.062212  0.044120   \n",
              "2  0.202962 -0.077555  0.138753 -0.000488 -0.112467  0.008789  0.086548   \n",
              "\n",
              "        t22       t23       t24       t25       t26       t27       t28  \\\n",
              "0  0.111035  0.101440 -0.204883 -0.223926 -0.151953  0.335547 -0.101514   \n",
              "1  0.154942 -0.016061  0.081545  0.030361 -0.075835  0.027137  0.066728   \n",
              "2 -0.017537 -0.016576  0.167806  0.097534  0.084310 -0.014735 -0.010167   \n",
              "\n",
              "        t29       t30       t31       t32       t33       t34       t35  \\\n",
              "0 -0.061676  0.032129  0.024976 -0.049664  0.012402 -0.121875 -0.144238   \n",
              "1 -0.047751 -0.061857  0.043405 -0.015472 -0.070090  0.016836 -0.041478   \n",
              "2 -0.250326 -0.128662  0.031006 -0.015706 -0.000529 -0.028310 -0.072472   \n",
              "\n",
              "        t36       t37       t38       t39       t40       t41       t42  \\\n",
              "0 -0.005859  0.233203  0.004871  0.171411  0.147290 -0.181250  0.196094   \n",
              "1  0.088283 -0.014659  0.033763  0.104623  0.050111 -0.158273  0.168069   \n",
              "2  0.084961  0.069336 -0.080404  0.139648 -0.058757  0.031779  0.197917   \n",
              "\n",
              "        t43       t44       t45       t46       t47       t48       t49  \\\n",
              "0  0.158301  0.026025 -0.138916 -0.135449 -0.069141 -0.021948  0.072559   \n",
              "1 -0.011043 -0.067073 -0.068355 -0.078875 -0.049966  0.009609  0.017397   \n",
              "2 -0.164307 -0.165527 -0.009644 -0.050802 -0.015483  0.161458  0.056152   \n",
              "\n",
              "        t50       t51       t52       t53       t54       t55       t56  \\\n",
              "0 -0.233643  0.212158 -0.090771 -0.026465 -0.030566  0.163647 -0.025781   \n",
              "1 -0.025532  0.145974  0.042249 -0.004987  0.053079 -0.034886 -0.065368   \n",
              "2  0.212321  0.009603 -0.005981 -0.134684  0.041829 -0.030558  0.065135   \n",
              "\n",
              "        t57       t58       t59       t60       t61       t62       t63  \\\n",
              "0 -0.037091  0.003735 -0.195801 -0.153857 -0.039685 -0.173242  0.001550   \n",
              "1 -0.044560  0.034807 -0.034746 -0.021083  0.066177 -0.160226 -0.019152   \n",
              "2  0.083496 -0.033122 -0.150391 -0.036784  0.060628 -0.059448 -0.286133   \n",
              "\n",
              "        t64       t65       t66       t67       t68       t69       t70  \\\n",
              "0 -0.065625 -0.131299 -0.033105 -0.130859  0.116309  0.256836  0.237109   \n",
              "1 -0.009565 -0.065063 -0.076922  0.024283 -0.027926  0.057822  0.098232   \n",
              "2  0.100342  0.118652 -0.292969  0.177246  0.003947  0.078939  0.148112   \n",
              "\n",
              "        t71       t72       t73       t74       t75       t76       t77  \\\n",
              "0  0.105762 -0.007764  0.015723 -0.257715 -0.049255  0.016803 -0.009851   \n",
              "1  0.092233  0.115104  0.006280 -0.191272 -0.116340  0.045394  0.142866   \n",
              "2  0.104899  0.060750 -0.125783 -0.227214 -0.117188  0.101807 -0.015462   \n",
              "\n",
              "        t78       t79       t80       t81       t82       t83       t84  \\\n",
              "0  0.006238  0.002979  0.001514  0.138770 -0.046753  0.007715  0.233887   \n",
              "1 -0.005694  0.059736 -0.162463 -0.028331  0.067810  0.009018  0.005733   \n",
              "2  0.076742 -0.098429 -0.005371 -0.196940  0.052409  0.087565  0.036336   \n",
              "\n",
              "        t85       t86       t87       t88       t89       t90       t91  \\\n",
              "0  0.075586 -0.094824  0.279102  0.046021 -0.042554 -0.060352 -0.197070   \n",
              "1 -0.013283 -0.002862  0.133615  0.083270  0.042997  0.038459  0.070086   \n",
              "2  0.011210 -0.131632  0.231771 -0.025391  0.123372 -0.007812  0.032552   \n",
              "\n",
              "        t92       t93       t94       t95       t96       t97       t98  \\\n",
              "0 -0.147217  0.130774 -0.059113  0.082520 -0.091309 -0.084106  0.189355   \n",
              "1 -0.147581 -0.053349 -0.006366 -0.037900  0.084333  0.032506  0.063561   \n",
              "2  0.036051 -0.040873 -0.121901 -0.109619 -0.010986 -0.132243  0.078776   \n",
              "\n",
              "        t99      t100      t101      t102      t103      t104      t105  \\\n",
              "0 -0.073340 -0.017419  0.200391  0.139050 -0.084863 -0.016846  0.232520   \n",
              "1 -0.008754 -0.121865 -0.014938  0.053807  0.026393 -0.041260 -0.054025   \n",
              "2  0.108236 -0.183594 -0.059082 -0.036458 -0.046834 -0.179036 -0.066406   \n",
              "\n",
              "       t106      t107      t108      t109      t110      t111      t112  \\\n",
              "0  0.039453  0.170703 -0.008691  0.047656 -0.044922 -0.179395 -0.052905   \n",
              "1 -0.063640  0.021460  0.079760 -0.086953 -0.154681 -0.101035 -0.036558   \n",
              "2  0.287435 -0.077530 -0.069743  0.084635 -0.094401  0.005371  0.008952   \n",
              "\n",
              "       t113      t114      t115      t116      t117      t118      t119  \\\n",
              "0 -0.038672 -0.020894  0.065918 -0.158105  0.004199 -0.000903 -0.084277   \n",
              "1 -0.014840  0.112305 -0.032194  0.031673 -0.106628 -0.014512  0.003254   \n",
              "2 -0.002523  0.162109  0.004069  0.034749  0.067301 -0.070964  0.266276   \n",
              "\n",
              "       t120      t121      t122      t123      t124      t125      t126  \\\n",
              "0  0.106775 -0.036182 -0.081836 -0.004175  0.145313 -0.229297 -0.151050   \n",
              "1 -0.111160  0.029419 -0.061432  0.088517  0.030186 -0.066302 -0.156930   \n",
              "2  0.059570  0.169271 -0.036947  0.031087 -0.004395  0.021077 -0.034831   \n",
              "\n",
              "       t127      t128      t129      t130      t131      t132      t133  \\\n",
              "0 -0.013974  0.040918  0.031787 -0.019385  0.144043  0.094971  0.040332   \n",
              "1 -0.023039 -0.009102 -0.091675 -0.125000 -0.143381 -0.070613  0.033029   \n",
              "2 -0.013590  0.318522  0.002340 -0.027507  0.023763 -0.114583  0.016032   \n",
              "\n",
              "       t134      t135      t136      t137      t138      t139      t140  \\\n",
              "0 -0.107324 -0.010547 -0.043555  0.088623  0.073547  0.126917  0.093652   \n",
              "1  0.037371 -0.064501 -0.002289  0.064309  0.024748  0.088841  0.032076   \n",
              "2 -0.018473  0.028117 -0.097036  0.104085  0.100830  0.004557  0.036947   \n",
              "\n",
              "       t141      t142      t143      t144      t145      t146      t147  \\\n",
              "0 -0.094879  0.056982 -0.063704 -0.065112  0.123389 -0.028906  0.081293   \n",
              "1 -0.101009  0.005336 -0.038583 -0.004377 -0.019039 -0.121286 -0.114184   \n",
              "2 -0.073242  0.021566 -0.020345 -0.042928  0.025635  0.100749  0.104655   \n",
              "\n",
              "       t148      t149      t150      t151      t152      t153      t154  \\\n",
              "0  0.110910 -0.190332  0.332300 -0.123853 -0.124072 -0.055713 -0.096875   \n",
              "1 -0.048846 -0.015703  0.132725  0.052883 -0.113735  0.062081 -0.018232   \n",
              "2  0.051270 -0.124552  0.094828 -0.175130 -0.071289  0.093852 -0.017577   \n",
              "\n",
              "       t155      t156      t157      t158      t159      t160      t161  \\\n",
              "0 -0.046167 -0.051624  0.174609  0.001440 -0.149463 -0.011218  0.107031   \n",
              "1  0.014747 -0.099243 -0.098328 -0.068848 -0.016946 -0.073147  0.112277   \n",
              "2 -0.076131 -0.134847 -0.106527 -0.150309  0.059814  0.095622  0.013957   \n",
              "\n",
              "       t162      t163      t164      t165      t166      t167      t168  \\\n",
              "0  0.096777 -0.011792  0.026862 -0.149219  0.103565 -0.069531 -0.148633   \n",
              "1 -0.026016  0.056745  0.016065 -0.188703  0.028740 -0.128270  0.010881   \n",
              "2 -0.029419  0.024495  0.012858 -0.055339  0.105306 -0.168376  0.017904   \n",
              "\n",
              "       t169      t170      t171      t172      t173      t174      t175  \\\n",
              "0  0.001855 -0.157715  0.024902  0.118164 -0.141992 -0.140918  0.073047   \n",
              "1 -0.028346 -0.119001 -0.044338 -0.074533 -0.013338 -0.109183 -0.035910   \n",
              "2  0.119273 -0.107015  0.077230 -0.076497  0.011556 -0.031576  0.034218   \n",
              "\n",
              "       t176      t177      t178      t179      t180      t181      t182  \\\n",
              "0  0.191187 -0.059521 -0.030298 -0.061230 -0.043359  0.017639  0.179736   \n",
              "1  0.150133 -0.106437 -0.014971  0.021593 -0.046334 -0.052717  0.050182   \n",
              "2  0.094076 -0.037191 -0.110189  0.125326 -0.096354 -0.111328 -0.090078   \n",
              "\n",
              "       t183      t184      t185      t186      t187      t188      t189  \\\n",
              "0  0.011450 -0.043506  0.009692  0.048633 -0.003125  0.025342  0.096143   \n",
              "1 -0.023215 -0.017840  0.031839 -0.037521  0.032619  0.115670  0.044604   \n",
              "2  0.041748 -0.097819 -0.074056  0.020996 -0.053996 -0.013509  0.075684   \n",
              "\n",
              "       t190      t191      t192      t193      t194      t195      t196  \\\n",
              "0  0.081689  0.091396 -0.214746  0.043872  0.192725  0.038037 -0.067609   \n",
              "1 -0.000645  0.061070  0.033578  0.048179 -0.005371 -0.021650  0.020839   \n",
              "2 -0.029704  0.070638  0.033529  0.078430 -0.060221 -0.080119 -0.098307   \n",
              "\n",
              "       t197      t198      t199      t200      t201      t202      t203  \\\n",
              "0  0.023694  0.153564 -0.152832  0.041772  0.069055  0.009375 -0.080859   \n",
              "1 -0.028008 -0.074624 -0.046430 -0.054559  0.084636 -0.074289  0.000392   \n",
              "2 -0.120443 -0.279948  0.012451  0.027098  0.236979 -0.106567 -0.060465   \n",
              "\n",
              "       t204      t205      t206      t207      t208      t209      t210  \\\n",
              "0  0.004895 -0.112939  0.086719 -0.022949 -0.117310  0.073291  0.041946   \n",
              "1 -0.026789  0.026071 -0.151986 -0.061125  0.027911  0.057251  0.029458   \n",
              "2  0.001221  0.060628 -0.071940 -0.007487  0.128459 -0.131673 -0.094482   \n",
              "\n",
              "       t211      t212      t213      t214      t215      t216      t217  \\\n",
              "0 -0.075122 -0.047607  0.081763  0.016992  0.071423 -0.040039 -0.111469   \n",
              "1  0.080994 -0.067383  0.027518 -0.098894  0.038565  0.073661  0.051880   \n",
              "2  0.042969 -0.081706  0.113281  0.231608  0.115519 -0.002604  0.076335   \n",
              "\n",
              "       t218      t219      t220      t221      t222      t223      t224  \\\n",
              "0  0.017505 -0.040503 -0.161328  0.062500  0.045410  0.177759  0.021155   \n",
              "1 -0.100534 -0.013027 -0.042953 -0.065674  0.002171 -0.030077  0.086129   \n",
              "2 -0.098633  0.107747 -0.080811  0.138041 -0.088867  0.149943 -0.010905   \n",
              "\n",
              "       t225      t226      t227      t228      t229      t230      t231  \\\n",
              "0  0.006104  0.024414 -0.000720 -0.013330 -0.051074  0.145825  0.013062   \n",
              "1  0.007517  0.089547  0.057080 -0.042071 -0.077942  0.046330 -0.033794   \n",
              "2 -0.085286  0.003571 -0.023254 -0.059326  0.008138 -0.047404 -0.026265   \n",
              "\n",
              "       t232      t233      t234      t235      t236      t237      t238  \\\n",
              "0 -0.097949 -0.024988 -0.004395  0.058643 -0.023523 -0.058545  0.059229   \n",
              "1 -0.004046  0.021270  0.103446  0.016462  0.005774 -0.004153  0.133545   \n",
              "2  0.025690 -0.027140 -0.081476  0.021342  0.029785  0.120443  0.075928   \n",
              "\n",
              "       t239      t240      t241      t242      t243      t244      t245  \\\n",
              "0  0.010107  0.025269  0.063330  0.080859  0.056567  0.065137  0.177997   \n",
              "1  0.008981  0.096976  0.015747  0.015418 -0.175354 -0.028327 -0.001452   \n",
              "2  0.002767  0.163737 -0.038086  0.090332 -0.117602  0.018311 -0.095133   \n",
              "\n",
              "       t246      t247      t248      t249      t250      t251      t252  \\\n",
              "0 -0.204248  0.153624 -0.169824 -0.094043  0.029498  0.047485  0.125342   \n",
              "1 -0.068238  0.075274 -0.074908 -0.075248 -0.022600 -0.001022  0.070151   \n",
              "2 -0.176758 -0.017741 -0.084595 -0.145833  0.009766  0.052043  0.131673   \n",
              "\n",
              "       t253      t254      t255      t256      t257      t258      t259  \\\n",
              "0  0.013672  0.021429  0.040906  0.085815  0.042188  0.096387 -0.224023   \n",
              "1  0.122295  0.086075 -0.040690  0.060420 -0.035948 -0.070112 -0.035906   \n",
              "2  0.049764  0.131673 -0.254720 -0.027100  0.117106  0.036296  0.013835   \n",
              "\n",
              "       t260      t261      t262      t263      t264      t265      t266  \\\n",
              "0  0.144287  0.064551  0.114062  0.007031  0.069580  0.163733  0.091336   \n",
              "1 -0.040187  0.027570 -0.125907  0.046112 -0.029081  0.062727 -0.035854   \n",
              "2  0.087402  0.067098 -0.161092  0.000814  0.129415 -0.006022 -0.108398   \n",
              "\n",
              "       t267      t268      t269      t270      t271      t272      t273  \\\n",
              "0  0.014941  0.072351  0.021094 -0.166406 -0.107458 -0.126450 -0.001663   \n",
              "1  0.041118 -0.057421  0.035324 -0.063969  0.213640  0.167280  0.070927   \n",
              "2  0.077311 -0.001719 -0.027588 -0.098470  0.118734 -0.090413  0.101725   \n",
              "\n",
              "       t274      t275      t276      t277      t278      t279      t280  \\\n",
              "0  0.094922  0.019629 -0.074170 -0.050815 -0.128979  0.259180 -0.103711   \n",
              "1  0.001700 -0.061162 -0.050343 -0.113591 -0.037903  0.031588  0.068987   \n",
              "2  0.167603 -0.149414 -0.102702 -0.027995 -0.094971  0.167358  0.140951   \n",
              "\n",
              "       t281      t282      t283      t284      t285      t286      t287  \\\n",
              "0  0.062769 -0.086444 -0.029028 -0.071680 -0.020801 -0.023938  0.084473   \n",
              "1 -0.052558  0.021000  0.135027 -0.034188  0.043516 -0.071230 -0.063962   \n",
              "2  0.171061  0.255534  0.073405 -0.000142 -0.036770  0.067301  0.091471   \n",
              "\n",
              "       t288      t289      t290      t291      t292      t293      t294  \\\n",
              "0  0.109802 -0.014111 -0.041663  0.020239 -0.049438  0.097559 -0.150610   \n",
              "1  0.013829  0.029641 -0.037490  0.087282 -0.195103  0.001875 -0.068643   \n",
              "2 -0.047038  0.131673 -0.112600  0.172119 -0.132894  0.080322  0.004364   \n",
              "\n",
              "       t295      t296      t297      t298      t299  \n",
              "0 -0.280664 -0.093506 -0.051709  0.004620  0.253906  \n",
              "1 -0.043719  0.059154 -0.059278 -0.062101  0.004259  \n",
              "2  0.045898  0.159180 -0.122965  0.052831 -0.096802  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "      <th>128</th>\n",
              "      <th>129</th>\n",
              "      <th>130</th>\n",
              "      <th>131</th>\n",
              "      <th>132</th>\n",
              "      <th>133</th>\n",
              "      <th>134</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "      <th>141</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>144</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "      <th>151</th>\n",
              "      <th>152</th>\n",
              "      <th>153</th>\n",
              "      <th>154</th>\n",
              "      <th>155</th>\n",
              "      <th>156</th>\n",
              "      <th>157</th>\n",
              "      <th>158</th>\n",
              "      <th>159</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "      <th>200</th>\n",
              "      <th>201</th>\n",
              "      <th>202</th>\n",
              "      <th>203</th>\n",
              "      <th>204</th>\n",
              "      <th>205</th>\n",
              "      <th>206</th>\n",
              "      <th>207</th>\n",
              "      <th>208</th>\n",
              "      <th>209</th>\n",
              "      <th>210</th>\n",
              "      <th>211</th>\n",
              "      <th>212</th>\n",
              "      <th>213</th>\n",
              "      <th>214</th>\n",
              "      <th>215</th>\n",
              "      <th>216</th>\n",
              "      <th>217</th>\n",
              "      <th>218</th>\n",
              "      <th>219</th>\n",
              "      <th>220</th>\n",
              "      <th>221</th>\n",
              "      <th>222</th>\n",
              "      <th>223</th>\n",
              "      <th>224</th>\n",
              "      <th>225</th>\n",
              "      <th>226</th>\n",
              "      <th>227</th>\n",
              "      <th>228</th>\n",
              "      <th>229</th>\n",
              "      <th>230</th>\n",
              "      <th>231</th>\n",
              "      <th>232</th>\n",
              "      <th>233</th>\n",
              "      <th>234</th>\n",
              "      <th>235</th>\n",
              "      <th>236</th>\n",
              "      <th>237</th>\n",
              "      <th>238</th>\n",
              "      <th>239</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "      <th>244</th>\n",
              "      <th>245</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "      <th>259</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "      <th>t0</th>\n",
              "      <th>t1</th>\n",
              "      <th>t2</th>\n",
              "      <th>t3</th>\n",
              "      <th>t4</th>\n",
              "      <th>t5</th>\n",
              "      <th>t6</th>\n",
              "      <th>t7</th>\n",
              "      <th>t8</th>\n",
              "      <th>t9</th>\n",
              "      <th>t10</th>\n",
              "      <th>t11</th>\n",
              "      <th>t12</th>\n",
              "      <th>t13</th>\n",
              "      <th>t14</th>\n",
              "      <th>t15</th>\n",
              "      <th>t16</th>\n",
              "      <th>t17</th>\n",
              "      <th>t18</th>\n",
              "      <th>t19</th>\n",
              "      <th>t20</th>\n",
              "      <th>t21</th>\n",
              "      <th>t22</th>\n",
              "      <th>t23</th>\n",
              "      <th>t24</th>\n",
              "      <th>t25</th>\n",
              "      <th>t26</th>\n",
              "      <th>t27</th>\n",
              "      <th>t28</th>\n",
              "      <th>t29</th>\n",
              "      <th>t30</th>\n",
              "      <th>t31</th>\n",
              "      <th>t32</th>\n",
              "      <th>t33</th>\n",
              "      <th>t34</th>\n",
              "      <th>t35</th>\n",
              "      <th>t36</th>\n",
              "      <th>t37</th>\n",
              "      <th>t38</th>\n",
              "      <th>t39</th>\n",
              "      <th>t40</th>\n",
              "      <th>t41</th>\n",
              "      <th>t42</th>\n",
              "      <th>t43</th>\n",
              "      <th>t44</th>\n",
              "      <th>t45</th>\n",
              "      <th>t46</th>\n",
              "      <th>t47</th>\n",
              "      <th>t48</th>\n",
              "      <th>t49</th>\n",
              "      <th>t50</th>\n",
              "      <th>t51</th>\n",
              "      <th>t52</th>\n",
              "      <th>t53</th>\n",
              "      <th>t54</th>\n",
              "      <th>t55</th>\n",
              "      <th>t56</th>\n",
              "      <th>t57</th>\n",
              "      <th>t58</th>\n",
              "      <th>t59</th>\n",
              "      <th>t60</th>\n",
              "      <th>t61</th>\n",
              "      <th>t62</th>\n",
              "      <th>t63</th>\n",
              "      <th>t64</th>\n",
              "      <th>t65</th>\n",
              "      <th>t66</th>\n",
              "      <th>t67</th>\n",
              "      <th>t68</th>\n",
              "      <th>t69</th>\n",
              "      <th>t70</th>\n",
              "      <th>t71</th>\n",
              "      <th>t72</th>\n",
              "      <th>t73</th>\n",
              "      <th>t74</th>\n",
              "      <th>t75</th>\n",
              "      <th>t76</th>\n",
              "      <th>t77</th>\n",
              "      <th>t78</th>\n",
              "      <th>t79</th>\n",
              "      <th>t80</th>\n",
              "      <th>t81</th>\n",
              "      <th>t82</th>\n",
              "      <th>t83</th>\n",
              "      <th>t84</th>\n",
              "      <th>t85</th>\n",
              "      <th>t86</th>\n",
              "      <th>t87</th>\n",
              "      <th>t88</th>\n",
              "      <th>t89</th>\n",
              "      <th>t90</th>\n",
              "      <th>t91</th>\n",
              "      <th>t92</th>\n",
              "      <th>t93</th>\n",
              "      <th>t94</th>\n",
              "      <th>t95</th>\n",
              "      <th>t96</th>\n",
              "      <th>t97</th>\n",
              "      <th>t98</th>\n",
              "      <th>t99</th>\n",
              "      <th>t100</th>\n",
              "      <th>t101</th>\n",
              "      <th>t102</th>\n",
              "      <th>t103</th>\n",
              "      <th>t104</th>\n",
              "      <th>t105</th>\n",
              "      <th>t106</th>\n",
              "      <th>t107</th>\n",
              "      <th>t108</th>\n",
              "      <th>t109</th>\n",
              "      <th>t110</th>\n",
              "      <th>t111</th>\n",
              "      <th>t112</th>\n",
              "      <th>t113</th>\n",
              "      <th>t114</th>\n",
              "      <th>t115</th>\n",
              "      <th>t116</th>\n",
              "      <th>t117</th>\n",
              "      <th>t118</th>\n",
              "      <th>t119</th>\n",
              "      <th>t120</th>\n",
              "      <th>t121</th>\n",
              "      <th>t122</th>\n",
              "      <th>t123</th>\n",
              "      <th>t124</th>\n",
              "      <th>t125</th>\n",
              "      <th>t126</th>\n",
              "      <th>t127</th>\n",
              "      <th>t128</th>\n",
              "      <th>t129</th>\n",
              "      <th>t130</th>\n",
              "      <th>t131</th>\n",
              "      <th>t132</th>\n",
              "      <th>t133</th>\n",
              "      <th>t134</th>\n",
              "      <th>t135</th>\n",
              "      <th>t136</th>\n",
              "      <th>t137</th>\n",
              "      <th>t138</th>\n",
              "      <th>t139</th>\n",
              "      <th>t140</th>\n",
              "      <th>t141</th>\n",
              "      <th>t142</th>\n",
              "      <th>t143</th>\n",
              "      <th>t144</th>\n",
              "      <th>t145</th>\n",
              "      <th>t146</th>\n",
              "      <th>t147</th>\n",
              "      <th>t148</th>\n",
              "      <th>t149</th>\n",
              "      <th>t150</th>\n",
              "      <th>t151</th>\n",
              "      <th>t152</th>\n",
              "      <th>t153</th>\n",
              "      <th>t154</th>\n",
              "      <th>t155</th>\n",
              "      <th>t156</th>\n",
              "      <th>t157</th>\n",
              "      <th>t158</th>\n",
              "      <th>t159</th>\n",
              "      <th>t160</th>\n",
              "      <th>t161</th>\n",
              "      <th>t162</th>\n",
              "      <th>t163</th>\n",
              "      <th>t164</th>\n",
              "      <th>t165</th>\n",
              "      <th>t166</th>\n",
              "      <th>t167</th>\n",
              "      <th>t168</th>\n",
              "      <th>t169</th>\n",
              "      <th>t170</th>\n",
              "      <th>t171</th>\n",
              "      <th>t172</th>\n",
              "      <th>t173</th>\n",
              "      <th>t174</th>\n",
              "      <th>t175</th>\n",
              "      <th>t176</th>\n",
              "      <th>t177</th>\n",
              "      <th>t178</th>\n",
              "      <th>t179</th>\n",
              "      <th>t180</th>\n",
              "      <th>t181</th>\n",
              "      <th>t182</th>\n",
              "      <th>t183</th>\n",
              "      <th>t184</th>\n",
              "      <th>t185</th>\n",
              "      <th>t186</th>\n",
              "      <th>t187</th>\n",
              "      <th>t188</th>\n",
              "      <th>t189</th>\n",
              "      <th>t190</th>\n",
              "      <th>t191</th>\n",
              "      <th>t192</th>\n",
              "      <th>t193</th>\n",
              "      <th>t194</th>\n",
              "      <th>t195</th>\n",
              "      <th>t196</th>\n",
              "      <th>t197</th>\n",
              "      <th>t198</th>\n",
              "      <th>t199</th>\n",
              "      <th>t200</th>\n",
              "      <th>t201</th>\n",
              "      <th>t202</th>\n",
              "      <th>t203</th>\n",
              "      <th>t204</th>\n",
              "      <th>t205</th>\n",
              "      <th>t206</th>\n",
              "      <th>t207</th>\n",
              "      <th>t208</th>\n",
              "      <th>t209</th>\n",
              "      <th>t210</th>\n",
              "      <th>t211</th>\n",
              "      <th>t212</th>\n",
              "      <th>t213</th>\n",
              "      <th>t214</th>\n",
              "      <th>t215</th>\n",
              "      <th>t216</th>\n",
              "      <th>t217</th>\n",
              "      <th>t218</th>\n",
              "      <th>t219</th>\n",
              "      <th>t220</th>\n",
              "      <th>t221</th>\n",
              "      <th>t222</th>\n",
              "      <th>t223</th>\n",
              "      <th>t224</th>\n",
              "      <th>t225</th>\n",
              "      <th>t226</th>\n",
              "      <th>t227</th>\n",
              "      <th>t228</th>\n",
              "      <th>t229</th>\n",
              "      <th>t230</th>\n",
              "      <th>t231</th>\n",
              "      <th>t232</th>\n",
              "      <th>t233</th>\n",
              "      <th>t234</th>\n",
              "      <th>t235</th>\n",
              "      <th>t236</th>\n",
              "      <th>t237</th>\n",
              "      <th>t238</th>\n",
              "      <th>t239</th>\n",
              "      <th>t240</th>\n",
              "      <th>t241</th>\n",
              "      <th>t242</th>\n",
              "      <th>t243</th>\n",
              "      <th>t244</th>\n",
              "      <th>t245</th>\n",
              "      <th>t246</th>\n",
              "      <th>t247</th>\n",
              "      <th>t248</th>\n",
              "      <th>t249</th>\n",
              "      <th>t250</th>\n",
              "      <th>t251</th>\n",
              "      <th>t252</th>\n",
              "      <th>t253</th>\n",
              "      <th>t254</th>\n",
              "      <th>t255</th>\n",
              "      <th>t256</th>\n",
              "      <th>t257</th>\n",
              "      <th>t258</th>\n",
              "      <th>t259</th>\n",
              "      <th>t260</th>\n",
              "      <th>t261</th>\n",
              "      <th>t262</th>\n",
              "      <th>t263</th>\n",
              "      <th>t264</th>\n",
              "      <th>t265</th>\n",
              "      <th>t266</th>\n",
              "      <th>t267</th>\n",
              "      <th>t268</th>\n",
              "      <th>t269</th>\n",
              "      <th>t270</th>\n",
              "      <th>t271</th>\n",
              "      <th>t272</th>\n",
              "      <th>t273</th>\n",
              "      <th>t274</th>\n",
              "      <th>t275</th>\n",
              "      <th>t276</th>\n",
              "      <th>t277</th>\n",
              "      <th>t278</th>\n",
              "      <th>t279</th>\n",
              "      <th>t280</th>\n",
              "      <th>t281</th>\n",
              "      <th>t282</th>\n",
              "      <th>t283</th>\n",
              "      <th>t284</th>\n",
              "      <th>t285</th>\n",
              "      <th>t286</th>\n",
              "      <th>t287</th>\n",
              "      <th>t288</th>\n",
              "      <th>t289</th>\n",
              "      <th>t290</th>\n",
              "      <th>t291</th>\n",
              "      <th>t292</th>\n",
              "      <th>t293</th>\n",
              "      <th>t294</th>\n",
              "      <th>t295</th>\n",
              "      <th>t296</th>\n",
              "      <th>t297</th>\n",
              "      <th>t298</th>\n",
              "      <th>t299</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.002900</td>\n",
              "      <td>0.044868</td>\n",
              "      <td>0.006784</td>\n",
              "      <td>0.074842</td>\n",
              "      <td>-0.102040</td>\n",
              "      <td>0.020503</td>\n",
              "      <td>0.050016</td>\n",
              "      <td>-0.074015</td>\n",
              "      <td>0.055866</td>\n",
              "      <td>0.099108</td>\n",
              "      <td>-0.051928</td>\n",
              "      <td>-0.119348</td>\n",
              "      <td>-0.016559</td>\n",
              "      <td>0.020685</td>\n",
              "      <td>-0.092850</td>\n",
              "      <td>0.080830</td>\n",
              "      <td>0.025311</td>\n",
              "      <td>0.084151</td>\n",
              "      <td>0.000409</td>\n",
              "      <td>-0.052561</td>\n",
              "      <td>0.004491</td>\n",
              "      <td>0.021908</td>\n",
              "      <td>0.052043</td>\n",
              "      <td>0.023320</td>\n",
              "      <td>0.002230</td>\n",
              "      <td>-0.010714</td>\n",
              "      <td>-0.032790</td>\n",
              "      <td>0.056665</td>\n",
              "      <td>0.015640</td>\n",
              "      <td>-0.052884</td>\n",
              "      <td>-0.026823</td>\n",
              "      <td>0.003278</td>\n",
              "      <td>0.002231</td>\n",
              "      <td>-0.074988</td>\n",
              "      <td>0.006963</td>\n",
              "      <td>-0.052575</td>\n",
              "      <td>0.012789</td>\n",
              "      <td>-0.016799</td>\n",
              "      <td>0.009686</td>\n",
              "      <td>0.029267</td>\n",
              "      <td>0.069609</td>\n",
              "      <td>-0.065814</td>\n",
              "      <td>0.124028</td>\n",
              "      <td>-0.042836</td>\n",
              "      <td>-0.058742</td>\n",
              "      <td>-0.068833</td>\n",
              "      <td>-0.031450</td>\n",
              "      <td>0.005493</td>\n",
              "      <td>0.023426</td>\n",
              "      <td>0.019639</td>\n",
              "      <td>0.000967</td>\n",
              "      <td>-0.004218</td>\n",
              "      <td>0.007726</td>\n",
              "      <td>-0.028178</td>\n",
              "      <td>0.038489</td>\n",
              "      <td>-0.003355</td>\n",
              "      <td>-0.008886</td>\n",
              "      <td>-0.075110</td>\n",
              "      <td>0.016318</td>\n",
              "      <td>-0.051416</td>\n",
              "      <td>-0.022196</td>\n",
              "      <td>0.044364</td>\n",
              "      <td>-0.096261</td>\n",
              "      <td>-0.030301</td>\n",
              "      <td>0.026029</td>\n",
              "      <td>-0.037417</td>\n",
              "      <td>-0.031374</td>\n",
              "      <td>0.060877</td>\n",
              "      <td>-0.031473</td>\n",
              "      <td>0.085079</td>\n",
              "      <td>0.040173</td>\n",
              "      <td>0.011696</td>\n",
              "      <td>0.057092</td>\n",
              "      <td>-0.027526</td>\n",
              "      <td>-0.138667</td>\n",
              "      <td>-0.075401</td>\n",
              "      <td>0.046112</td>\n",
              "      <td>0.053008</td>\n",
              "      <td>0.020549</td>\n",
              "      <td>0.050109</td>\n",
              "      <td>0.013487</td>\n",
              "      <td>-0.042455</td>\n",
              "      <td>0.023044</td>\n",
              "      <td>-0.003710</td>\n",
              "      <td>-0.010401</td>\n",
              "      <td>-0.020149</td>\n",
              "      <td>-0.040324</td>\n",
              "      <td>0.155952</td>\n",
              "      <td>0.052127</td>\n",
              "      <td>0.030129</td>\n",
              "      <td>0.015279</td>\n",
              "      <td>0.052953</td>\n",
              "      <td>-0.046857</td>\n",
              "      <td>-0.059532</td>\n",
              "      <td>-0.039087</td>\n",
              "      <td>-0.065372</td>\n",
              "      <td>0.030723</td>\n",
              "      <td>0.017287</td>\n",
              "      <td>0.055403</td>\n",
              "      <td>-0.021920</td>\n",
              "      <td>-0.035279</td>\n",
              "      <td>-0.049337</td>\n",
              "      <td>0.020826</td>\n",
              "      <td>0.003350</td>\n",
              "      <td>-0.022710</td>\n",
              "      <td>-0.081744</td>\n",
              "      <td>-0.005923</td>\n",
              "      <td>-0.009235</td>\n",
              "      <td>0.019858</td>\n",
              "      <td>-0.083950</td>\n",
              "      <td>-0.062847</td>\n",
              "      <td>-0.085897</td>\n",
              "      <td>-0.008505</td>\n",
              "      <td>-0.015938</td>\n",
              "      <td>0.051282</td>\n",
              "      <td>0.033426</td>\n",
              "      <td>0.035956</td>\n",
              "      <td>-0.023264</td>\n",
              "      <td>0.025825</td>\n",
              "      <td>0.023855</td>\n",
              "      <td>-0.057292</td>\n",
              "      <td>-0.003651</td>\n",
              "      <td>-0.070389</td>\n",
              "      <td>0.046999</td>\n",
              "      <td>0.015469</td>\n",
              "      <td>0.007323</td>\n",
              "      <td>-0.039538</td>\n",
              "      <td>-0.058791</td>\n",
              "      <td>0.057725</td>\n",
              "      <td>0.033074</td>\n",
              "      <td>-0.055515</td>\n",
              "      <td>-0.012679</td>\n",
              "      <td>-0.080372</td>\n",
              "      <td>0.009174</td>\n",
              "      <td>-0.017444</td>\n",
              "      <td>0.025905</td>\n",
              "      <td>0.044887</td>\n",
              "      <td>0.009169</td>\n",
              "      <td>0.002169</td>\n",
              "      <td>0.073048</td>\n",
              "      <td>0.057993</td>\n",
              "      <td>-0.088614</td>\n",
              "      <td>-0.030991</td>\n",
              "      <td>-0.003501</td>\n",
              "      <td>0.008017</td>\n",
              "      <td>0.036120</td>\n",
              "      <td>-0.020569</td>\n",
              "      <td>-0.031011</td>\n",
              "      <td>-0.071039</td>\n",
              "      <td>-0.043187</td>\n",
              "      <td>0.089305</td>\n",
              "      <td>-0.029168</td>\n",
              "      <td>-0.05180</td>\n",
              "      <td>0.088863</td>\n",
              "      <td>-0.029610</td>\n",
              "      <td>-0.004764</td>\n",
              "      <td>-0.061786</td>\n",
              "      <td>-0.049806</td>\n",
              "      <td>-0.075134</td>\n",
              "      <td>-0.019008</td>\n",
              "      <td>-0.028310</td>\n",
              "      <td>0.051731</td>\n",
              "      <td>0.003766</td>\n",
              "      <td>0.011950</td>\n",
              "      <td>0.015680</td>\n",
              "      <td>-0.065050</td>\n",
              "      <td>0.068574</td>\n",
              "      <td>-0.075329</td>\n",
              "      <td>-0.001712</td>\n",
              "      <td>-0.063603</td>\n",
              "      <td>-0.127667</td>\n",
              "      <td>-0.050622</td>\n",
              "      <td>-0.050118</td>\n",
              "      <td>-0.036819</td>\n",
              "      <td>-0.061596</td>\n",
              "      <td>-0.042022</td>\n",
              "      <td>0.124270</td>\n",
              "      <td>-0.081843</td>\n",
              "      <td>-0.014566</td>\n",
              "      <td>0.034287</td>\n",
              "      <td>-0.095007</td>\n",
              "      <td>-0.067428</td>\n",
              "      <td>0.051999</td>\n",
              "      <td>0.006993</td>\n",
              "      <td>-0.026237</td>\n",
              "      <td>-0.021041</td>\n",
              "      <td>-0.030848</td>\n",
              "      <td>0.000314</td>\n",
              "      <td>0.041433</td>\n",
              "      <td>0.034017</td>\n",
              "      <td>0.033520</td>\n",
              "      <td>-0.000682</td>\n",
              "      <td>-0.021132</td>\n",
              "      <td>0.004080</td>\n",
              "      <td>-0.007918</td>\n",
              "      <td>0.044946</td>\n",
              "      <td>0.003345</td>\n",
              "      <td>-0.040761</td>\n",
              "      <td>-0.099482</td>\n",
              "      <td>-0.108041</td>\n",
              "      <td>0.019720</td>\n",
              "      <td>0.047250</td>\n",
              "      <td>-0.056443</td>\n",
              "      <td>0.004647</td>\n",
              "      <td>0.056614</td>\n",
              "      <td>-0.030238</td>\n",
              "      <td>-0.018902</td>\n",
              "      <td>-0.030592</td>\n",
              "      <td>0.018074</td>\n",
              "      <td>-0.024106</td>\n",
              "      <td>-0.017929</td>\n",
              "      <td>0.056056</td>\n",
              "      <td>-0.072432</td>\n",
              "      <td>0.067731</td>\n",
              "      <td>-0.079447</td>\n",
              "      <td>0.030726</td>\n",
              "      <td>0.042337</td>\n",
              "      <td>0.009965</td>\n",
              "      <td>-0.089965</td>\n",
              "      <td>0.038176</td>\n",
              "      <td>-0.024134</td>\n",
              "      <td>0.006570</td>\n",
              "      <td>0.001954</td>\n",
              "      <td>0.003252</td>\n",
              "      <td>0.050383</td>\n",
              "      <td>-0.029478</td>\n",
              "      <td>0.029882</td>\n",
              "      <td>0.036708</td>\n",
              "      <td>0.053066</td>\n",
              "      <td>-0.000418</td>\n",
              "      <td>0.015018</td>\n",
              "      <td>-0.003766</td>\n",
              "      <td>0.021896</td>\n",
              "      <td>-0.020837</td>\n",
              "      <td>0.019659</td>\n",
              "      <td>-0.021408</td>\n",
              "      <td>-0.008348</td>\n",
              "      <td>-0.004686</td>\n",
              "      <td>0.070430</td>\n",
              "      <td>-0.005205</td>\n",
              "      <td>0.056064</td>\n",
              "      <td>-0.000275</td>\n",
              "      <td>-0.024886</td>\n",
              "      <td>-0.114259</td>\n",
              "      <td>0.023371</td>\n",
              "      <td>-0.028838</td>\n",
              "      <td>-0.029380</td>\n",
              "      <td>0.035168</td>\n",
              "      <td>0.021669</td>\n",
              "      <td>-0.065137</td>\n",
              "      <td>-0.008155</td>\n",
              "      <td>0.004890</td>\n",
              "      <td>0.051684</td>\n",
              "      <td>0.016112</td>\n",
              "      <td>0.020894</td>\n",
              "      <td>-0.070980</td>\n",
              "      <td>0.004604</td>\n",
              "      <td>0.001860</td>\n",
              "      <td>-0.044157</td>\n",
              "      <td>-0.061054</td>\n",
              "      <td>0.020566</td>\n",
              "      <td>0.006596</td>\n",
              "      <td>-0.074984</td>\n",
              "      <td>0.005134</td>\n",
              "      <td>0.037322</td>\n",
              "      <td>0.043370</td>\n",
              "      <td>-0.060567</td>\n",
              "      <td>-0.057459</td>\n",
              "      <td>-0.031847</td>\n",
              "      <td>0.012542</td>\n",
              "      <td>0.015441</td>\n",
              "      <td>0.077615</td>\n",
              "      <td>0.062218</td>\n",
              "      <td>0.025076</td>\n",
              "      <td>0.090393</td>\n",
              "      <td>-0.063815</td>\n",
              "      <td>-0.084251</td>\n",
              "      <td>-0.105852</td>\n",
              "      <td>-0.039521</td>\n",
              "      <td>0.019388</td>\n",
              "      <td>-0.009751</td>\n",
              "      <td>0.003045</td>\n",
              "      <td>0.068890</td>\n",
              "      <td>0.048047</td>\n",
              "      <td>-0.024688</td>\n",
              "      <td>-0.012525</td>\n",
              "      <td>-0.082527</td>\n",
              "      <td>0.024600</td>\n",
              "      <td>-0.002151</td>\n",
              "      <td>0.063706</td>\n",
              "      <td>-0.051720</td>\n",
              "      <td>0.060278</td>\n",
              "      <td>-0.099974</td>\n",
              "      <td>0.049301</td>\n",
              "      <td>-0.063635</td>\n",
              "      <td>-0.036664</td>\n",
              "      <td>0.010074</td>\n",
              "      <td>0.010265</td>\n",
              "      <td>-0.036405</td>\n",
              "      <td>-0.023421</td>\n",
              "      <td>-0.018311</td>\n",
              "      <td>0.038574</td>\n",
              "      <td>0.121716</td>\n",
              "      <td>0.000391</td>\n",
              "      <td>-0.012106</td>\n",
              "      <td>0.092407</td>\n",
              "      <td>0.001611</td>\n",
              "      <td>-0.098041</td>\n",
              "      <td>-0.056152</td>\n",
              "      <td>0.056384</td>\n",
              "      <td>-0.119556</td>\n",
              "      <td>-0.098828</td>\n",
              "      <td>-0.129980</td>\n",
              "      <td>0.148145</td>\n",
              "      <td>-0.032080</td>\n",
              "      <td>0.192139</td>\n",
              "      <td>0.074927</td>\n",
              "      <td>0.231836</td>\n",
              "      <td>-0.143042</td>\n",
              "      <td>-0.218604</td>\n",
              "      <td>-0.009082</td>\n",
              "      <td>0.102295</td>\n",
              "      <td>0.111035</td>\n",
              "      <td>0.101440</td>\n",
              "      <td>-0.204883</td>\n",
              "      <td>-0.223926</td>\n",
              "      <td>-0.151953</td>\n",
              "      <td>0.335547</td>\n",
              "      <td>-0.101514</td>\n",
              "      <td>-0.061676</td>\n",
              "      <td>0.032129</td>\n",
              "      <td>0.024976</td>\n",
              "      <td>-0.049664</td>\n",
              "      <td>0.012402</td>\n",
              "      <td>-0.121875</td>\n",
              "      <td>-0.144238</td>\n",
              "      <td>-0.005859</td>\n",
              "      <td>0.233203</td>\n",
              "      <td>0.004871</td>\n",
              "      <td>0.171411</td>\n",
              "      <td>0.147290</td>\n",
              "      <td>-0.181250</td>\n",
              "      <td>0.196094</td>\n",
              "      <td>0.158301</td>\n",
              "      <td>0.026025</td>\n",
              "      <td>-0.138916</td>\n",
              "      <td>-0.135449</td>\n",
              "      <td>-0.069141</td>\n",
              "      <td>-0.021948</td>\n",
              "      <td>0.072559</td>\n",
              "      <td>-0.233643</td>\n",
              "      <td>0.212158</td>\n",
              "      <td>-0.090771</td>\n",
              "      <td>-0.026465</td>\n",
              "      <td>-0.030566</td>\n",
              "      <td>0.163647</td>\n",
              "      <td>-0.025781</td>\n",
              "      <td>-0.037091</td>\n",
              "      <td>0.003735</td>\n",
              "      <td>-0.195801</td>\n",
              "      <td>-0.153857</td>\n",
              "      <td>-0.039685</td>\n",
              "      <td>-0.173242</td>\n",
              "      <td>0.001550</td>\n",
              "      <td>-0.065625</td>\n",
              "      <td>-0.131299</td>\n",
              "      <td>-0.033105</td>\n",
              "      <td>-0.130859</td>\n",
              "      <td>0.116309</td>\n",
              "      <td>0.256836</td>\n",
              "      <td>0.237109</td>\n",
              "      <td>0.105762</td>\n",
              "      <td>-0.007764</td>\n",
              "      <td>0.015723</td>\n",
              "      <td>-0.257715</td>\n",
              "      <td>-0.049255</td>\n",
              "      <td>0.016803</td>\n",
              "      <td>-0.009851</td>\n",
              "      <td>0.006238</td>\n",
              "      <td>0.002979</td>\n",
              "      <td>0.001514</td>\n",
              "      <td>0.138770</td>\n",
              "      <td>-0.046753</td>\n",
              "      <td>0.007715</td>\n",
              "      <td>0.233887</td>\n",
              "      <td>0.075586</td>\n",
              "      <td>-0.094824</td>\n",
              "      <td>0.279102</td>\n",
              "      <td>0.046021</td>\n",
              "      <td>-0.042554</td>\n",
              "      <td>-0.060352</td>\n",
              "      <td>-0.197070</td>\n",
              "      <td>-0.147217</td>\n",
              "      <td>0.130774</td>\n",
              "      <td>-0.059113</td>\n",
              "      <td>0.082520</td>\n",
              "      <td>-0.091309</td>\n",
              "      <td>-0.084106</td>\n",
              "      <td>0.189355</td>\n",
              "      <td>-0.073340</td>\n",
              "      <td>-0.017419</td>\n",
              "      <td>0.200391</td>\n",
              "      <td>0.139050</td>\n",
              "      <td>-0.084863</td>\n",
              "      <td>-0.016846</td>\n",
              "      <td>0.232520</td>\n",
              "      <td>0.039453</td>\n",
              "      <td>0.170703</td>\n",
              "      <td>-0.008691</td>\n",
              "      <td>0.047656</td>\n",
              "      <td>-0.044922</td>\n",
              "      <td>-0.179395</td>\n",
              "      <td>-0.052905</td>\n",
              "      <td>-0.038672</td>\n",
              "      <td>-0.020894</td>\n",
              "      <td>0.065918</td>\n",
              "      <td>-0.158105</td>\n",
              "      <td>0.004199</td>\n",
              "      <td>-0.000903</td>\n",
              "      <td>-0.084277</td>\n",
              "      <td>0.106775</td>\n",
              "      <td>-0.036182</td>\n",
              "      <td>-0.081836</td>\n",
              "      <td>-0.004175</td>\n",
              "      <td>0.145313</td>\n",
              "      <td>-0.229297</td>\n",
              "      <td>-0.151050</td>\n",
              "      <td>-0.013974</td>\n",
              "      <td>0.040918</td>\n",
              "      <td>0.031787</td>\n",
              "      <td>-0.019385</td>\n",
              "      <td>0.144043</td>\n",
              "      <td>0.094971</td>\n",
              "      <td>0.040332</td>\n",
              "      <td>-0.107324</td>\n",
              "      <td>-0.010547</td>\n",
              "      <td>-0.043555</td>\n",
              "      <td>0.088623</td>\n",
              "      <td>0.073547</td>\n",
              "      <td>0.126917</td>\n",
              "      <td>0.093652</td>\n",
              "      <td>-0.094879</td>\n",
              "      <td>0.056982</td>\n",
              "      <td>-0.063704</td>\n",
              "      <td>-0.065112</td>\n",
              "      <td>0.123389</td>\n",
              "      <td>-0.028906</td>\n",
              "      <td>0.081293</td>\n",
              "      <td>0.110910</td>\n",
              "      <td>-0.190332</td>\n",
              "      <td>0.332300</td>\n",
              "      <td>-0.123853</td>\n",
              "      <td>-0.124072</td>\n",
              "      <td>-0.055713</td>\n",
              "      <td>-0.096875</td>\n",
              "      <td>-0.046167</td>\n",
              "      <td>-0.051624</td>\n",
              "      <td>0.174609</td>\n",
              "      <td>0.001440</td>\n",
              "      <td>-0.149463</td>\n",
              "      <td>-0.011218</td>\n",
              "      <td>0.107031</td>\n",
              "      <td>0.096777</td>\n",
              "      <td>-0.011792</td>\n",
              "      <td>0.026862</td>\n",
              "      <td>-0.149219</td>\n",
              "      <td>0.103565</td>\n",
              "      <td>-0.069531</td>\n",
              "      <td>-0.148633</td>\n",
              "      <td>0.001855</td>\n",
              "      <td>-0.157715</td>\n",
              "      <td>0.024902</td>\n",
              "      <td>0.118164</td>\n",
              "      <td>-0.141992</td>\n",
              "      <td>-0.140918</td>\n",
              "      <td>0.073047</td>\n",
              "      <td>0.191187</td>\n",
              "      <td>-0.059521</td>\n",
              "      <td>-0.030298</td>\n",
              "      <td>-0.061230</td>\n",
              "      <td>-0.043359</td>\n",
              "      <td>0.017639</td>\n",
              "      <td>0.179736</td>\n",
              "      <td>0.011450</td>\n",
              "      <td>-0.043506</td>\n",
              "      <td>0.009692</td>\n",
              "      <td>0.048633</td>\n",
              "      <td>-0.003125</td>\n",
              "      <td>0.025342</td>\n",
              "      <td>0.096143</td>\n",
              "      <td>0.081689</td>\n",
              "      <td>0.091396</td>\n",
              "      <td>-0.214746</td>\n",
              "      <td>0.043872</td>\n",
              "      <td>0.192725</td>\n",
              "      <td>0.038037</td>\n",
              "      <td>-0.067609</td>\n",
              "      <td>0.023694</td>\n",
              "      <td>0.153564</td>\n",
              "      <td>-0.152832</td>\n",
              "      <td>0.041772</td>\n",
              "      <td>0.069055</td>\n",
              "      <td>0.009375</td>\n",
              "      <td>-0.080859</td>\n",
              "      <td>0.004895</td>\n",
              "      <td>-0.112939</td>\n",
              "      <td>0.086719</td>\n",
              "      <td>-0.022949</td>\n",
              "      <td>-0.117310</td>\n",
              "      <td>0.073291</td>\n",
              "      <td>0.041946</td>\n",
              "      <td>-0.075122</td>\n",
              "      <td>-0.047607</td>\n",
              "      <td>0.081763</td>\n",
              "      <td>0.016992</td>\n",
              "      <td>0.071423</td>\n",
              "      <td>-0.040039</td>\n",
              "      <td>-0.111469</td>\n",
              "      <td>0.017505</td>\n",
              "      <td>-0.040503</td>\n",
              "      <td>-0.161328</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.045410</td>\n",
              "      <td>0.177759</td>\n",
              "      <td>0.021155</td>\n",
              "      <td>0.006104</td>\n",
              "      <td>0.024414</td>\n",
              "      <td>-0.000720</td>\n",
              "      <td>-0.013330</td>\n",
              "      <td>-0.051074</td>\n",
              "      <td>0.145825</td>\n",
              "      <td>0.013062</td>\n",
              "      <td>-0.097949</td>\n",
              "      <td>-0.024988</td>\n",
              "      <td>-0.004395</td>\n",
              "      <td>0.058643</td>\n",
              "      <td>-0.023523</td>\n",
              "      <td>-0.058545</td>\n",
              "      <td>0.059229</td>\n",
              "      <td>0.010107</td>\n",
              "      <td>0.025269</td>\n",
              "      <td>0.063330</td>\n",
              "      <td>0.080859</td>\n",
              "      <td>0.056567</td>\n",
              "      <td>0.065137</td>\n",
              "      <td>0.177997</td>\n",
              "      <td>-0.204248</td>\n",
              "      <td>0.153624</td>\n",
              "      <td>-0.169824</td>\n",
              "      <td>-0.094043</td>\n",
              "      <td>0.029498</td>\n",
              "      <td>0.047485</td>\n",
              "      <td>0.125342</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.021429</td>\n",
              "      <td>0.040906</td>\n",
              "      <td>0.085815</td>\n",
              "      <td>0.042188</td>\n",
              "      <td>0.096387</td>\n",
              "      <td>-0.224023</td>\n",
              "      <td>0.144287</td>\n",
              "      <td>0.064551</td>\n",
              "      <td>0.114062</td>\n",
              "      <td>0.007031</td>\n",
              "      <td>0.069580</td>\n",
              "      <td>0.163733</td>\n",
              "      <td>0.091336</td>\n",
              "      <td>0.014941</td>\n",
              "      <td>0.072351</td>\n",
              "      <td>0.021094</td>\n",
              "      <td>-0.166406</td>\n",
              "      <td>-0.107458</td>\n",
              "      <td>-0.126450</td>\n",
              "      <td>-0.001663</td>\n",
              "      <td>0.094922</td>\n",
              "      <td>0.019629</td>\n",
              "      <td>-0.074170</td>\n",
              "      <td>-0.050815</td>\n",
              "      <td>-0.128979</td>\n",
              "      <td>0.259180</td>\n",
              "      <td>-0.103711</td>\n",
              "      <td>0.062769</td>\n",
              "      <td>-0.086444</td>\n",
              "      <td>-0.029028</td>\n",
              "      <td>-0.071680</td>\n",
              "      <td>-0.020801</td>\n",
              "      <td>-0.023938</td>\n",
              "      <td>0.084473</td>\n",
              "      <td>0.109802</td>\n",
              "      <td>-0.014111</td>\n",
              "      <td>-0.041663</td>\n",
              "      <td>0.020239</td>\n",
              "      <td>-0.049438</td>\n",
              "      <td>0.097559</td>\n",
              "      <td>-0.150610</td>\n",
              "      <td>-0.280664</td>\n",
              "      <td>-0.093506</td>\n",
              "      <td>-0.051709</td>\n",
              "      <td>0.004620</td>\n",
              "      <td>0.253906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.088189</td>\n",
              "      <td>0.007243</td>\n",
              "      <td>0.091960</td>\n",
              "      <td>0.115451</td>\n",
              "      <td>0.055745</td>\n",
              "      <td>0.077894</td>\n",
              "      <td>0.029948</td>\n",
              "      <td>0.123128</td>\n",
              "      <td>0.025262</td>\n",
              "      <td>0.071333</td>\n",
              "      <td>-0.032091</td>\n",
              "      <td>-0.036662</td>\n",
              "      <td>-0.032471</td>\n",
              "      <td>0.015869</td>\n",
              "      <td>-0.173570</td>\n",
              "      <td>0.156345</td>\n",
              "      <td>0.020345</td>\n",
              "      <td>0.013679</td>\n",
              "      <td>0.100316</td>\n",
              "      <td>-0.107720</td>\n",
              "      <td>0.081896</td>\n",
              "      <td>-0.051080</td>\n",
              "      <td>0.146905</td>\n",
              "      <td>-0.009928</td>\n",
              "      <td>0.041124</td>\n",
              "      <td>0.041829</td>\n",
              "      <td>-0.162733</td>\n",
              "      <td>0.026671</td>\n",
              "      <td>0.042638</td>\n",
              "      <td>-0.090773</td>\n",
              "      <td>-0.083252</td>\n",
              "      <td>0.021064</td>\n",
              "      <td>-0.049778</td>\n",
              "      <td>0.002726</td>\n",
              "      <td>-0.023702</td>\n",
              "      <td>0.010422</td>\n",
              "      <td>0.062147</td>\n",
              "      <td>0.020616</td>\n",
              "      <td>0.104736</td>\n",
              "      <td>0.098009</td>\n",
              "      <td>0.094523</td>\n",
              "      <td>-0.028428</td>\n",
              "      <td>0.071235</td>\n",
              "      <td>-0.000644</td>\n",
              "      <td>-0.053440</td>\n",
              "      <td>-0.078857</td>\n",
              "      <td>0.009440</td>\n",
              "      <td>-0.148098</td>\n",
              "      <td>0.000678</td>\n",
              "      <td>-0.025160</td>\n",
              "      <td>-0.076857</td>\n",
              "      <td>0.082316</td>\n",
              "      <td>0.076850</td>\n",
              "      <td>-0.025648</td>\n",
              "      <td>-0.000427</td>\n",
              "      <td>-0.024943</td>\n",
              "      <td>-0.070236</td>\n",
              "      <td>-0.039971</td>\n",
              "      <td>0.045383</td>\n",
              "      <td>-0.059306</td>\n",
              "      <td>0.059955</td>\n",
              "      <td>-0.036699</td>\n",
              "      <td>-0.088555</td>\n",
              "      <td>-0.090515</td>\n",
              "      <td>0.018077</td>\n",
              "      <td>-0.005819</td>\n",
              "      <td>-0.165371</td>\n",
              "      <td>0.106106</td>\n",
              "      <td>-0.090712</td>\n",
              "      <td>0.024540</td>\n",
              "      <td>0.114041</td>\n",
              "      <td>0.063192</td>\n",
              "      <td>0.141602</td>\n",
              "      <td>-0.020124</td>\n",
              "      <td>-0.158393</td>\n",
              "      <td>-0.080902</td>\n",
              "      <td>0.029217</td>\n",
              "      <td>0.126831</td>\n",
              "      <td>-0.054226</td>\n",
              "      <td>0.048733</td>\n",
              "      <td>-0.120768</td>\n",
              "      <td>-0.070357</td>\n",
              "      <td>-0.056939</td>\n",
              "      <td>0.054755</td>\n",
              "      <td>-0.079047</td>\n",
              "      <td>-0.013960</td>\n",
              "      <td>0.009250</td>\n",
              "      <td>0.108541</td>\n",
              "      <td>-0.018390</td>\n",
              "      <td>0.055203</td>\n",
              "      <td>0.018053</td>\n",
              "      <td>0.041463</td>\n",
              "      <td>-0.055366</td>\n",
              "      <td>-0.059380</td>\n",
              "      <td>-0.035563</td>\n",
              "      <td>-0.087375</td>\n",
              "      <td>0.048774</td>\n",
              "      <td>0.025336</td>\n",
              "      <td>0.036654</td>\n",
              "      <td>-0.128845</td>\n",
              "      <td>-0.159532</td>\n",
              "      <td>-0.011040</td>\n",
              "      <td>0.007541</td>\n",
              "      <td>0.044406</td>\n",
              "      <td>-0.052097</td>\n",
              "      <td>-0.063707</td>\n",
              "      <td>-0.018809</td>\n",
              "      <td>0.011041</td>\n",
              "      <td>0.024272</td>\n",
              "      <td>-0.115682</td>\n",
              "      <td>-0.093608</td>\n",
              "      <td>0.041821</td>\n",
              "      <td>0.011315</td>\n",
              "      <td>-0.015245</td>\n",
              "      <td>0.106283</td>\n",
              "      <td>-0.003045</td>\n",
              "      <td>0.026747</td>\n",
              "      <td>-0.081940</td>\n",
              "      <td>0.001587</td>\n",
              "      <td>0.032471</td>\n",
              "      <td>-0.006375</td>\n",
              "      <td>-0.035712</td>\n",
              "      <td>-0.098985</td>\n",
              "      <td>0.056286</td>\n",
              "      <td>0.048313</td>\n",
              "      <td>0.069048</td>\n",
              "      <td>-0.108344</td>\n",
              "      <td>0.015734</td>\n",
              "      <td>-0.016466</td>\n",
              "      <td>0.035224</td>\n",
              "      <td>-0.154012</td>\n",
              "      <td>-0.037082</td>\n",
              "      <td>0.001926</td>\n",
              "      <td>-0.003788</td>\n",
              "      <td>0.037543</td>\n",
              "      <td>-0.000081</td>\n",
              "      <td>-0.063341</td>\n",
              "      <td>0.036931</td>\n",
              "      <td>0.139506</td>\n",
              "      <td>0.049364</td>\n",
              "      <td>0.073107</td>\n",
              "      <td>-0.073931</td>\n",
              "      <td>-0.018853</td>\n",
              "      <td>0.004435</td>\n",
              "      <td>0.024431</td>\n",
              "      <td>-0.064128</td>\n",
              "      <td>-0.012729</td>\n",
              "      <td>-0.050157</td>\n",
              "      <td>-0.081604</td>\n",
              "      <td>-0.089179</td>\n",
              "      <td>0.075236</td>\n",
              "      <td>-0.038113</td>\n",
              "      <td>0.02592</td>\n",
              "      <td>0.024251</td>\n",
              "      <td>0.008892</td>\n",
              "      <td>-0.063938</td>\n",
              "      <td>-0.103651</td>\n",
              "      <td>-0.000882</td>\n",
              "      <td>-0.147678</td>\n",
              "      <td>-0.062283</td>\n",
              "      <td>0.058755</td>\n",
              "      <td>0.024222</td>\n",
              "      <td>-0.022732</td>\n",
              "      <td>0.054565</td>\n",
              "      <td>0.031067</td>\n",
              "      <td>-0.192247</td>\n",
              "      <td>0.135914</td>\n",
              "      <td>-0.055759</td>\n",
              "      <td>-0.048231</td>\n",
              "      <td>0.050334</td>\n",
              "      <td>-0.034105</td>\n",
              "      <td>-0.070502</td>\n",
              "      <td>-0.079102</td>\n",
              "      <td>-0.086263</td>\n",
              "      <td>-0.033569</td>\n",
              "      <td>0.015654</td>\n",
              "      <td>0.104824</td>\n",
              "      <td>-0.118137</td>\n",
              "      <td>-0.014459</td>\n",
              "      <td>-0.088637</td>\n",
              "      <td>-0.166287</td>\n",
              "      <td>-0.079264</td>\n",
              "      <td>0.025662</td>\n",
              "      <td>-0.048726</td>\n",
              "      <td>-0.107530</td>\n",
              "      <td>0.027649</td>\n",
              "      <td>-0.039341</td>\n",
              "      <td>-0.104696</td>\n",
              "      <td>0.084323</td>\n",
              "      <td>0.087362</td>\n",
              "      <td>0.067410</td>\n",
              "      <td>-0.009169</td>\n",
              "      <td>0.035862</td>\n",
              "      <td>-0.016137</td>\n",
              "      <td>-0.099555</td>\n",
              "      <td>0.168660</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.010123</td>\n",
              "      <td>-0.016927</td>\n",
              "      <td>-0.088026</td>\n",
              "      <td>-0.014004</td>\n",
              "      <td>0.048435</td>\n",
              "      <td>-0.102322</td>\n",
              "      <td>-0.095350</td>\n",
              "      <td>0.042611</td>\n",
              "      <td>0.020806</td>\n",
              "      <td>-0.204029</td>\n",
              "      <td>-0.005995</td>\n",
              "      <td>-0.033474</td>\n",
              "      <td>0.072361</td>\n",
              "      <td>0.045763</td>\n",
              "      <td>0.079590</td>\n",
              "      <td>-0.020847</td>\n",
              "      <td>-0.043789</td>\n",
              "      <td>-0.088976</td>\n",
              "      <td>0.052572</td>\n",
              "      <td>0.167426</td>\n",
              "      <td>-0.003418</td>\n",
              "      <td>-0.101766</td>\n",
              "      <td>-0.042155</td>\n",
              "      <td>0.002700</td>\n",
              "      <td>0.010810</td>\n",
              "      <td>0.028130</td>\n",
              "      <td>-0.028756</td>\n",
              "      <td>0.113525</td>\n",
              "      <td>0.004313</td>\n",
              "      <td>0.040853</td>\n",
              "      <td>0.029036</td>\n",
              "      <td>0.010722</td>\n",
              "      <td>-0.091092</td>\n",
              "      <td>0.100193</td>\n",
              "      <td>0.027866</td>\n",
              "      <td>-0.001383</td>\n",
              "      <td>0.084839</td>\n",
              "      <td>0.055108</td>\n",
              "      <td>-0.004367</td>\n",
              "      <td>0.001600</td>\n",
              "      <td>0.050103</td>\n",
              "      <td>0.041341</td>\n",
              "      <td>0.002348</td>\n",
              "      <td>-0.033411</td>\n",
              "      <td>0.013624</td>\n",
              "      <td>0.009910</td>\n",
              "      <td>-0.121636</td>\n",
              "      <td>-0.035078</td>\n",
              "      <td>0.046644</td>\n",
              "      <td>-0.006586</td>\n",
              "      <td>0.068244</td>\n",
              "      <td>-0.047831</td>\n",
              "      <td>0.003977</td>\n",
              "      <td>0.019701</td>\n",
              "      <td>0.033976</td>\n",
              "      <td>-0.007785</td>\n",
              "      <td>0.115845</td>\n",
              "      <td>0.013068</td>\n",
              "      <td>-0.099718</td>\n",
              "      <td>-0.084975</td>\n",
              "      <td>-0.003499</td>\n",
              "      <td>-0.083266</td>\n",
              "      <td>0.000732</td>\n",
              "      <td>-0.097168</td>\n",
              "      <td>0.020101</td>\n",
              "      <td>-0.108222</td>\n",
              "      <td>0.006131</td>\n",
              "      <td>-0.065145</td>\n",
              "      <td>0.106622</td>\n",
              "      <td>-0.011868</td>\n",
              "      <td>0.007395</td>\n",
              "      <td>0.011353</td>\n",
              "      <td>-0.112196</td>\n",
              "      <td>0.030952</td>\n",
              "      <td>0.154975</td>\n",
              "      <td>0.161947</td>\n",
              "      <td>0.067444</td>\n",
              "      <td>0.139296</td>\n",
              "      <td>-0.049983</td>\n",
              "      <td>-0.045176</td>\n",
              "      <td>-0.134861</td>\n",
              "      <td>-0.026910</td>\n",
              "      <td>0.028887</td>\n",
              "      <td>0.027313</td>\n",
              "      <td>0.029446</td>\n",
              "      <td>0.051520</td>\n",
              "      <td>0.112657</td>\n",
              "      <td>0.126980</td>\n",
              "      <td>0.008865</td>\n",
              "      <td>-0.073242</td>\n",
              "      <td>0.019864</td>\n",
              "      <td>-0.032162</td>\n",
              "      <td>0.135525</td>\n",
              "      <td>-0.020562</td>\n",
              "      <td>0.030989</td>\n",
              "      <td>-0.087918</td>\n",
              "      <td>-0.067471</td>\n",
              "      <td>-0.021484</td>\n",
              "      <td>-0.055718</td>\n",
              "      <td>-0.062086</td>\n",
              "      <td>-0.081441</td>\n",
              "      <td>-0.021539</td>\n",
              "      <td>0.057465</td>\n",
              "      <td>0.089098</td>\n",
              "      <td>0.020556</td>\n",
              "      <td>0.023010</td>\n",
              "      <td>0.100045</td>\n",
              "      <td>-0.119978</td>\n",
              "      <td>0.056863</td>\n",
              "      <td>0.058434</td>\n",
              "      <td>-0.004328</td>\n",
              "      <td>0.027222</td>\n",
              "      <td>0.072852</td>\n",
              "      <td>-0.029977</td>\n",
              "      <td>-0.114386</td>\n",
              "      <td>-0.047398</td>\n",
              "      <td>0.012390</td>\n",
              "      <td>-0.087734</td>\n",
              "      <td>0.115348</td>\n",
              "      <td>0.073425</td>\n",
              "      <td>0.091221</td>\n",
              "      <td>0.132725</td>\n",
              "      <td>-0.111607</td>\n",
              "      <td>0.062212</td>\n",
              "      <td>0.044120</td>\n",
              "      <td>0.154942</td>\n",
              "      <td>-0.016061</td>\n",
              "      <td>0.081545</td>\n",
              "      <td>0.030361</td>\n",
              "      <td>-0.075835</td>\n",
              "      <td>0.027137</td>\n",
              "      <td>0.066728</td>\n",
              "      <td>-0.047751</td>\n",
              "      <td>-0.061857</td>\n",
              "      <td>0.043405</td>\n",
              "      <td>-0.015472</td>\n",
              "      <td>-0.070090</td>\n",
              "      <td>0.016836</td>\n",
              "      <td>-0.041478</td>\n",
              "      <td>0.088283</td>\n",
              "      <td>-0.014659</td>\n",
              "      <td>0.033763</td>\n",
              "      <td>0.104623</td>\n",
              "      <td>0.050111</td>\n",
              "      <td>-0.158273</td>\n",
              "      <td>0.168069</td>\n",
              "      <td>-0.011043</td>\n",
              "      <td>-0.067073</td>\n",
              "      <td>-0.068355</td>\n",
              "      <td>-0.078875</td>\n",
              "      <td>-0.049966</td>\n",
              "      <td>0.009609</td>\n",
              "      <td>0.017397</td>\n",
              "      <td>-0.025532</td>\n",
              "      <td>0.145974</td>\n",
              "      <td>0.042249</td>\n",
              "      <td>-0.004987</td>\n",
              "      <td>0.053079</td>\n",
              "      <td>-0.034886</td>\n",
              "      <td>-0.065368</td>\n",
              "      <td>-0.044560</td>\n",
              "      <td>0.034807</td>\n",
              "      <td>-0.034746</td>\n",
              "      <td>-0.021083</td>\n",
              "      <td>0.066177</td>\n",
              "      <td>-0.160226</td>\n",
              "      <td>-0.019152</td>\n",
              "      <td>-0.009565</td>\n",
              "      <td>-0.065063</td>\n",
              "      <td>-0.076922</td>\n",
              "      <td>0.024283</td>\n",
              "      <td>-0.027926</td>\n",
              "      <td>0.057822</td>\n",
              "      <td>0.098232</td>\n",
              "      <td>0.092233</td>\n",
              "      <td>0.115104</td>\n",
              "      <td>0.006280</td>\n",
              "      <td>-0.191272</td>\n",
              "      <td>-0.116340</td>\n",
              "      <td>0.045394</td>\n",
              "      <td>0.142866</td>\n",
              "      <td>-0.005694</td>\n",
              "      <td>0.059736</td>\n",
              "      <td>-0.162463</td>\n",
              "      <td>-0.028331</td>\n",
              "      <td>0.067810</td>\n",
              "      <td>0.009018</td>\n",
              "      <td>0.005733</td>\n",
              "      <td>-0.013283</td>\n",
              "      <td>-0.002862</td>\n",
              "      <td>0.133615</td>\n",
              "      <td>0.083270</td>\n",
              "      <td>0.042997</td>\n",
              "      <td>0.038459</td>\n",
              "      <td>0.070086</td>\n",
              "      <td>-0.147581</td>\n",
              "      <td>-0.053349</td>\n",
              "      <td>-0.006366</td>\n",
              "      <td>-0.037900</td>\n",
              "      <td>0.084333</td>\n",
              "      <td>0.032506</td>\n",
              "      <td>0.063561</td>\n",
              "      <td>-0.008754</td>\n",
              "      <td>-0.121865</td>\n",
              "      <td>-0.014938</td>\n",
              "      <td>0.053807</td>\n",
              "      <td>0.026393</td>\n",
              "      <td>-0.041260</td>\n",
              "      <td>-0.054025</td>\n",
              "      <td>-0.063640</td>\n",
              "      <td>0.021460</td>\n",
              "      <td>0.079760</td>\n",
              "      <td>-0.086953</td>\n",
              "      <td>-0.154681</td>\n",
              "      <td>-0.101035</td>\n",
              "      <td>-0.036558</td>\n",
              "      <td>-0.014840</td>\n",
              "      <td>0.112305</td>\n",
              "      <td>-0.032194</td>\n",
              "      <td>0.031673</td>\n",
              "      <td>-0.106628</td>\n",
              "      <td>-0.014512</td>\n",
              "      <td>0.003254</td>\n",
              "      <td>-0.111160</td>\n",
              "      <td>0.029419</td>\n",
              "      <td>-0.061432</td>\n",
              "      <td>0.088517</td>\n",
              "      <td>0.030186</td>\n",
              "      <td>-0.066302</td>\n",
              "      <td>-0.156930</td>\n",
              "      <td>-0.023039</td>\n",
              "      <td>-0.009102</td>\n",
              "      <td>-0.091675</td>\n",
              "      <td>-0.125000</td>\n",
              "      <td>-0.143381</td>\n",
              "      <td>-0.070613</td>\n",
              "      <td>0.033029</td>\n",
              "      <td>0.037371</td>\n",
              "      <td>-0.064501</td>\n",
              "      <td>-0.002289</td>\n",
              "      <td>0.064309</td>\n",
              "      <td>0.024748</td>\n",
              "      <td>0.088841</td>\n",
              "      <td>0.032076</td>\n",
              "      <td>-0.101009</td>\n",
              "      <td>0.005336</td>\n",
              "      <td>-0.038583</td>\n",
              "      <td>-0.004377</td>\n",
              "      <td>-0.019039</td>\n",
              "      <td>-0.121286</td>\n",
              "      <td>-0.114184</td>\n",
              "      <td>-0.048846</td>\n",
              "      <td>-0.015703</td>\n",
              "      <td>0.132725</td>\n",
              "      <td>0.052883</td>\n",
              "      <td>-0.113735</td>\n",
              "      <td>0.062081</td>\n",
              "      <td>-0.018232</td>\n",
              "      <td>0.014747</td>\n",
              "      <td>-0.099243</td>\n",
              "      <td>-0.098328</td>\n",
              "      <td>-0.068848</td>\n",
              "      <td>-0.016946</td>\n",
              "      <td>-0.073147</td>\n",
              "      <td>0.112277</td>\n",
              "      <td>-0.026016</td>\n",
              "      <td>0.056745</td>\n",
              "      <td>0.016065</td>\n",
              "      <td>-0.188703</td>\n",
              "      <td>0.028740</td>\n",
              "      <td>-0.128270</td>\n",
              "      <td>0.010881</td>\n",
              "      <td>-0.028346</td>\n",
              "      <td>-0.119001</td>\n",
              "      <td>-0.044338</td>\n",
              "      <td>-0.074533</td>\n",
              "      <td>-0.013338</td>\n",
              "      <td>-0.109183</td>\n",
              "      <td>-0.035910</td>\n",
              "      <td>0.150133</td>\n",
              "      <td>-0.106437</td>\n",
              "      <td>-0.014971</td>\n",
              "      <td>0.021593</td>\n",
              "      <td>-0.046334</td>\n",
              "      <td>-0.052717</td>\n",
              "      <td>0.050182</td>\n",
              "      <td>-0.023215</td>\n",
              "      <td>-0.017840</td>\n",
              "      <td>0.031839</td>\n",
              "      <td>-0.037521</td>\n",
              "      <td>0.032619</td>\n",
              "      <td>0.115670</td>\n",
              "      <td>0.044604</td>\n",
              "      <td>-0.000645</td>\n",
              "      <td>0.061070</td>\n",
              "      <td>0.033578</td>\n",
              "      <td>0.048179</td>\n",
              "      <td>-0.005371</td>\n",
              "      <td>-0.021650</td>\n",
              "      <td>0.020839</td>\n",
              "      <td>-0.028008</td>\n",
              "      <td>-0.074624</td>\n",
              "      <td>-0.046430</td>\n",
              "      <td>-0.054559</td>\n",
              "      <td>0.084636</td>\n",
              "      <td>-0.074289</td>\n",
              "      <td>0.000392</td>\n",
              "      <td>-0.026789</td>\n",
              "      <td>0.026071</td>\n",
              "      <td>-0.151986</td>\n",
              "      <td>-0.061125</td>\n",
              "      <td>0.027911</td>\n",
              "      <td>0.057251</td>\n",
              "      <td>0.029458</td>\n",
              "      <td>0.080994</td>\n",
              "      <td>-0.067383</td>\n",
              "      <td>0.027518</td>\n",
              "      <td>-0.098894</td>\n",
              "      <td>0.038565</td>\n",
              "      <td>0.073661</td>\n",
              "      <td>0.051880</td>\n",
              "      <td>-0.100534</td>\n",
              "      <td>-0.013027</td>\n",
              "      <td>-0.042953</td>\n",
              "      <td>-0.065674</td>\n",
              "      <td>0.002171</td>\n",
              "      <td>-0.030077</td>\n",
              "      <td>0.086129</td>\n",
              "      <td>0.007517</td>\n",
              "      <td>0.089547</td>\n",
              "      <td>0.057080</td>\n",
              "      <td>-0.042071</td>\n",
              "      <td>-0.077942</td>\n",
              "      <td>0.046330</td>\n",
              "      <td>-0.033794</td>\n",
              "      <td>-0.004046</td>\n",
              "      <td>0.021270</td>\n",
              "      <td>0.103446</td>\n",
              "      <td>0.016462</td>\n",
              "      <td>0.005774</td>\n",
              "      <td>-0.004153</td>\n",
              "      <td>0.133545</td>\n",
              "      <td>0.008981</td>\n",
              "      <td>0.096976</td>\n",
              "      <td>0.015747</td>\n",
              "      <td>0.015418</td>\n",
              "      <td>-0.175354</td>\n",
              "      <td>-0.028327</td>\n",
              "      <td>-0.001452</td>\n",
              "      <td>-0.068238</td>\n",
              "      <td>0.075274</td>\n",
              "      <td>-0.074908</td>\n",
              "      <td>-0.075248</td>\n",
              "      <td>-0.022600</td>\n",
              "      <td>-0.001022</td>\n",
              "      <td>0.070151</td>\n",
              "      <td>0.122295</td>\n",
              "      <td>0.086075</td>\n",
              "      <td>-0.040690</td>\n",
              "      <td>0.060420</td>\n",
              "      <td>-0.035948</td>\n",
              "      <td>-0.070112</td>\n",
              "      <td>-0.035906</td>\n",
              "      <td>-0.040187</td>\n",
              "      <td>0.027570</td>\n",
              "      <td>-0.125907</td>\n",
              "      <td>0.046112</td>\n",
              "      <td>-0.029081</td>\n",
              "      <td>0.062727</td>\n",
              "      <td>-0.035854</td>\n",
              "      <td>0.041118</td>\n",
              "      <td>-0.057421</td>\n",
              "      <td>0.035324</td>\n",
              "      <td>-0.063969</td>\n",
              "      <td>0.213640</td>\n",
              "      <td>0.167280</td>\n",
              "      <td>0.070927</td>\n",
              "      <td>0.001700</td>\n",
              "      <td>-0.061162</td>\n",
              "      <td>-0.050343</td>\n",
              "      <td>-0.113591</td>\n",
              "      <td>-0.037903</td>\n",
              "      <td>0.031588</td>\n",
              "      <td>0.068987</td>\n",
              "      <td>-0.052558</td>\n",
              "      <td>0.021000</td>\n",
              "      <td>0.135027</td>\n",
              "      <td>-0.034188</td>\n",
              "      <td>0.043516</td>\n",
              "      <td>-0.071230</td>\n",
              "      <td>-0.063962</td>\n",
              "      <td>0.013829</td>\n",
              "      <td>0.029641</td>\n",
              "      <td>-0.037490</td>\n",
              "      <td>0.087282</td>\n",
              "      <td>-0.195103</td>\n",
              "      <td>0.001875</td>\n",
              "      <td>-0.068643</td>\n",
              "      <td>-0.043719</td>\n",
              "      <td>0.059154</td>\n",
              "      <td>-0.059278</td>\n",
              "      <td>-0.062101</td>\n",
              "      <td>0.004259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.043005</td>\n",
              "      <td>0.080643</td>\n",
              "      <td>-0.008636</td>\n",
              "      <td>0.118901</td>\n",
              "      <td>-0.073688</td>\n",
              "      <td>-0.004681</td>\n",
              "      <td>-0.044107</td>\n",
              "      <td>-0.062599</td>\n",
              "      <td>-0.042023</td>\n",
              "      <td>0.148212</td>\n",
              "      <td>-0.116117</td>\n",
              "      <td>-0.137677</td>\n",
              "      <td>-0.033790</td>\n",
              "      <td>-0.012560</td>\n",
              "      <td>-0.103225</td>\n",
              "      <td>0.168814</td>\n",
              "      <td>0.104610</td>\n",
              "      <td>0.053401</td>\n",
              "      <td>0.050854</td>\n",
              "      <td>-0.094546</td>\n",
              "      <td>-0.066885</td>\n",
              "      <td>0.037579</td>\n",
              "      <td>0.166987</td>\n",
              "      <td>-0.031743</td>\n",
              "      <td>0.044471</td>\n",
              "      <td>-0.005512</td>\n",
              "      <td>0.011256</td>\n",
              "      <td>0.014382</td>\n",
              "      <td>0.026179</td>\n",
              "      <td>0.030738</td>\n",
              "      <td>-0.015289</td>\n",
              "      <td>0.043077</td>\n",
              "      <td>-0.023405</td>\n",
              "      <td>-0.104455</td>\n",
              "      <td>-0.036922</td>\n",
              "      <td>0.011700</td>\n",
              "      <td>0.032654</td>\n",
              "      <td>0.012219</td>\n",
              "      <td>0.040889</td>\n",
              "      <td>0.148212</td>\n",
              "      <td>0.111298</td>\n",
              "      <td>-0.147668</td>\n",
              "      <td>0.147837</td>\n",
              "      <td>-0.006129</td>\n",
              "      <td>-0.021409</td>\n",
              "      <td>-0.090104</td>\n",
              "      <td>-0.050051</td>\n",
              "      <td>0.003947</td>\n",
              "      <td>-0.124154</td>\n",
              "      <td>0.063026</td>\n",
              "      <td>-0.070064</td>\n",
              "      <td>0.065117</td>\n",
              "      <td>0.075721</td>\n",
              "      <td>0.072885</td>\n",
              "      <td>-0.016714</td>\n",
              "      <td>0.014907</td>\n",
              "      <td>-0.034901</td>\n",
              "      <td>-0.029010</td>\n",
              "      <td>0.147529</td>\n",
              "      <td>-0.138851</td>\n",
              "      <td>-0.016465</td>\n",
              "      <td>0.125352</td>\n",
              "      <td>-0.054331</td>\n",
              "      <td>0.015553</td>\n",
              "      <td>-0.029710</td>\n",
              "      <td>-0.074407</td>\n",
              "      <td>-0.053964</td>\n",
              "      <td>0.032507</td>\n",
              "      <td>-0.048323</td>\n",
              "      <td>0.136024</td>\n",
              "      <td>0.086731</td>\n",
              "      <td>0.045422</td>\n",
              "      <td>0.112488</td>\n",
              "      <td>-0.074895</td>\n",
              "      <td>-0.175565</td>\n",
              "      <td>-0.083909</td>\n",
              "      <td>0.031743</td>\n",
              "      <td>0.075970</td>\n",
              "      <td>0.139123</td>\n",
              "      <td>0.112333</td>\n",
              "      <td>-0.012921</td>\n",
              "      <td>-0.015076</td>\n",
              "      <td>0.106023</td>\n",
              "      <td>-0.015627</td>\n",
              "      <td>-0.035288</td>\n",
              "      <td>0.027006</td>\n",
              "      <td>-0.076641</td>\n",
              "      <td>0.162177</td>\n",
              "      <td>-0.032856</td>\n",
              "      <td>0.012010</td>\n",
              "      <td>0.109571</td>\n",
              "      <td>0.032290</td>\n",
              "      <td>-0.092302</td>\n",
              "      <td>-0.001883</td>\n",
              "      <td>-0.030702</td>\n",
              "      <td>-0.053429</td>\n",
              "      <td>0.075749</td>\n",
              "      <td>0.064798</td>\n",
              "      <td>0.024825</td>\n",
              "      <td>-0.078530</td>\n",
              "      <td>-0.073106</td>\n",
              "      <td>0.014198</td>\n",
              "      <td>0.073829</td>\n",
              "      <td>0.036020</td>\n",
              "      <td>0.044260</td>\n",
              "      <td>-0.055021</td>\n",
              "      <td>0.017936</td>\n",
              "      <td>0.029503</td>\n",
              "      <td>0.033809</td>\n",
              "      <td>-0.105929</td>\n",
              "      <td>-0.066378</td>\n",
              "      <td>0.035889</td>\n",
              "      <td>0.045113</td>\n",
              "      <td>-0.006066</td>\n",
              "      <td>0.203942</td>\n",
              "      <td>-0.041438</td>\n",
              "      <td>-0.035950</td>\n",
              "      <td>-0.041255</td>\n",
              "      <td>-0.100652</td>\n",
              "      <td>0.056502</td>\n",
              "      <td>-0.059148</td>\n",
              "      <td>0.019005</td>\n",
              "      <td>-0.048950</td>\n",
              "      <td>0.081881</td>\n",
              "      <td>-0.092745</td>\n",
              "      <td>-0.017757</td>\n",
              "      <td>-0.078012</td>\n",
              "      <td>-0.114848</td>\n",
              "      <td>0.044063</td>\n",
              "      <td>0.009083</td>\n",
              "      <td>-0.190080</td>\n",
              "      <td>0.030019</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>0.025395</td>\n",
              "      <td>-0.028757</td>\n",
              "      <td>-0.041654</td>\n",
              "      <td>0.034903</td>\n",
              "      <td>0.006423</td>\n",
              "      <td>0.063589</td>\n",
              "      <td>0.078501</td>\n",
              "      <td>0.048037</td>\n",
              "      <td>-0.107132</td>\n",
              "      <td>-0.051101</td>\n",
              "      <td>-0.058331</td>\n",
              "      <td>0.043679</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>-0.023165</td>\n",
              "      <td>-0.077486</td>\n",
              "      <td>-0.109290</td>\n",
              "      <td>-0.106445</td>\n",
              "      <td>0.074270</td>\n",
              "      <td>0.055007</td>\n",
              "      <td>-0.08729</td>\n",
              "      <td>0.031982</td>\n",
              "      <td>-0.050725</td>\n",
              "      <td>0.028879</td>\n",
              "      <td>-0.066673</td>\n",
              "      <td>-0.089524</td>\n",
              "      <td>-0.135968</td>\n",
              "      <td>-0.046869</td>\n",
              "      <td>0.021794</td>\n",
              "      <td>0.039537</td>\n",
              "      <td>-0.057767</td>\n",
              "      <td>0.042302</td>\n",
              "      <td>0.170560</td>\n",
              "      <td>-0.158105</td>\n",
              "      <td>0.064904</td>\n",
              "      <td>-0.048697</td>\n",
              "      <td>0.033945</td>\n",
              "      <td>0.061843</td>\n",
              "      <td>-0.113262</td>\n",
              "      <td>0.036602</td>\n",
              "      <td>-0.092680</td>\n",
              "      <td>-0.114246</td>\n",
              "      <td>-0.168442</td>\n",
              "      <td>0.039652</td>\n",
              "      <td>0.162274</td>\n",
              "      <td>-0.128981</td>\n",
              "      <td>-0.000559</td>\n",
              "      <td>0.005426</td>\n",
              "      <td>-0.015400</td>\n",
              "      <td>-0.026832</td>\n",
              "      <td>0.091830</td>\n",
              "      <td>-0.010052</td>\n",
              "      <td>-0.053317</td>\n",
              "      <td>-0.091036</td>\n",
              "      <td>-0.040952</td>\n",
              "      <td>0.012902</td>\n",
              "      <td>0.042415</td>\n",
              "      <td>0.113347</td>\n",
              "      <td>-0.046035</td>\n",
              "      <td>-0.010019</td>\n",
              "      <td>0.014141</td>\n",
              "      <td>0.032536</td>\n",
              "      <td>0.066636</td>\n",
              "      <td>0.083890</td>\n",
              "      <td>-0.027381</td>\n",
              "      <td>0.045119</td>\n",
              "      <td>-0.077209</td>\n",
              "      <td>-0.010075</td>\n",
              "      <td>0.073702</td>\n",
              "      <td>0.041072</td>\n",
              "      <td>-0.184284</td>\n",
              "      <td>-0.052185</td>\n",
              "      <td>0.009926</td>\n",
              "      <td>-0.037278</td>\n",
              "      <td>-0.025531</td>\n",
              "      <td>0.058959</td>\n",
              "      <td>0.009179</td>\n",
              "      <td>0.040227</td>\n",
              "      <td>-0.064819</td>\n",
              "      <td>0.062933</td>\n",
              "      <td>-0.122352</td>\n",
              "      <td>0.019409</td>\n",
              "      <td>-0.031184</td>\n",
              "      <td>0.105999</td>\n",
              "      <td>0.099628</td>\n",
              "      <td>0.005991</td>\n",
              "      <td>-0.067148</td>\n",
              "      <td>-0.031210</td>\n",
              "      <td>-0.126131</td>\n",
              "      <td>0.008939</td>\n",
              "      <td>-0.035856</td>\n",
              "      <td>0.001810</td>\n",
              "      <td>0.108070</td>\n",
              "      <td>-0.000657</td>\n",
              "      <td>0.015057</td>\n",
              "      <td>0.009775</td>\n",
              "      <td>0.001850</td>\n",
              "      <td>0.138165</td>\n",
              "      <td>0.029924</td>\n",
              "      <td>0.066923</td>\n",
              "      <td>0.016872</td>\n",
              "      <td>0.059786</td>\n",
              "      <td>0.060115</td>\n",
              "      <td>0.111214</td>\n",
              "      <td>-0.113790</td>\n",
              "      <td>-0.052147</td>\n",
              "      <td>0.109937</td>\n",
              "      <td>0.007986</td>\n",
              "      <td>0.197378</td>\n",
              "      <td>-0.021268</td>\n",
              "      <td>-0.004765</td>\n",
              "      <td>0.035345</td>\n",
              "      <td>-0.013240</td>\n",
              "      <td>0.022134</td>\n",
              "      <td>0.034292</td>\n",
              "      <td>-0.005920</td>\n",
              "      <td>-0.023910</td>\n",
              "      <td>-0.041485</td>\n",
              "      <td>-0.066064</td>\n",
              "      <td>0.063242</td>\n",
              "      <td>0.147987</td>\n",
              "      <td>0.088125</td>\n",
              "      <td>0.090971</td>\n",
              "      <td>-0.162335</td>\n",
              "      <td>0.030649</td>\n",
              "      <td>0.023902</td>\n",
              "      <td>-0.030180</td>\n",
              "      <td>-0.109826</td>\n",
              "      <td>-0.007136</td>\n",
              "      <td>0.010615</td>\n",
              "      <td>-0.130222</td>\n",
              "      <td>0.060246</td>\n",
              "      <td>0.018799</td>\n",
              "      <td>0.041171</td>\n",
              "      <td>-0.042978</td>\n",
              "      <td>-0.100417</td>\n",
              "      <td>0.002826</td>\n",
              "      <td>-0.022114</td>\n",
              "      <td>0.091905</td>\n",
              "      <td>0.104999</td>\n",
              "      <td>0.053176</td>\n",
              "      <td>0.103732</td>\n",
              "      <td>0.007597</td>\n",
              "      <td>-0.075083</td>\n",
              "      <td>-0.075533</td>\n",
              "      <td>-0.038828</td>\n",
              "      <td>-0.067167</td>\n",
              "      <td>-0.031973</td>\n",
              "      <td>-0.017768</td>\n",
              "      <td>-0.016386</td>\n",
              "      <td>0.076248</td>\n",
              "      <td>0.097592</td>\n",
              "      <td>-0.021288</td>\n",
              "      <td>-0.032921</td>\n",
              "      <td>-0.080318</td>\n",
              "      <td>-0.045194</td>\n",
              "      <td>-0.029705</td>\n",
              "      <td>0.070133</td>\n",
              "      <td>-0.042634</td>\n",
              "      <td>0.110220</td>\n",
              "      <td>-0.058350</td>\n",
              "      <td>-0.008043</td>\n",
              "      <td>-0.026741</td>\n",
              "      <td>-0.097187</td>\n",
              "      <td>0.008345</td>\n",
              "      <td>-0.056997</td>\n",
              "      <td>0.042084</td>\n",
              "      <td>0.002141</td>\n",
              "      <td>0.113180</td>\n",
              "      <td>0.039530</td>\n",
              "      <td>0.011556</td>\n",
              "      <td>0.017370</td>\n",
              "      <td>-0.044596</td>\n",
              "      <td>0.148844</td>\n",
              "      <td>0.005778</td>\n",
              "      <td>-0.109985</td>\n",
              "      <td>-0.055013</td>\n",
              "      <td>0.192383</td>\n",
              "      <td>-0.185018</td>\n",
              "      <td>-0.281576</td>\n",
              "      <td>0.035360</td>\n",
              "      <td>0.036621</td>\n",
              "      <td>-0.051921</td>\n",
              "      <td>0.202962</td>\n",
              "      <td>-0.077555</td>\n",
              "      <td>0.138753</td>\n",
              "      <td>-0.000488</td>\n",
              "      <td>-0.112467</td>\n",
              "      <td>0.008789</td>\n",
              "      <td>0.086548</td>\n",
              "      <td>-0.017537</td>\n",
              "      <td>-0.016576</td>\n",
              "      <td>0.167806</td>\n",
              "      <td>0.097534</td>\n",
              "      <td>0.084310</td>\n",
              "      <td>-0.014735</td>\n",
              "      <td>-0.010167</td>\n",
              "      <td>-0.250326</td>\n",
              "      <td>-0.128662</td>\n",
              "      <td>0.031006</td>\n",
              "      <td>-0.015706</td>\n",
              "      <td>-0.000529</td>\n",
              "      <td>-0.028310</td>\n",
              "      <td>-0.072472</td>\n",
              "      <td>0.084961</td>\n",
              "      <td>0.069336</td>\n",
              "      <td>-0.080404</td>\n",
              "      <td>0.139648</td>\n",
              "      <td>-0.058757</td>\n",
              "      <td>0.031779</td>\n",
              "      <td>0.197917</td>\n",
              "      <td>-0.164307</td>\n",
              "      <td>-0.165527</td>\n",
              "      <td>-0.009644</td>\n",
              "      <td>-0.050802</td>\n",
              "      <td>-0.015483</td>\n",
              "      <td>0.161458</td>\n",
              "      <td>0.056152</td>\n",
              "      <td>0.212321</td>\n",
              "      <td>0.009603</td>\n",
              "      <td>-0.005981</td>\n",
              "      <td>-0.134684</td>\n",
              "      <td>0.041829</td>\n",
              "      <td>-0.030558</td>\n",
              "      <td>0.065135</td>\n",
              "      <td>0.083496</td>\n",
              "      <td>-0.033122</td>\n",
              "      <td>-0.150391</td>\n",
              "      <td>-0.036784</td>\n",
              "      <td>0.060628</td>\n",
              "      <td>-0.059448</td>\n",
              "      <td>-0.286133</td>\n",
              "      <td>0.100342</td>\n",
              "      <td>0.118652</td>\n",
              "      <td>-0.292969</td>\n",
              "      <td>0.177246</td>\n",
              "      <td>0.003947</td>\n",
              "      <td>0.078939</td>\n",
              "      <td>0.148112</td>\n",
              "      <td>0.104899</td>\n",
              "      <td>0.060750</td>\n",
              "      <td>-0.125783</td>\n",
              "      <td>-0.227214</td>\n",
              "      <td>-0.117188</td>\n",
              "      <td>0.101807</td>\n",
              "      <td>-0.015462</td>\n",
              "      <td>0.076742</td>\n",
              "      <td>-0.098429</td>\n",
              "      <td>-0.005371</td>\n",
              "      <td>-0.196940</td>\n",
              "      <td>0.052409</td>\n",
              "      <td>0.087565</td>\n",
              "      <td>0.036336</td>\n",
              "      <td>0.011210</td>\n",
              "      <td>-0.131632</td>\n",
              "      <td>0.231771</td>\n",
              "      <td>-0.025391</td>\n",
              "      <td>0.123372</td>\n",
              "      <td>-0.007812</td>\n",
              "      <td>0.032552</td>\n",
              "      <td>0.036051</td>\n",
              "      <td>-0.040873</td>\n",
              "      <td>-0.121901</td>\n",
              "      <td>-0.109619</td>\n",
              "      <td>-0.010986</td>\n",
              "      <td>-0.132243</td>\n",
              "      <td>0.078776</td>\n",
              "      <td>0.108236</td>\n",
              "      <td>-0.183594</td>\n",
              "      <td>-0.059082</td>\n",
              "      <td>-0.036458</td>\n",
              "      <td>-0.046834</td>\n",
              "      <td>-0.179036</td>\n",
              "      <td>-0.066406</td>\n",
              "      <td>0.287435</td>\n",
              "      <td>-0.077530</td>\n",
              "      <td>-0.069743</td>\n",
              "      <td>0.084635</td>\n",
              "      <td>-0.094401</td>\n",
              "      <td>0.005371</td>\n",
              "      <td>0.008952</td>\n",
              "      <td>-0.002523</td>\n",
              "      <td>0.162109</td>\n",
              "      <td>0.004069</td>\n",
              "      <td>0.034749</td>\n",
              "      <td>0.067301</td>\n",
              "      <td>-0.070964</td>\n",
              "      <td>0.266276</td>\n",
              "      <td>0.059570</td>\n",
              "      <td>0.169271</td>\n",
              "      <td>-0.036947</td>\n",
              "      <td>0.031087</td>\n",
              "      <td>-0.004395</td>\n",
              "      <td>0.021077</td>\n",
              "      <td>-0.034831</td>\n",
              "      <td>-0.013590</td>\n",
              "      <td>0.318522</td>\n",
              "      <td>0.002340</td>\n",
              "      <td>-0.027507</td>\n",
              "      <td>0.023763</td>\n",
              "      <td>-0.114583</td>\n",
              "      <td>0.016032</td>\n",
              "      <td>-0.018473</td>\n",
              "      <td>0.028117</td>\n",
              "      <td>-0.097036</td>\n",
              "      <td>0.104085</td>\n",
              "      <td>0.100830</td>\n",
              "      <td>0.004557</td>\n",
              "      <td>0.036947</td>\n",
              "      <td>-0.073242</td>\n",
              "      <td>0.021566</td>\n",
              "      <td>-0.020345</td>\n",
              "      <td>-0.042928</td>\n",
              "      <td>0.025635</td>\n",
              "      <td>0.100749</td>\n",
              "      <td>0.104655</td>\n",
              "      <td>0.051270</td>\n",
              "      <td>-0.124552</td>\n",
              "      <td>0.094828</td>\n",
              "      <td>-0.175130</td>\n",
              "      <td>-0.071289</td>\n",
              "      <td>0.093852</td>\n",
              "      <td>-0.017577</td>\n",
              "      <td>-0.076131</td>\n",
              "      <td>-0.134847</td>\n",
              "      <td>-0.106527</td>\n",
              "      <td>-0.150309</td>\n",
              "      <td>0.059814</td>\n",
              "      <td>0.095622</td>\n",
              "      <td>0.013957</td>\n",
              "      <td>-0.029419</td>\n",
              "      <td>0.024495</td>\n",
              "      <td>0.012858</td>\n",
              "      <td>-0.055339</td>\n",
              "      <td>0.105306</td>\n",
              "      <td>-0.168376</td>\n",
              "      <td>0.017904</td>\n",
              "      <td>0.119273</td>\n",
              "      <td>-0.107015</td>\n",
              "      <td>0.077230</td>\n",
              "      <td>-0.076497</td>\n",
              "      <td>0.011556</td>\n",
              "      <td>-0.031576</td>\n",
              "      <td>0.034218</td>\n",
              "      <td>0.094076</td>\n",
              "      <td>-0.037191</td>\n",
              "      <td>-0.110189</td>\n",
              "      <td>0.125326</td>\n",
              "      <td>-0.096354</td>\n",
              "      <td>-0.111328</td>\n",
              "      <td>-0.090078</td>\n",
              "      <td>0.041748</td>\n",
              "      <td>-0.097819</td>\n",
              "      <td>-0.074056</td>\n",
              "      <td>0.020996</td>\n",
              "      <td>-0.053996</td>\n",
              "      <td>-0.013509</td>\n",
              "      <td>0.075684</td>\n",
              "      <td>-0.029704</td>\n",
              "      <td>0.070638</td>\n",
              "      <td>0.033529</td>\n",
              "      <td>0.078430</td>\n",
              "      <td>-0.060221</td>\n",
              "      <td>-0.080119</td>\n",
              "      <td>-0.098307</td>\n",
              "      <td>-0.120443</td>\n",
              "      <td>-0.279948</td>\n",
              "      <td>0.012451</td>\n",
              "      <td>0.027098</td>\n",
              "      <td>0.236979</td>\n",
              "      <td>-0.106567</td>\n",
              "      <td>-0.060465</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.060628</td>\n",
              "      <td>-0.071940</td>\n",
              "      <td>-0.007487</td>\n",
              "      <td>0.128459</td>\n",
              "      <td>-0.131673</td>\n",
              "      <td>-0.094482</td>\n",
              "      <td>0.042969</td>\n",
              "      <td>-0.081706</td>\n",
              "      <td>0.113281</td>\n",
              "      <td>0.231608</td>\n",
              "      <td>0.115519</td>\n",
              "      <td>-0.002604</td>\n",
              "      <td>0.076335</td>\n",
              "      <td>-0.098633</td>\n",
              "      <td>0.107747</td>\n",
              "      <td>-0.080811</td>\n",
              "      <td>0.138041</td>\n",
              "      <td>-0.088867</td>\n",
              "      <td>0.149943</td>\n",
              "      <td>-0.010905</td>\n",
              "      <td>-0.085286</td>\n",
              "      <td>0.003571</td>\n",
              "      <td>-0.023254</td>\n",
              "      <td>-0.059326</td>\n",
              "      <td>0.008138</td>\n",
              "      <td>-0.047404</td>\n",
              "      <td>-0.026265</td>\n",
              "      <td>0.025690</td>\n",
              "      <td>-0.027140</td>\n",
              "      <td>-0.081476</td>\n",
              "      <td>0.021342</td>\n",
              "      <td>0.029785</td>\n",
              "      <td>0.120443</td>\n",
              "      <td>0.075928</td>\n",
              "      <td>0.002767</td>\n",
              "      <td>0.163737</td>\n",
              "      <td>-0.038086</td>\n",
              "      <td>0.090332</td>\n",
              "      <td>-0.117602</td>\n",
              "      <td>0.018311</td>\n",
              "      <td>-0.095133</td>\n",
              "      <td>-0.176758</td>\n",
              "      <td>-0.017741</td>\n",
              "      <td>-0.084595</td>\n",
              "      <td>-0.145833</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.052043</td>\n",
              "      <td>0.131673</td>\n",
              "      <td>0.049764</td>\n",
              "      <td>0.131673</td>\n",
              "      <td>-0.254720</td>\n",
              "      <td>-0.027100</td>\n",
              "      <td>0.117106</td>\n",
              "      <td>0.036296</td>\n",
              "      <td>0.013835</td>\n",
              "      <td>0.087402</td>\n",
              "      <td>0.067098</td>\n",
              "      <td>-0.161092</td>\n",
              "      <td>0.000814</td>\n",
              "      <td>0.129415</td>\n",
              "      <td>-0.006022</td>\n",
              "      <td>-0.108398</td>\n",
              "      <td>0.077311</td>\n",
              "      <td>-0.001719</td>\n",
              "      <td>-0.027588</td>\n",
              "      <td>-0.098470</td>\n",
              "      <td>0.118734</td>\n",
              "      <td>-0.090413</td>\n",
              "      <td>0.101725</td>\n",
              "      <td>0.167603</td>\n",
              "      <td>-0.149414</td>\n",
              "      <td>-0.102702</td>\n",
              "      <td>-0.027995</td>\n",
              "      <td>-0.094971</td>\n",
              "      <td>0.167358</td>\n",
              "      <td>0.140951</td>\n",
              "      <td>0.171061</td>\n",
              "      <td>0.255534</td>\n",
              "      <td>0.073405</td>\n",
              "      <td>-0.000142</td>\n",
              "      <td>-0.036770</td>\n",
              "      <td>0.067301</td>\n",
              "      <td>0.091471</td>\n",
              "      <td>-0.047038</td>\n",
              "      <td>0.131673</td>\n",
              "      <td>-0.112600</td>\n",
              "      <td>0.172119</td>\n",
              "      <td>-0.132894</td>\n",
              "      <td>0.080322</td>\n",
              "      <td>0.004364</td>\n",
              "      <td>0.045898</td>\n",
              "      <td>0.159180</td>\n",
              "      <td>-0.122965</td>\n",
              "      <td>0.052831</td>\n",
              "      <td>-0.096802</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 319
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "algos = {\n",
        "  'RF' : RandomForestRegressor(n_estimators=50,random_state=1,n_jobs=-1),\n",
        "  'KNN' : KNeighborsRegressor(n_neighbors=5,n_jobs=-1,metric='cosine'),\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(20,10),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models_multioutput(rev_a_title_train_wv_google, y_train, rev_a_title_test_wv_google, y_test.to_numpy(), algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Z0jWdOigxJN5",
        "outputId": "cdd3ef8a-7178-4292-c957-e2550af7eb83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## RF #############\n",
            "Note: MAE = 1.245, Accuracy = 0.186, Customized Accuracy = 0.600\n",
            "Utilité: MAE = 2.966, Accuracy = 0.097, Customized Accuracy = 0.440, Binary Thresholding Score = 0.602\n",
            "Les deux: Accuracy = 0.016, Customized Accuracy = 0.253\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,  529, 3729,  328,    0],\n",
              "       [   0,  186, 2503,  369,    0],\n",
              "       [   0,  105, 2588,  613,    0],\n",
              "       [   0,   84, 2813, 1106,    0],\n",
              "       [   0,   57, 3803, 2102,    1]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,    0,    8,  113,  909, 1747, 1045,  298,   26,    0,    0],\n",
              "       [   0,    0,    0,    3,   55,  141,   94,   25,    6,    0,    0],\n",
              "       [   0,    0,    0,   10,  134,  311,  231,   78,    4,    0,    0],\n",
              "       [   0,    0,    0,   14,  152,  368,  358,  148,   16,    0,    0],\n",
              "       [   0,    0,    0,    4,   57,  171,  201,   75,   12,    0,    0],\n",
              "       [   0,    0,    0,   29,  366, 1050, 1049,  416,   39,    0,    0],\n",
              "       [   0,    0,    0,    8,   83,  261,  397,  201,   22,    0,    0],\n",
              "       [   0,    0,    0,    8,  172,  638,  813,  420,   52,    0,    0],\n",
              "       [   0,    0,    0,   12,  180,  637,  907,  563,   86,    1,    0],\n",
              "       [   0,    0,    0,    4,   81,  368,  694,  524,   98,    0,    0],\n",
              "       [   0,    0,    0,   28,  475, 1259, 1382,  681,   97,    1,    0]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## KNN #############\n",
            "Note: MAE = 1.332, Accuracy = 0.195, Customized Accuracy = 0.597\n",
            "Utilité: MAE = 3.177, Accuracy = 0.090, Customized Accuracy = 0.442, Binary Thresholding Score = 0.537\n",
            "Les deux: Accuracy = 0.017, Customized Accuracy = 0.261\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[ 108, 1068, 2162, 1164,   84],\n",
              "       [  48,  599, 1467,  875,   69],\n",
              "       [  31,  540, 1586, 1040,  109],\n",
              "       [  38,  543, 1800, 1488,  134],\n",
              "       [  37,  780, 2530, 2312,  304]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  0,  55, 164, 282, 518, 782, 922, 781, 449, 166,  27],\n",
              "       [  0,   1,   5,  16,  37,  58,  83,  68,  41,  15,   0],\n",
              "       [  0,   2,   8,  36,  91, 149, 187, 160, 110,  24,   1],\n",
              "       [  0,   1,  11,  45, 112, 196, 244, 256, 155,  35,   1],\n",
              "       [  0,   2,   4,  26,  57,  92, 122, 118,  70,  27,   2],\n",
              "       [  0,  23,  37, 120, 319, 546, 745, 651, 397, 101,  10],\n",
              "       [  0,   1,   6,  42,  93, 182, 257, 215, 123,  52,   1],\n",
              "       [  0,  10,  34,  83, 210, 394, 501, 494, 274,  99,   4],\n",
              "       [  0,   5,  37,  78, 222, 418, 564, 587, 350, 114,  11],\n",
              "       [  0,   3,  20,  58, 139, 319, 419, 430, 273, 100,   8],\n",
              "       [  0,  32,  67, 170, 411, 676, 910, 900, 538, 205,  14]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## MLP #############\n",
            "Note: MAE = 1.028, Accuracy = 0.276, Customized Accuracy = 0.757\n",
            "Utilité: MAE = 3.148, Accuracy = 0.089, Customized Accuracy = 0.461, Binary Thresholding Score = 0.568\n",
            "Les deux: Accuracy = 0.024, Customized Accuracy = 0.346\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[ 630, 1976, 1533,  432,   15],\n",
              "       [ 110,  990, 1396,  542,   20],\n",
              "       [  45,  635, 1542, 1013,   71],\n",
              "       [  16,  343, 1491, 1948,  205],\n",
              "       [  14,  322, 1725, 3231,  671]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  0, 360, 308, 411, 537, 616, 693, 686, 353, 125,  57],\n",
              "       [  0,  27,  19,  32,  61,  48,  58,  47,  19,   5,   8],\n",
              "       [  0,  42,  44,  61,  98, 119, 157, 142,  71,  21,  13],\n",
              "       [  0,  46,  48,  95, 133, 170, 226, 192,  93,  40,  13],\n",
              "       [  0,  20,  20,  38,  66,  97, 103,  99,  54,  19,   4],\n",
              "       [  0, 136, 115, 220, 322, 460, 611, 608, 338, 100,  39],\n",
              "       [  0,  26,  33,  62,  97, 154, 206, 226, 127,  28,  13],\n",
              "       [  0,  70,  83, 115, 230, 335, 433, 457, 280,  76,  24],\n",
              "       [  0,  66,  77, 127, 190, 327, 490, 615, 355, 103,  36],\n",
              "       [  0,  37,  37,  66, 128, 213, 392, 502, 292,  80,  22],\n",
              "       [  0, 188, 153, 242, 428, 554, 782, 856, 490, 162,  68]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pas d'amélioration par rapport à son équivalent dans la partie précédente."
      ],
      "metadata": {
        "id": "cr6e6YMNFkpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Courbe d'apprentissage du meilleur modèle"
      ],
      "metadata": {
        "id": "_FUCwp-6xoz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "algos = {\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(20,10),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models_multioutput(rev_a_title_train_wv_google.iloc[:int(y_train.shape[0] * 0.8)], y_train.iloc[:int(y_train.shape[0] * 0.8)], rev_a_title_test_wv_google, y_test.to_numpy(), algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "W5xmk-FOxxPN",
        "outputId": "79599919-60e1-41fd-dfe9-3be7e3e09d73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## MLP #############\n",
            "Note: MAE = 1.046, Accuracy = 0.266, Customized Accuracy = 0.741\n",
            "Utilité: MAE = 3.204, Accuracy = 0.091, Customized Accuracy = 0.456, Binary Thresholding Score = 0.564\n",
            "Les deux: Accuracy = 0.022, Customized Accuracy = 0.333\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[ 551, 1894, 1787,  341,   13],\n",
              "       [ 117,  948, 1565,  410,   18],\n",
              "       [  39,  639, 1746,  813,   69],\n",
              "       [  19,  314, 1842, 1569,  259],\n",
              "       [  10,  288, 2107, 2809,  749]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  0, 367, 258, 390, 511, 652, 697, 599, 371, 169, 132],\n",
              "       [  0,  16,  12,  27,  50,  51,  63,  51,  28,  16,  10],\n",
              "       [  0,  39,  27,  53,  97, 134, 153, 130,  83,  27,  25],\n",
              "       [  0,  46,  49,  80, 109, 166, 241, 205, 104,  32,  24],\n",
              "       [  0,  15,  11,  45,  50,  89, 119,  94,  65,  21,  11],\n",
              "       [  0, 151, 140, 190, 328, 455, 602, 539, 332, 131,  81],\n",
              "       [  0,  31,  28,  50,  96, 172, 186, 227, 112,  44,  26],\n",
              "       [  0,  67,  74, 136, 187, 316, 407, 476, 247, 115,  78],\n",
              "       [  0,  82,  74, 127, 200, 285, 509, 527, 323, 153, 106],\n",
              "       [  0,  52,  29,  71, 128, 219, 372, 424, 275, 128,  71],\n",
              "       [  0, 214, 163, 262, 364, 536, 733, 747, 500, 245, 159]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le modèle peut probablement mieux apprendre avec un peu plus de données mais c'est assez stable."
      ],
      "metadata": {
        "id": "7p1dC6Gax9s0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Amélioration du meilleur modèle"
      ],
      "metadata": {
        "id": "RE5-H525y_NY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Moins de neurones\n",
        "algos = {\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(10, 5),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models_multioutput(rev_a_title_train_wv_google, y_train, rev_a_title_test_wv_google, y_test.to_numpy(), algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "Yg4B69zTzAEr",
        "outputId": "4460a4bd-43cb-449f-c796-c92fa12d9599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## MLP #############\n",
            "Note: MAE = 1.049, Accuracy = 0.264, Customized Accuracy = 0.741\n",
            "Utilité: MAE = 3.043, Accuracy = 0.091, Customized Accuracy = 0.441, Binary Thresholding Score = 0.576\n",
            "Les deux: Accuracy = 0.022, Customized Accuracy = 0.319\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[ 540, 1831, 1847,  351,   17],\n",
              "       [ 124,  842, 1658,  415,   19],\n",
              "       [  40,  550, 1798,  861,   57],\n",
              "       [  13,  308, 1798, 1679,  205],\n",
              "       [   9,  245, 2088, 2956,  665]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,   69,  134,  376,  720, 1121, 1053,  529,  125,   19,    0],\n",
              "       [   0,    3,    8,   22,   53,   90,   87,   40,   17,    3,    1],\n",
              "       [   0,    7,   18,   56,  121,  205,  205,  121,   33,    1,    1],\n",
              "       [   0,   10,   20,   44,  156,  263,  307,  188,   59,    9,    0],\n",
              "       [   0,    2,   10,   23,   62,  134,  162,   98,   22,    7,    0],\n",
              "       [   0,   19,   47,  161,  373,  775,  867,  545,  137,   23,    2],\n",
              "       [   0,    5,    5,   36,   90,  243,  313,  211,   55,    8,    6],\n",
              "       [   0,   20,   18,   70,  217,  472,  690,  462,  133,   17,    4],\n",
              "       [   0,   11,   21,   85,  214,  501,  758,  582,  186,   23,    5],\n",
              "       [   0,    1,   12,   35,  111,  350,  589,  479,  163,   28,    1],\n",
              "       [   0,   27,   52,  177,  454,  978, 1223,  748,  228,   30,    6]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pas d'amélioration"
      ],
      "metadata": {
        "id": "WKo9fsrbzIRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plus de neurones\n",
        "algos = {\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(40, 20),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models_multioutput(rev_a_title_train_wv_google, y_train, rev_a_title_test_wv_google, y_test.to_numpy(), algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "IFKSCN-CzMdY",
        "outputId": "7866c1f9-d11b-4202-a6bb-3d132b8d908d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## MLP #############\n",
            "Note: MAE = 1.033, Accuracy = 0.275, Customized Accuracy = 0.755\n",
            "Utilité: MAE = 3.322, Accuracy = 0.089, Customized Accuracy = 0.451, Binary Thresholding Score = 0.546\n",
            "Les deux: Accuracy = 0.023, Customized Accuracy = 0.336\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[ 641, 2067, 1470,  387,   21],\n",
              "       [ 121, 1091, 1341,  486,   19],\n",
              "       [  37,  709, 1573,  928,   59],\n",
              "       [  21,  400, 1631, 1732,  219],\n",
              "       [  20,  363, 1839, 3036,  705]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  0, 663, 296, 350, 431, 523, 565, 525, 364, 220, 209],\n",
              "       [  0,  51,  23,  20,  32,  34,  62,  45,  31,  14,  12],\n",
              "       [  0,  80,  39,  70,  78, 110, 123, 131,  62,  52,  23],\n",
              "       [  0, 109,  55,  70,  91, 141, 207, 176, 118,  57,  32],\n",
              "       [  0,  36,  21,  41,  53,  74,  96,  93,  65,  23,  18],\n",
              "       [  0, 274, 137, 219, 293, 412, 511, 478, 316, 174, 135],\n",
              "       [  0,  63,  42,  59,  92, 143, 204, 180, 100,  58,  31],\n",
              "       [  0, 150, 100, 140, 181, 288, 358, 383, 271, 138,  94],\n",
              "       [  0, 144, 105, 138, 201, 302, 434, 436, 311, 187, 128],\n",
              "       [  0,  89,  75,  93, 145, 252, 285, 344, 255, 135,  96],\n",
              "       [  0, 362, 182, 296, 386, 524, 624, 575, 446, 317, 211]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pas d'amélioration non plus."
      ],
      "metadata": {
        "id": "818p-yVk0Sgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### II. Prise en compte de la longueur, de la spécificité et de la subjectivité du texte"
      ],
      "metadata": {
        "id": "1N-OB6Zc37rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul de la longueur, la spécificité et la subjectivité\n",
        "df_meta_train=pd.DataFrame([])\n",
        "df_meta_train['length_text'] = [len(reviews_train_tokens.iloc[i]) for i in range(len(reviews_train_tokens))]\n",
        "df_meta_train['polarity'] = [polarity_txt(X_train[\"reviewText\"].iloc[i]) for i in range(len(X_train))]\n",
        "df_meta_train['subjectivity'] = [subj_txt(X_train[\"reviewText\"].iloc[i]) for i in range(len(X_train))]\n",
        "\n",
        "df_meta_test=pd.DataFrame([])\n",
        "df_meta_test['length_text'] = [len(reviews_test_tokens.iloc[i]) for i in range(len(reviews_test_tokens))]\n",
        "df_meta_test['polarity'] = [polarity_txt(X_test[\"reviewText\"].iloc[i]) for i in range(len(X_test))]\n",
        "df_meta_test['subjectivity'] = [subj_txt(X_test[\"reviewText\"].iloc[i]) for i in range(len(X_test))]"
      ],
      "metadata": {
        "id": "41R2biP70t5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concaténation\n",
        "corpus_train_wv_google_meta=pd.concat([reviews_train_wv_google,title_train_wv_google,df_meta_train],axis=1)\n",
        "corpus_test_wv_google_meta=pd.concat([reviews_test_wv_google,title_test_wv_google,df_meta_test],axis=1)"
      ],
      "metadata": {
        "id": "XpCCF-wG0xm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entraînement et évaluation\n",
        "algos = {\n",
        "  'RF' : RandomForestRegressor(n_estimators=50,random_state=1,n_jobs=-1),\n",
        "  'KNN' : KNeighborsRegressor(n_neighbors=5,n_jobs=-1,metric='cosine'),\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(20,10),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models_multioutput(corpus_train_wv_google_meta,y_train,corpus_test_wv_google_meta,y_test.to_numpy(),algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WE8-yF9X01r7",
        "outputId": "d1c1c84f-bb5d-4b73-8657-0c9ef14bf3a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## RF #############\n",
            "Note: MAE = 1.105, Accuracy = 0.226, Customized Accuracy = 0.731\n",
            "Utilité: MAE = 2.941, Accuracy = 0.095, Customized Accuracy = 0.445, Binary Thresholding Score = 0.611\n",
            "Les deux: Accuracy = 0.019, Customized Accuracy = 0.319\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  18, 2179, 1839,  550,    0],\n",
              "       [   2,  957, 1509,  590,    0],\n",
              "       [   2,  544, 1659, 1099,    2],\n",
              "       [   0,  342, 1594, 2062,    5],\n",
              "       [   1,  377, 1932, 3622,   31]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,    2,   33,  246, 1128, 1352,  997,  344,   44,    0,    0],\n",
              "       [   0,    0,    0,    8,   70,  121,   89,   32,    4,    0,    0],\n",
              "       [   0,    0,    0,   19,  152,  254,  261,   74,    8,    0,    0],\n",
              "       [   0,    0,    1,   18,  165,  329,  349,  168,   26,    0,    0],\n",
              "       [   0,    0,    0,   14,   70,  150,  189,   85,   12,    0,    0],\n",
              "       [   0,    0,    4,   75,  447,  911,  960,  495,   57,    0,    0],\n",
              "       [   0,    0,    0,   11,   79,  242,  377,  228,   35,    0,    0],\n",
              "       [   0,    0,    2,   36,  221,  523,  769,  481,   71,    0,    0],\n",
              "       [   0,    0,    0,   32,  184,  539,  874,  636,  121,    0,    0],\n",
              "       [   0,    0,    1,    8,   89,  285,  687,  573,  125,    1,    0],\n",
              "       [   0,    0,    7,   86,  479, 1128, 1331,  760,  131,    1,    0]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## KNN #############\n",
            "Note: MAE = 1.252, Accuracy = 0.219, Customized Accuracy = 0.637\n",
            "Utilité: MAE = 3.046, Accuracy = 0.097, Customized Accuracy = 0.466, Binary Thresholding Score = 0.583\n",
            "Les deux: Accuracy = 0.020, Customized Accuracy = 0.294\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[ 205, 1183, 2020, 1094,   84],\n",
              "       [  66,  643, 1397,  871,   81],\n",
              "       [  40,  479, 1509, 1150,  128],\n",
              "       [  16,  432, 1671, 1620,  264],\n",
              "       [  33,  501, 2299, 2532,  598]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  0, 120, 221, 394, 615, 823, 846, 610, 379, 122,  16],\n",
              "       [  0,   5,   8,  19,  45,  64,  71,  66,  33,  11,   2],\n",
              "       [  0,   6,  23,  55,  92, 171, 158, 153,  79,  29,   2],\n",
              "       [  0,   9,  39,  67, 115, 183, 242, 209, 142,  48,   2],\n",
              "       [  0,   2,  13,  20,  60,  94, 117, 124,  67,  20,   3],\n",
              "       [  0,  31,  79, 168, 328, 592, 647, 594, 363, 136,  11],\n",
              "       [  0,   5,  18,  40,  91, 158, 233, 216, 152,  55,   4],\n",
              "       [  0,  21,  30,  94, 176, 351, 518, 463, 330, 114,   6],\n",
              "       [  0,  10,  29,  80, 200, 343, 532, 601, 430, 155,   6],\n",
              "       [  0,   6,   9,  40, 120, 238, 406, 469, 334, 135,  12],\n",
              "       [  0,  39, 106, 211, 369, 632, 853, 861, 592, 243,  17]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## MLP #############\n",
            "Note: MAE = 0.983, Accuracy = 0.297, Customized Accuracy = 0.778\n",
            "Utilité: MAE = 3.035, Accuracy = 0.091, Customized Accuracy = 0.475, Binary Thresholding Score = 0.592\n",
            "Les deux: Accuracy = 0.026, Customized Accuracy = 0.368\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[ 620, 2116, 1446,  387,   17],\n",
              "       [ 125, 1047, 1336,  525,   25],\n",
              "       [  42,  606, 1534, 1032,   92],\n",
              "       [  13,  303, 1340, 1966,  381],\n",
              "       [  11,  260, 1529, 3122, 1041]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,  391,  269,  309,  488,  705,  954,  614,  223,   83,  110],\n",
              "       [   0,   27,   18,   19,   42,   74,   84,   40,   11,    5,    4],\n",
              "       [   0,   52,   37,   50,   86,  157,  218,  124,   27,    7,   10],\n",
              "       [   0,   46,   27,   66,  102,  205,  330,  211,   51,    9,    9],\n",
              "       [   0,   19,   18,   19,   61,   88,  161,  109,   31,    9,    5],\n",
              "       [   0,  132,  109,  153,  294,  504,  830,  651,  185,   51,   40],\n",
              "       [   0,   19,   23,   40,   80,  147,  319,  262,   59,   14,    9],\n",
              "       [   0,   65,   46,   74,  153,  330,  651,  557,  176,   29,   22],\n",
              "       [   0,   46,   41,   77,  138,  336,  765,  700,  218,   47,   18],\n",
              "       [   0,   29,   19,   31,   87,  199,  481,  652,  212,   43,   16],\n",
              "       [   0,  155,  125,  168,  320,  580, 1077, 1024,  329,   82,   63]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pas d'amélioration par rapport à leurs équivalents dans la partie précédente."
      ],
      "metadata": {
        "id": "e_B6MpKV1AR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### III. Prise en compte de l'ordre des mots (LSTM)"
      ],
      "metadata": {
        "id": "zLt-yMWocgCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concaténation des reviews et des titres\n",
        "title_reviews_train_tokens = pd.Series([title_train_tokens[i] + reviews_train_tokens[i] for i in reviews_train_tokens.index])\n",
        "title_reviews_test_tokens = pd.Series([title_test_tokens[i] + reviews_test_tokens[i] for i in reviews_test_tokens.index])"
      ],
      "metadata": {
        "id": "pcCgJrUL1HAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_sequences = [to_sequence(word2idx, x) for x in title_reviews_train_tokens]\n",
        "X_test_sequences = [to_sequence(word2idx, x) for x in  title_reviews_test_tokens]"
      ],
      "metadata": {
        "id": "pBl4fQK01MJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGHT=75\n",
        "N_FEATURES = len(model_google_vec_neg_300.index_to_key)\n",
        "X_train_sequences = pad_sequences(X_train_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)\n",
        "X_test_sequences = pad_sequences(X_test_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)"
      ],
      "metadata": {
        "id": "789fv_S51PNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDINGS_LEN = model_google_vec_neg_300.vector_size\n",
        "embeddings_index = np.zeros((len(model_google_vec_neg_300.index_to_key)+1, EMBEDDINGS_LEN))\n",
        "embeddings_index.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzy42Kmc1VDL",
        "outputId": "d8361f4d-84a3-40b7-9675-2f0baa536de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000001, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 330
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word, idx in word2idx.items():\n",
        "    try:\n",
        "        embedding = model_google_vec_neg_300[word]\n",
        "        embeddings_index[idx] = embedding\n",
        "    except:\n",
        "        pass\n",
        "      \n",
        "print(\"EMBEDDINGS_LEN=\", EMBEDDINGS_LEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKb6VHGc1Yr2",
        "outputId": "0b6133cc-8aba-4df2-8953-d75b1eb202bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EMBEDDINGS_LEN= 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(len(model_google_vec_neg_300.key_to_index)+1,\n",
        "                    EMBEDDINGS_LEN,  # Embedding size\n",
        "                    weights=[embeddings_index],\n",
        "                    trainable=False))\n",
        "\n",
        "model_lstm.add(LSTM(300, dropout=0.2))\n",
        "\n",
        "model_lstm.add(Dense(2))\n",
        "model_lstm.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "model_lstm.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D61zaNL1azU",
        "outputId": "7a8565a8-a6b4-47a3-d0b8-c149286fa76e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_7 (Embedding)     (None, None, 300)         900000300 \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 300)               721200    \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 2)                 602       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 900,722,102\n",
            "Trainable params: 721,802\n",
            "Non-trainable params: 900,000,300\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_lstm.fit(X_train_sequences, y_train, epochs=10, batch_size=128, verbose=1,validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7MotGW61nSR",
        "outputId": "b5811455-04b6-4ccf-cd48-cc00616850ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "589/589 [==============================] - 133s 224ms/step - loss: 2.0468 - mean_absolute_error: 2.0468 - val_loss: 1.9625 - val_mean_absolute_error: 1.9625\n",
            "Epoch 2/10\n",
            "589/589 [==============================] - 121s 205ms/step - loss: 1.9402 - mean_absolute_error: 1.9402 - val_loss: 1.9225 - val_mean_absolute_error: 1.9225\n",
            "Epoch 3/10\n",
            "589/589 [==============================] - 119s 203ms/step - loss: 1.9052 - mean_absolute_error: 1.9052 - val_loss: 1.9117 - val_mean_absolute_error: 1.9117\n",
            "Epoch 4/10\n",
            "589/589 [==============================] - 118s 201ms/step - loss: 1.8840 - mean_absolute_error: 1.8840 - val_loss: 1.8870 - val_mean_absolute_error: 1.8870\n",
            "Epoch 5/10\n",
            "589/589 [==============================] - 117s 199ms/step - loss: 1.8649 - mean_absolute_error: 1.8649 - val_loss: 1.8831 - val_mean_absolute_error: 1.8831\n",
            "Epoch 6/10\n",
            "589/589 [==============================] - 119s 203ms/step - loss: 1.8444 - mean_absolute_error: 1.8444 - val_loss: 1.8771 - val_mean_absolute_error: 1.8771\n",
            "Epoch 7/10\n",
            "589/589 [==============================] - 119s 201ms/step - loss: 1.8256 - mean_absolute_error: 1.8256 - val_loss: 1.8687 - val_mean_absolute_error: 1.8687\n",
            "Epoch 8/10\n",
            "589/589 [==============================] - 119s 202ms/step - loss: 1.8225 - mean_absolute_error: 1.8225 - val_loss: 1.8833 - val_mean_absolute_error: 1.8833\n",
            "Epoch 9/10\n",
            "589/589 [==============================] - 119s 202ms/step - loss: 1.7951 - mean_absolute_error: 1.7951 - val_loss: 1.8677 - val_mean_absolute_error: 1.8677\n",
            "Epoch 10/10\n",
            "589/589 [==============================] - 118s 201ms/step - loss: 1.7707 - mean_absolute_error: 1.7707 - val_loss: 1.8803 - val_mean_absolute_error: 1.8803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores_lstm = model_lstm.evaluate(X_test_sequences, y_test, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CSQUbNE1sb-",
        "outputId": "db353d01-6585-4193-9347-2464aaa39f2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "654/654 [==============================] - 24s 37ms/step - loss: 1.8650 - mean_absolute_error: 1.8650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction=model_lstm.predict(X_test_sequences)\n",
        "\n",
        "ytest_overall = y_test.iloc[:,0].to_numpy()\n",
        "pred_overall = prediction[:,0]\n",
        "pred_overall[pred_overall < 1] = 1\n",
        "pred_overall[pred_overall > 5] = 5\n",
        "        \n",
        "ytest_helpful = y_test.iloc[:,1].to_numpy()\n",
        "pred_helpful = prediction[:,1]\n",
        "pred_helpful[pred_helpful < 1] = 1\n",
        "pred_helpful[pred_helpful > 10] = 10\n",
        "        \n",
        "OMAE=mean_absolute_error(ytest_overall,pred_overall)\n",
        "OACC=accuracy_score(ytest_overall,np.round(pred_overall))\n",
        "OCACC=customized_accuracy(ytest_overall,np.round(pred_overall),1)\n",
        "\n",
        "HMAE=mean_absolute_error(ytest_helpful,pred_helpful)\n",
        "HACC=accuracy_score(ytest_helpful,np.round(pred_helpful))\n",
        "HCACC=customized_accuracy(ytest_helpful,np.round(pred_helpful),2)\n",
        "HBTS=binary_tresholding_score(ytest_helpful,np.round(pred_helpful))\n",
        "\n",
        "ACC=customized_accuracy_score_double(ytest_overall,np.round(pred_overall),ytest_helpful,np.round(pred_helpful))\n",
        "CACC=customized_accuracy_score_double(ytest_overall,np.round(pred_overall),ytest_helpful,np.round(pred_helpful),1,2)\n",
        "\n",
        "print('Note: MAE = {0:.3f}, Accuracy = {1:.3f}, Customized Accuracy = {2:.3f}'.format(OMAE,OACC,OCACC))\n",
        "print('Utilité: MAE = {0:.3f}, Accuracy = {1:.3f}, Customized Accuracy = {2:.3f}, Binary Thresholding Score = {3:.3f}'.format(HMAE,HACC,HCACC,HBTS))\n",
        "print('Les deux: Accuracy = {0:.3f}, Customized Accuracy = {1:.3f}'.format(ACC,CACC))\n",
        "display(confusion_matrix(ytest_overall,np.round(pred_overall)))\n",
        "display(confusion_matrix(ytest_helpful,np.round(pred_helpful)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "gz-RP3JU10gS",
        "outputId": "d0646e09-bf67-43d5-d438-3035e6c46cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "654/654 [==============================] - 24s 36ms/step\n",
            "Note: MAE = 0.843, Accuracy = 0.431, Customized Accuracy = 0.813\n",
            "Utilité: MAE = 2.878, Accuracy = 0.105, Customized Accuracy = 0.517, Binary Thresholding Score = 0.617\n",
            "Les deux: Accuracy = 0.046, Customized Accuracy = 0.424\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[2248, 1126,  657,  437,  118],\n",
              "       [ 623,  937,  826,  549,  123],\n",
              "       [ 256,  656, 1020, 1062,  312],\n",
              "       [  87,  272,  778, 1713, 1153],\n",
              "       [  89,  270,  731, 1780, 3093]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  0, 443, 231, 242, 502, 790, 844, 654, 390,  49,   1],\n",
              "       [  0,   4,   9,  29,  64,  81,  71,  45,  20,   1,   0],\n",
              "       [  0,  18,  18,  58, 103, 180, 152, 145,  89,   5,   0],\n",
              "       [  0,  26,  13,  49, 136, 199, 251, 228, 141,  12,   1],\n",
              "       [  0,   3,  11,  15,  55, 109, 125, 118,  79,   5,   0],\n",
              "       [  0,  94,  66, 129, 292, 523, 628, 709, 454,  53,   1],\n",
              "       [  0,   9,   6,  19,  77, 134, 212, 276, 213,  26,   0],\n",
              "       [  0,  35,  23,  51, 153, 295, 452, 556, 482,  55,   1],\n",
              "       [  0,  27,  29,  47, 123, 293, 462, 653, 670,  80,   2],\n",
              "       [  0,  19,  11,  17,  39, 137, 281, 510, 641, 113,   1],\n",
              "       [  0, 158,  80, 111, 299, 543, 831, 897, 853, 149,   2]])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pas d'amélioration par rapport à son équivalent de la partie précédente."
      ],
      "metadata": {
        "id": "iP8RxWJIGcAB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### IV. Utilisation de la sortie du LSTM dans d'autres modèles seupervisés"
      ],
      "metadata": {
        "id": "dxEFEFDqvbh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "Model_rf = Model(inputs=model_lstm.inputs, outputs=model_lstm.layers[1].output)\n",
        "#Model_rf.save('../models/model_lstm_features.h5')\n",
        "corpus_train_lstm = Model_rf.predict(X_train_sequences)\n",
        "corpus_test_lstm = Model_rf.predict(X_test_sequences)"
      ],
      "metadata": {
        "id": "bKA3OGOmJPTV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0206ee33-3e31-4282-a285-400ea9d17ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2615/2615 [==============================] - 97s 37ms/step\n",
            "654/654 [==============================] - 24s 37ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "algos = {\n",
        "  'RF' : RandomForestRegressor(n_estimators=50,random_state=1,n_jobs=-1),\n",
        "  'KNN' : KNeighborsRegressor(n_neighbors=5,n_jobs=-1,metric='cosine'),\n",
        "  'MLP' : MLPRegressor(hidden_layer_sizes=(20,10),max_iter=800,random_state=1,alpha=0.001)\n",
        "}\n",
        "\n",
        "run_models_multioutput(corpus_train_lstm,y_train,corpus_test_lstm,y_test.to_numpy(),algos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XcZzu-6o2Azz",
        "outputId": "5946e636-c58c-48ac-f927-cd5b4bc648fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################## RF #############\n",
            "Note: MAE = 0.908, Accuracy = 0.332, Customized Accuracy = 0.823\n",
            "Utilité: MAE = 2.937, Accuracy = 0.093, Customized Accuracy = 0.463, Binary Thresholding Score = 0.606\n",
            "Les deux: Accuracy = 0.030, Customized Accuracy = 0.380\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[1011, 2235,  887,  426,   27],\n",
              "       [ 194, 1273, 1029,  535,   27],\n",
              "       [  62,  797, 1293, 1092,   62],\n",
              "       [  22,  317, 1126, 2172,  366],\n",
              "       [  16,  327,  992, 3434, 1194]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,   62,  185,  427,  821, 1066,  961,  500,  123,    1,    0],\n",
              "       [   0,    0,    2,   27,   80,  105,   76,   28,    6,    0,    0],\n",
              "       [   0,    4,   10,   46,  166,  209,  208,  104,   21,    0,    0],\n",
              "       [   0,    4,   11,   54,  185,  290,  306,  169,   37,    0,    0],\n",
              "       [   0,    0,    4,   18,   74,  144,  164,   90,   25,    1,    0],\n",
              "       [   0,    9,   42,  170,  438,  730,  856,  577,  127,    0,    0],\n",
              "       [   0,    1,    5,   27,   93,  218,  345,  233,   50,    0,    0],\n",
              "       [   0,    3,   17,   66,  239,  461,  663,  507,  145,    2,    0],\n",
              "       [   0,    5,   12,   42,  224,  478,  753,  666,  203,    3,    0],\n",
              "       [   0,    5,    4,   25,   79,  267,  546,  590,  239,   14,    0],\n",
              "       [   0,   18,   66,  172,  463,  878, 1161,  892,  261,   12,    0]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## KNN #############\n",
            "Note: MAE = 0.909, Accuracy = 0.379, Customized Accuracy = 0.801\n",
            "Utilité: MAE = 3.072, Accuracy = 0.094, Customized Accuracy = 0.471, Binary Thresholding Score = 0.579\n",
            "Les deux: Accuracy = 0.034, Customized Accuracy = 0.378\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[1600, 1597,  842,  426,  121],\n",
              "       [ 411, 1061,  933,  547,  106],\n",
              "       [ 202,  731, 1155,  951,  267],\n",
              "       [  66,  315,  995, 1785,  842],\n",
              "       [  69,  329,  873, 2367, 2325]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  0, 203, 293, 404, 604, 693, 729, 646, 385, 168,  21],\n",
              "       [  0,   2,  13,  37,  65,  65,  64,  41,  29,   7,   1],\n",
              "       [  0,  18,  32,  68, 100, 160, 154, 143,  66,  23,   4],\n",
              "       [  0,  16,  44,  83, 118, 216, 210, 199, 129,  36,   5],\n",
              "       [  0,   2,  18,  32,  72, 104, 124,  94,  55,  15,   4],\n",
              "       [  0,  71, 107, 257, 336, 517, 593, 571, 349, 138,  10],\n",
              "       [  0,   6,  19,  54,  80, 168, 217, 222, 161,  44,   1],\n",
              "       [  0,  19,  66, 103, 220, 341, 438, 443, 331, 127,  15],\n",
              "       [  0,  24,  42, 104, 218, 357, 530, 546, 402, 151,  12],\n",
              "       [  0,  12,  28,  58, 113, 243, 326, 446, 366, 167,  10],\n",
              "       [  0,  91, 131, 233, 442, 618, 783, 796, 517, 281,  31]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################## MLP #############\n",
            "Note: MAE = 0.873, Accuracy = 0.372, Customized Accuracy = 0.828\n",
            "Utilité: MAE = 2.954, Accuracy = 0.093, Customized Accuracy = 0.485, Binary Thresholding Score = 0.599\n",
            "Les deux: Accuracy = 0.036, Customized Accuracy = 0.401\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[1198, 2034,  880,  427,   47],\n",
              "       [ 248, 1181, 1058,  528,   43],\n",
              "       [  87,  735, 1260, 1097,  127],\n",
              "       [  19,  274,  983, 2054,  673],\n",
              "       [  17,  257,  891, 2711, 2087]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,  197,  301,  310,  450,  800, 1089,  778,  188,   28,    5],\n",
              "       [   0,    5,    8,   29,   34,   81,  104,   53,    8,    2,    0],\n",
              "       [   0,   16,   34,   56,   84,  175,  218,  137,   42,    5,    1],\n",
              "       [   0,   12,   29,   53,   97,  228,  331,  253,   47,    4,    2],\n",
              "       [   0,    6,   17,   21,   38,  105,  174,  131,   26,    2,    0],\n",
              "       [   0,   67,  101,  138,  218,  563,  921,  750,  176,   13,    2],\n",
              "       [   0,    8,   24,   20,   44,  161,  302,  333,   75,    5,    0],\n",
              "       [   0,   22,   42,   81,  121,  347,  605,  675,  197,   13,    0],\n",
              "       [   0,   17,   34,   78,   88,  322,  718,  864,  246,   17,    2],\n",
              "       [   0,    9,   17,   19,   47,  179,  472,  717,  289,   19,    1],\n",
              "       [   0,   83,  118,  154,  256,  601, 1097, 1187,  396,   28,    3]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pas d'amélioration par rapport à leurs équivalents de la partie précédente."
      ],
      "metadata": {
        "id": "DZZHTPlqGvpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Sélection du meilleur modèle"
      ],
      "metadata": {
        "id": "aMovemjZZSXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A. Meilleur modèle de la partie 2: Prédiction que de la note"
      ],
      "metadata": {
        "id": "POJ-7NrKSZkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I. Meilleur modèle n'utilisant que les commentaire pour la prédiction de la note\n",
        "  \n",
        "Tableau récapiutulatif:  \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>Sous partie</td>\n",
        "    <td>Description</td>\n",
        "    <td>MAE</td>\n",
        "    <td>Accuracy</td>\n",
        "    <td>Cust. Accuracy</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2.B.I</td>\n",
        "    <td>MLP(20,10)</td>\n",
        "    <td>0.976</td>\n",
        "    <td>0.325</td>\n",
        "    <td>0.790</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2.B.II</td>\n",
        "    <td>MLP(20,10)</td>\n",
        "    <td>0.890</td>\n",
        "    <td>0.348</td>\n",
        "    <td>0.823</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2.B.III</td>\n",
        "    <td>LSTM(300,1)</td>\n",
        "    <td>0.862</td>\n",
        "    <td>0.389</td>\n",
        "    <td>0.828</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2.B.IV</td>\n",
        "    <td>LSTM(300)+MLP(20,10)</td>\n",
        "    <td>0.899</td>\n",
        "    <td>0.380</td>\n",
        "    <td>0.809</td>\n",
        "  </tr>\n",
        "</table>\n",
        "  \n",
        "Le meilleur modèle c'est le LSTM de la sous partie 2.B.III."
      ],
      "metadata": {
        "id": "f8W-lSx1TIlg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### II. Meilleur modèle utilisant le titre du produit et le commenaitre pour la prédiction de la note\n",
        "  \n",
        "Tableau récapiutulatif:  \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>Sous partie</td>\n",
        "    <td>Description</td>\n",
        "    <td>MAE</td>\n",
        "    <td>Accuracy</td>\n",
        "    <td>Cust. Accuracy</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2.C.I</td>\n",
        "    <td>MLP(20,10)</td>\n",
        "    <td>1.044</td>\n",
        "    <td>0.317</td>\n",
        "    <td>0.748</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2.C.II</td>\n",
        "    <td>MLP(20,10)</td>\n",
        "    <td>0.944</td>\n",
        "    <td>0.338</td>\n",
        "    <td>0.793</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2.C.III</td>\n",
        "    <td>LSTM(300,1)</td>\n",
        "    <td>0.847</td>\n",
        "    <td>0.399</td>\n",
        "    <td>0.830</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2.C.IV</td>\n",
        "    <td>LSTM(300)+MLP(20,10)</td>\n",
        "    <td>0.904</td>\n",
        "    <td>0.365</td>\n",
        "    <td>0.813</td>\n",
        "  </tr>\n",
        "</table>\n",
        "  \n",
        "Le meilleur modèle c'est le LSTM de la sous partie 2.C.III."
      ],
      "metadata": {
        "id": "PjvNPGYhThXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### III. Meilleur modèle pour la prédiction de la note\n",
        "  \n",
        "Tableau récapiutulatif:  \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>Sous partie</td>\n",
        "    <td>Description</td>\n",
        "    <td>MAE</td>\n",
        "    <td>Accuracy</td>\n",
        "    <td>Cust. Accuracy</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2.B.III</td>\n",
        "    <td>LSTM(300,1)</td>\n",
        "    <td>0.862</td>\n",
        "    <td>0.389</td>\n",
        "    <td>0.828</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2.C.III</td>\n",
        "    <td>LSTM(300,1)</td>\n",
        "    <td>0.847</td>\n",
        "    <td>0.399</td>\n",
        "    <td>0.830</td>\n",
        "  </tr>\n",
        "</table>\n",
        "  \n",
        "Le meilleur modèle c'est le LSTM de la sous partie 2.C.III, mais les deux modèles ont des performances similaires, nous sélectionnons donc le plus simple, soit le modèle de la sous partie 2.B.III qui prend moins de paramètres."
      ],
      "metadata": {
        "id": "fD7S-x2iT6Zh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B. Meilleur modèle de la partie 3: Prédiction que de l'utilité"
      ],
      "metadata": {
        "id": "nkJc2chAShrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I. Meilleur modèle n'utilisant que les commentaire pour la prédiction de l'utilité\n",
        "  \n",
        "Tableau récapiutulatif:  \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>Sous partie</td>\n",
        "    <td>Description</td>\n",
        "    <td>MAE</td>\n",
        "    <td>Accuracy</td>\n",
        "    <td>Cust. Accuracy</td>\n",
        "    <td>Binary Thresholding Score</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3.B.I</td>\n",
        "    <td>MLP(20,10)</td>\n",
        "    <td>2.688</td>\n",
        "    <td>0.106</td>\n",
        "    <td>0.536</td>\n",
        "    <td>0.621</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3.B.II</td>\n",
        "    <td>MLP(20,10)</td>\n",
        "    <td>2.710</td>\n",
        "    <td>0.102</td>\n",
        "    <td>0.536</td>\n",
        "    <td>0.618</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3.B.III</td>\n",
        "    <td>LSTM(300,1)</td>\n",
        "    <td>2.633</td>\n",
        "    <td>0.124</td>\n",
        "    <td>0.355</td>\n",
        "    <td>0.631</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3.B.IV</td>\n",
        "    <td>LSTM(300)+MLP(20,10)</td>\n",
        "    <td>2.703</td>\n",
        "    <td>0.105</td>\n",
        "    <td>0.548</td>\n",
        "    <td>0.617</td>\n",
        "  </tr>\n",
        "</table>\n",
        "  \n",
        "Le meilleur modèle c'est le LSTM+MLP de la sous partie 3.B.IV."
      ],
      "metadata": {
        "id": "R1b7Ol7BTZh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### II. Meilleur modèle utilisant le titre du produit et le commenaitre pour la prédiction de\n",
        "  \n",
        "Tableau récapiutulatif:  \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>Sous partie</td>\n",
        "    <td>Description</td>\n",
        "    <td>MAE</td>\n",
        "    <td>Accuracy</td>\n",
        "    <td>Cust. Accuracy</td>\n",
        "    <td>Binary Thresholding Score</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3.C.I</td>\n",
        "    <td>MLP(20,10)</td>\n",
        "    <td>2.927</td>\n",
        "    <td>0.097</td>\n",
        "    <td>0.503</td>\n",
        "    <td>0.573</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3.C.II</td>\n",
        "    <td>MLP(20,10)</td>\n",
        "    <td>2.808</td>\n",
        "    <td>0.102</td>\n",
        "    <td>0.525</td>\n",
        "    <td>0.605</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3.C.III</td>\n",
        "    <td>LSTM(300,1)</td>\n",
        "    <td>2.620</td>\n",
        "    <td>0.122</td>\n",
        "    <td>0.348</td>\n",
        "    <td>0.629</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3.C.IV</td>\n",
        "    <td>LSTM(300)+MLP(20,10)</td>\n",
        "    <td>2.659</td>\n",
        "    <td>0.106</td>\n",
        "    <td>0.541</td>\n",
        "    <td>0.626</td>\n",
        "  </tr>\n",
        "</table>\n",
        "  \n",
        "Le meilleur modèle c'est le LSTM+MLP de la sous partie 3.C.IV."
      ],
      "metadata": {
        "id": "Ixe5gxV_TzpY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### III. Meilleur modèle pour la prédiction de l'utilité\n",
        "  \n",
        "Tableau récapiutulatif:  \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>Sous partie</td>\n",
        "    <td>Description</td>\n",
        "    <td>MAE</td>\n",
        "    <td>Accuracy</td>\n",
        "    <td>Cust. Accuracy</td>\n",
        "    <td>Binary Thresholding Score</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3.B.IV</td>\n",
        "    <td>LSTM(300)+MLP(20,10)</td>\n",
        "    <td>2.703</td>\n",
        "    <td>0.105</td>\n",
        "    <td>0.548</td>\n",
        "    <td>0.617</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3.C.IV</td>\n",
        "    <td>LSTM(300)+MLP(20,10)</td>\n",
        "    <td>2.659</td>\n",
        "    <td>0.106</td>\n",
        "    <td>0.541</td>\n",
        "    <td>0.626</td>\n",
        "  </tr>\n",
        "</table>\n",
        "  \n",
        "Le meilleur modèle c'est le LSTM+MLP de la sous partie 3.C.IV mais la différence de performance est tellement négligeable que nous sélectionnons le modèle de la sous partie 3.B.IV qui prend moins de paramètres."
      ],
      "metadata": {
        "id": "pBl2fQIiUCPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C. Meilleur modèle de la partie 4: Même modèle pour la prédiction de la note et de l'utilité"
      ],
      "metadata": {
        "id": "halkPyZVSjYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I. Meilleur modèle n'utilisant que les commentaire pour la prédiction de la note et de l'utilité\n",
        "  \n",
        "Tableau récapiutulatif:  \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>Sous partie</td>\n",
        "    <td>Description</td>\n",
        "    <td>MAE (note, utilité)</td>\n",
        "    <td>Accuracy (note, utilité, les deux)</td>\n",
        "    <td>Cust. Accuracy (note, utilité, les deux)</td>\n",
        "    <td>Binary Thresholding Score (utilité)</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>4.B.I</td>\n",
        "    <td>MLP(20,10)</td>\n",
        "    <td>(1.013, 2.981)</td>\n",
        "    <td>(0.283, 0.093, 0.026)</td>\n",
        "    <td>(0.763, 0.463, 0.347)</td>\n",
        "    <td>0.593</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>4.B.II</td>\n",
        "    <td>MLP(20,10)</td>\n",
        "    <td>(0.974, 2.971)</td>\n",
        "    <td>(0.293, 0.086, 0.023)</td>\n",
        "    <td>(0.789, 0.444, 0.343)</td>\n",
        "    <td>0.609</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>4.B.III</td>\n",
        "    <td>LSTM(300,2)</td>\n",
        "    <td>(0.832, 2.898)</td>\n",
        "    <td>(0.439, 0.102, 0.047)</td>\n",
        "    <td>(0.820, 0.500, 0.410)</td>\n",
        "    <td>0.614</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>4.B.IV</td>\n",
        "    <td>LSTM(300,2)+MLP(20,10)</td>\n",
        "    <td>(0.876, 2.986)</td>\n",
        "    <td>(0.362, 0.089, 0.032)</td>\n",
        "    <td>(0.832, 0.487, 0.404)</td>\n",
        "    <td>0.598</td>\n",
        "  </tr>\n",
        "</table>\n",
        "  \n",
        "Le meilleur modèle c'est le LSTM de la sous partie 4.B.III."
      ],
      "metadata": {
        "id": "qBb_rCXTTat6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### II. Meilleur modèle utilisant le titre du produit et le commenaitre pour la prédiction de la note et de l'utilité\n",
        "  \n",
        "Tableau récapiutulatif:  \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>Sous partie</td>\n",
        "    <td>Description</td>\n",
        "    <td>MAE (note, utilité)</td>\n",
        "    <td>Accuracy (note, utilité, les deux)</td>\n",
        "    <td>Cust. Accuracy (note, utilité, les deux)</td>\n",
        "    <td>Binary Thresholding Score (utilité)</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>4.C.I</td>\n",
        "    <td>MLP(20,10)</td>\n",
        "    <td>(1.028, 3.148)</td>\n",
        "    <td>(0.276, 0.089, 0.024)</td>\n",
        "    <td>(0.757, 0.461, 0.346)</td>\n",
        "    <td>0.568</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>4.C.II</td>\n",
        "    <td>MLP(20,10)</td>\n",
        "    <td>(0.983, 3.035)</td>\n",
        "    <td>(0.297, 0.091, 0.026)</td>\n",
        "    <td>(0.778, 0.475, 0.368)</td>\n",
        "    <td>0.592</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>4.C.III</td>\n",
        "    <td>LSTM(300,2)</td>\n",
        "    <td>(0.843, 2.878)</td>\n",
        "    <td>(0.431, 0.105, 0.046)</td>\n",
        "    <td>(0.813, 0.517, 0.424)</td>\n",
        "    <td>0.617</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>4.C.IV</td>\n",
        "    <td>LSTM(300,2)+MLP(20,10)</td>\n",
        "    <td>(0.873, 2.954)</td>\n",
        "    <td>(0.372, 0.093, 0.036)</td>\n",
        "    <td>(0.828, 0.485, 0.401)</td>\n",
        "    <td>0.599</td>\n",
        "  </tr>\n",
        "</table>\n",
        "  \n",
        "Le meilleur modèle c'est le LSTM de la sous partie 4.C.III."
      ],
      "metadata": {
        "id": "dTMgOawaT1ar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### III. Meilleur modèle pour la prédiction de la note et de l'utilité\n",
        "  \n",
        "Tableau récapiutulatif:  \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>Sous partie</td>\n",
        "    <td>Description</td>\n",
        "    <td>MAE (note, utilité)</td>\n",
        "    <td>Accuracy (note, utilité, les deux)</td>\n",
        "    <td>Cust. Accuracy (note, utilité, les deux)</td>\n",
        "    <td>Binary Thresholding Score (note)</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>4.B.III</td>\n",
        "    <td>LSTM(300,2)</td>\n",
        "    <td>(0.832, 2.898)</td>\n",
        "    <td>(0.439, 0.102, 0.047)</td>\n",
        "    <td>(0.820, 0.500, 0.410)</td>\n",
        "    <td>0.614</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>4.C.III</td>\n",
        "    <td>LSTM(300,2)</td>\n",
        "    <td>(0.843, 2.878)</td>\n",
        "    <td>(0.431, 0.105, 0.046)</td>\n",
        "    <td>(0.813, 0.517, 0.424)</td>\n",
        "    <td>0.617</td>\n",
        "  </tr>\n",
        "</table>\n",
        "  \n",
        "Les deux modèles ont des performances similaires, nous optons donc pour la simplicité, nous sélectionnons donc le modèle de la sous partie 4.B.III qui prend moins de paramètres."
      ],
      "metadata": {
        "id": "mVjqW_e2UJUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### D. Modèle unique vs 2 modèles distinct pour la prédiction"
      ],
      "metadata": {
        "id": "cKlif828Umuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I. Proportion de prédiction correctes simultané de l'Accuracy et du Customized Accuracy"
      ],
      "metadata": {
        "id": "sA77RYY3d5xD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Entrainement du modèle qui prédit que la note (modèle de la sous partie 2.B.III)"
      ],
      "metadata": {
        "id": "BY9gUs-bfB5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nombre de lignes à prendre par catégorie\n",
        "nb_line_per_category = df['overall'].value_counts().min()\n",
        "\n",
        "# Affichage\n",
        "nb_line_per_category"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZQzKNU3fICv",
        "outputId": "03cdc540-2e9b-439c-af44-155e6a543e15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28525"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset utilisé pour la prédiction de la note\n",
        "note_df = pd.concat([data_sample_by_note(df, nb_line_per_category, i) for i in df['overall'].unique()])"
      ],
      "metadata": {
        "id": "MGQPK8bczdXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Affectation et transformation\n",
        "Y = note_df['overall']\n",
        "X = note_df[['title', 'reviewText']]"
      ],
      "metadata": {
        "id": "1k4YgYaqgJUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,shuffle=True,random_state=1)"
      ],
      "metadata": {
        "id": "S-9tOSjRf8VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_train_tokens = X_train['reviewText'].apply(lambda line : preprocess_text(line))\n",
        "reviews_test_tokens = X_test['reviewText'].apply(lambda line : preprocess_text(line))"
      ],
      "metadata": {
        "id": "0gLiqO0YgBPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = {word: idx for idx, word in enumerate(model_google_vec_neg_300.index_to_key)}\n",
        "X_train_sequences = [to_sequence(word2idx, x) for x in reviews_train_tokens]\n",
        "X_test_sequences = [to_sequence(word2idx, x) for x in  reviews_test_tokens]"
      ],
      "metadata": {
        "id": "_UMv1QaigO2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import pad_sequences\n",
        "\n",
        "MAX_SEQ_LENGHT=50\n",
        "N_FEATURES = len(model_google_vec_neg_300.index_to_key)\n",
        "X_train_sequences = pad_sequences(X_train_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)\n",
        "X_test_sequences = pad_sequences(X_test_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)"
      ],
      "metadata": {
        "id": "ai0nsHbqgYQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDINGS_LEN = model_google_vec_neg_300.vector_size\n",
        "embeddings_index = np.zeros((len(model_google_vec_neg_300.index_to_key)+1, EMBEDDINGS_LEN))\n",
        "embeddings_index.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YoAC-y_ggm7",
        "outputId": "3a03fafd-2a4b-443a-a97e-2a61f89ae72a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000001, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word, idx in word2idx.items():\n",
        "    try:\n",
        "        embedding = model_google_vec_neg_300[word]\n",
        "        embeddings_index[idx] = embedding\n",
        "    except:\n",
        "        pass\n",
        "      \n",
        "print(\"EMBEDDINGS_LEN=\", EMBEDDINGS_LEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aag4lZOsgleL",
        "outputId": "c6a07241-e8d9-4975-bd6b-e8b1e8870cd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EMBEDDINGS_LEN= 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z54Iwc6g74L",
        "outputId": "3f8c34ae-c59e-470b-b612-d63215fbfc45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000001, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense,LSTM\n",
        "\n",
        "\n",
        "model_lstm_note = Sequential()\n",
        "model_lstm_note.add(Embedding(len(model_google_vec_neg_300.key_to_index)+1,\n",
        "                    EMBEDDINGS_LEN,  # Embedding size\n",
        "                    weights=[embeddings_index],\n",
        "                    trainable=False))\n",
        "\n",
        "model_lstm_note.add(LSTM(300, dropout=0.2))\n",
        "\n",
        "model_lstm_note.add(Dense(1))\n",
        "model_lstm_note.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "model_lstm_note.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6s7ww2kgcgM",
        "outputId": "ceb67a6c-0d98-4ad8-babc-0c4c13f64e47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, None, 300)         900000300 \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 300)               721200    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 301       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 900,721,801\n",
            "Trainable params: 721,501\n",
            "Non-trainable params: 900,000,300\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_lstm_note.fit(X_train_sequences, y_train, epochs=10, batch_size=128, verbose=1,validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzeXyQQbhzub",
        "outputId": "7759dcdd-0296-4c94-adb5-39702dc3f48b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "803/803 [==============================] - 113s 139ms/step - loss: 0.9895 - mean_absolute_error: 0.9895 - val_loss: 0.8840 - val_mean_absolute_error: 0.8840\n",
            "Epoch 2/10\n",
            "803/803 [==============================] - 111s 138ms/step - loss: 0.8848 - mean_absolute_error: 0.8848 - val_loss: 0.8498 - val_mean_absolute_error: 0.8498\n",
            "Epoch 3/10\n",
            "803/803 [==============================] - 111s 138ms/step - loss: 0.8459 - mean_absolute_error: 0.8459 - val_loss: 0.8010 - val_mean_absolute_error: 0.8010\n",
            "Epoch 4/10\n",
            "803/803 [==============================] - 111s 138ms/step - loss: 0.8166 - mean_absolute_error: 0.8166 - val_loss: 0.7960 - val_mean_absolute_error: 0.7960\n",
            "Epoch 5/10\n",
            "803/803 [==============================] - 111s 138ms/step - loss: 0.7944 - mean_absolute_error: 0.7944 - val_loss: 0.7753 - val_mean_absolute_error: 0.7753\n",
            "Epoch 6/10\n",
            "803/803 [==============================] - 111s 138ms/step - loss: 0.7739 - mean_absolute_error: 0.7739 - val_loss: 0.7704 - val_mean_absolute_error: 0.7704\n",
            "Epoch 7/10\n",
            "803/803 [==============================] - 112s 139ms/step - loss: 0.7546 - mean_absolute_error: 0.7546 - val_loss: 0.7651 - val_mean_absolute_error: 0.7651\n",
            "Epoch 8/10\n",
            "803/803 [==============================] - 113s 141ms/step - loss: 0.7345 - mean_absolute_error: 0.7345 - val_loss: 0.7594 - val_mean_absolute_error: 0.7594\n",
            "Epoch 9/10\n",
            "803/803 [==============================] - 112s 140ms/step - loss: 0.7190 - mean_absolute_error: 0.7190 - val_loss: 0.7638 - val_mean_absolute_error: 0.7638\n",
            "Epoch 10/10\n",
            "803/803 [==============================] - 112s 140ms/step - loss: 0.7036 - mean_absolute_error: 0.7036 - val_loss: 0.7545 - val_mean_absolute_error: 0.7545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction=model_lstm_note.predict(X_test_sequences)\n",
        "prediction[prediction<1]=1\n",
        "prediction[prediction>5]=5\n",
        "ACC=accuracy_score(y_test,np.round(prediction))\n",
        "MAE=mean_absolute_error(y_test,prediction)\n",
        "CACC=customized_accuracy(y_test.to_numpy(), np.round(prediction))\n",
        "print('For LSTM MAE = {0:.3f}, Accuracy = {1:.3f}, Customized Accuracy = {2:.3f}'.format(MAE,ACC,CACC))\n",
        "display(confusion_matrix(y_test,np.round(prediction)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "V-6om4RxiKUc",
        "outputId": "bfda216a-1022-447e-fdfb-3d28b0566724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "892/892 [==============================] - 22s 25ms/step\n",
            "For LSTM MAE = 0.772, Accuracy = 0.436, Customized Accuracy = 0.856\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[2406, 2070,  718,  395,   94],\n",
              "       [ 828, 2527, 1478,  785,  134],\n",
              "       [ 233, 1383, 1903, 1754,  312],\n",
              "       [  55,  410, 1244, 2747, 1275],\n",
              "       [  59,  310,  614, 1933, 2858]])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Entrainement du modèle qui prédit que l'utilité (modèle de la sous partie 3.B.IV)"
      ],
      "metadata": {
        "id": "cciLclDJjYgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_line_per_category = 20000"
      ],
      "metadata": {
        "id": "CxJpd6wnirvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset utilisé pour la prédiction de l'utilité\n",
        "helpful_df = pd.concat([data_sample_by_helpful(df, nb_line_per_category, i) for i in df['helpful'].unique()])\n",
        "# Vérification de la taille\n",
        "helpful_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IKjogQKj1lU",
        "outputId": "3848da3e-d085-48e7-d130-3ca7bd6567c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(118375, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Affectation et transformation\n",
        "Y = helpful_df['helpful']\n",
        "X = helpful_df[['title', 'reviewText']]"
      ],
      "metadata": {
        "id": "yuacSbfGj5tE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,shuffle=True,random_state=1)"
      ],
      "metadata": {
        "id": "zYWKSu2nj6n_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_train_tokens = X_train['reviewText'].apply(lambda line : preprocess_text(line))\n",
        "reviews_test_tokens = X_test['reviewText'].apply(lambda line : preprocess_text(line))"
      ],
      "metadata": {
        "id": "RnSNsDxCj9t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = {word: idx for idx, word in enumerate(model_google_vec_neg_300.index_to_key)}\n",
        "X_train_sequences = [to_sequence(word2idx, x) for x in reviews_train_tokens]\n",
        "X_test_sequences = [to_sequence(word2idx, x) for x in  reviews_test_tokens]"
      ],
      "metadata": {
        "id": "oegjkWA_j_Vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import pad_sequences\n",
        "\n",
        "MAX_SEQ_LENGHT=50\n",
        "N_FEATURES = len(model_google_vec_neg_300.index_to_key)\n",
        "X_train_sequences = pad_sequences(X_train_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)\n",
        "X_test_sequences = pad_sequences(X_test_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)\n",
        "print(X_train_sequences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytWQAfqQkFY6",
        "outputId": "59f16c14-6297-44ff-eb11-f7bf800e7f34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000000 3000000 3000000 3000000 3000000 3000000 3000000 3000000 3000000\n",
            " 3000000 3000000 3000000 3000000 3000000 3000000 3000000 3000000 3000000\n",
            " 3000000 3000000 3000000 3000000 3000000 3000000 3000000 3000000 3000000\n",
            " 3000000 3000000 3000000 3000000 3000000 3000000 3000000 3000000 3000000\n",
            " 3000000 3000000 3000000 3000000     150    5027  972514    1021     422\n",
            "     502    1069      15    4929    1269]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDINGS_LEN = model_google_vec_neg_300.vector_size\n",
        "embeddings_index = np.zeros((len(model_google_vec_neg_300.index_to_key)+1, EMBEDDINGS_LEN))\n",
        "embeddings_index.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKFELgtUkJlC",
        "outputId": "0d7c0d2b-f55c-44a7-98f8-d52933a66ddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000001, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word, idx in word2idx.items():\n",
        "    try:\n",
        "        embedding = model_google_vec_neg_300[word]\n",
        "        embeddings_index[idx] = embedding\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "eixS4iRSkNs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense,LSTM\n",
        "\n",
        "\n",
        "model_lstm_util = Sequential()\n",
        "model_lstm_util.add(Embedding(len(model_google_vec_neg_300.key_to_index)+1,\n",
        "                    EMBEDDINGS_LEN,  # Embedding size\n",
        "                    weights=[embeddings_index],\n",
        "                    trainable=False))\n",
        "\n",
        "model_lstm_util.add(LSTM(300, dropout=0.2))\n",
        "\n",
        "model_lstm_util.add(Dense(1))\n",
        "model_lstm_util.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "model_lstm_util.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFc5ykfAkRFW",
        "outputId": "b29e8b18-4488-4528-a92d-b0fa74feb392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, None, 300)         900000300 \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 300)               721200    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 301       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 900,721,801\n",
            "Trainable params: 721,501\n",
            "Non-trainable params: 900,000,300\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_lstm_util.fit(X_train_sequences, y_train, epochs=10, batch_size=128, verbose=1,validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjqirDfKkkPO",
        "outputId": "be8340ca-9eef-4e01-da1e-9de2dc528681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "666/666 [==============================] - 105s 139ms/step - loss: 2.7949 - mean_absolute_error: 2.7949 - val_loss: 2.7017 - val_mean_absolute_error: 2.7017\n",
            "Epoch 2/10\n",
            "666/666 [==============================] - 92s 138ms/step - loss: 2.7035 - mean_absolute_error: 2.7035 - val_loss: 2.7205 - val_mean_absolute_error: 2.7205\n",
            "Epoch 3/10\n",
            "666/666 [==============================] - 93s 140ms/step - loss: 2.6950 - mean_absolute_error: 2.6950 - val_loss: 2.6845 - val_mean_absolute_error: 2.6845\n",
            "Epoch 4/10\n",
            "666/666 [==============================] - 93s 140ms/step - loss: 2.6871 - mean_absolute_error: 2.6871 - val_loss: 2.6840 - val_mean_absolute_error: 2.6840\n",
            "Epoch 5/10\n",
            "666/666 [==============================] - 92s 139ms/step - loss: 2.6803 - mean_absolute_error: 2.6803 - val_loss: 2.6937 - val_mean_absolute_error: 2.6937\n",
            "Epoch 6/10\n",
            "666/666 [==============================] - 93s 139ms/step - loss: 2.6730 - mean_absolute_error: 2.6730 - val_loss: 2.7043 - val_mean_absolute_error: 2.7043\n",
            "Epoch 7/10\n",
            "666/666 [==============================] - 94s 141ms/step - loss: 2.6646 - mean_absolute_error: 2.6646 - val_loss: 2.6921 - val_mean_absolute_error: 2.6921\n",
            "Epoch 8/10\n",
            "666/666 [==============================] - 94s 141ms/step - loss: 2.6556 - mean_absolute_error: 2.6556 - val_loss: 2.6914 - val_mean_absolute_error: 2.6914\n",
            "Epoch 9/10\n",
            "666/666 [==============================] - 94s 142ms/step - loss: 2.6417 - mean_absolute_error: 2.6417 - val_loss: 2.7034 - val_mean_absolute_error: 2.7034\n",
            "Epoch 10/10\n",
            "666/666 [==============================] - 94s 141ms/step - loss: 2.6294 - mean_absolute_error: 2.6294 - val_loss: 2.7017 - val_mean_absolute_error: 2.7017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "Model_rf_util = Model(inputs=model_lstm_util.inputs, outputs=model_lstm_util.layers[1].output)\n",
        "#Model_rf.save('../models/model_lstm_features.h5')\n",
        "corpus_train_lstm = Model_rf_util.predict(X_train_sequences)\n",
        "corpus_test_lstm = Model_rf_util.predict(X_test_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLBPN2ixqDDK",
        "outputId": "65fd8544-18c4-41eb-a3d4-491912a954a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2960/2960 [==============================] - 73s 25ms/step\n",
            "740/740 [==============================] - 18s 24ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_util = MLPRegressor(hidden_layer_sizes=(20,10),max_iter=800,random_state=1,alpha=0.001)"
      ],
      "metadata": {
        "id": "giEitt6Kl0Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_util.fit(corpus_train_lstm,y_train)\n",
        "prediction=mlp_util.predict(corpus_test_lstm)\n",
        "prediction[prediction<1]=1\n",
        "prediction[prediction>10]=10\n",
        "MAE=mean_absolute_error(y_test.to_numpy(),prediction)\n",
        "ACC=accuracy_score(y_test.to_numpy(),np.round(prediction))\n",
        "CACC=customized_accuracy(y_test.to_numpy(),np.round(prediction),2)\n",
        "BTS=binary_tresholding_score(y_test.to_numpy(),np.round(prediction))\n",
        "\n",
        "print('MAE = {0:.3f}, Accuracy = {1:.3f}, Customized Accuracy = {2:.3f}, Binary Thresholding Score = {3:.3f}'.format(MAE,ACC, CACC, BTS))\n",
        "display(confusion_matrix(y_test.to_numpy(),np.round(prediction)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "lhni_JYfmEG9",
        "outputId": "a49e6361-b5d0-4cfb-fab4-68d0511e8d93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE = 2.776, Accuracy = 0.102, Customized Accuracy = 0.513, Binary Thresholding Score = 0.611\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,  169,  117,  298,  711, 1082,  945,  642,   72,   16,   18],\n",
              "       [   0,   12,    9,   15,   61,   89,   79,   52,    6,    1,    2],\n",
              "       [   0,   12,    7,   29,  111,  203,  236,  154,   22,    3,    1],\n",
              "       [   0,   10,   15,   49,  138,  266,  286,  273,   16,    3,    2],\n",
              "       [   0,    3,    6,   20,   56,  146,  191,  166,   13,    0,    0],\n",
              "       [   0,   46,   44,  132,  475,  988, 1138, 1051,  119,   12,    9],\n",
              "       [   0,    7,    4,   18,   82,  200,  303,  350,   26,    2,    2],\n",
              "       [   0,   14,   20,   72,  216,  477,  745,  800,   94,    5,    5],\n",
              "       [   0,   21,   19,   50,  202,  520,  940, 1170,  167,    4,    2],\n",
              "       [   0,    5,    5,   21,   77,  260,  597, 1150,  203,   11,    6],\n",
              "       [   0,   54,   38,  148,  397,  765, 1038, 1272,  219,   14,   11]])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Proportion de prédiction correct simultanés"
      ],
      "metadata": {
        "id": "h1kXeEEck7Mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_line_per_category = 20000"
      ],
      "metadata": {
        "id": "I5W8xdl7omVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset utilisé pour la prédiction de l'utilité\n",
        "ho_df = pd.concat([data_sample_by_helpful_and_balanced_note(df, nb_line_per_category, i) for i in df['helpful'].unique()])"
      ],
      "metadata": {
        "id": "v6NkO5KvpAn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Affectation et transformation\n",
        "Y = ho_df[['overall', 'helpful']]\n",
        "X = ho_df[['title', 'reviewText']]"
      ],
      "metadata": {
        "id": "s80r2vY5pFbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,shuffle=True,random_state=1)"
      ],
      "metadata": {
        "id": "kWs_XwJgpIin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_train_tokens = X_train['reviewText'].apply(lambda line : preprocess_text(line))\n",
        "reviews_test_tokens = X_test['reviewText'].apply(lambda line : preprocess_text(line))"
      ],
      "metadata": {
        "id": "7_CDGleGpOEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = {word: idx for idx, word in enumerate(model_google_vec_neg_300.index_to_key)}\n",
        "X_train_sequences = [to_sequence(word2idx, x) for x in reviews_train_tokens]\n",
        "X_test_sequences = [to_sequence(word2idx, x) for x in  reviews_test_tokens]"
      ],
      "metadata": {
        "id": "nrdpB58frURl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import pad_sequences\n",
        "\n",
        "MAX_SEQ_LENGHT=50\n",
        "N_FEATURES = len(model_google_vec_neg_300.index_to_key)\n",
        "X_test_sequences = pad_sequences(X_test_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)"
      ],
      "metadata": {
        "id": "7alFEpPKrWyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ytest_overall = y_test.iloc[:,0].to_numpy()\n",
        "pred_overall = model_lstm_note.predict(X_test_sequences)\n",
        "pred_overall[pred_overall < 1] = 1\n",
        "pred_overall[pred_overall > 5] = 5\n",
        "\n",
        "ytest_helpful = y_test.iloc[:,1].to_numpy()\n",
        "corpus_test_lstm_util = Model_rf_util.predict(X_test_sequences)\n",
        "pred_helpful = mlp_util.predict(corpus_test_lstm_util)\n",
        "pred_helpful[pred_helpful < 1] = 1\n",
        "pred_helpful[pred_helpful > 10] = 10\n",
        "        \n",
        "OMAE=mean_absolute_error(ytest_overall,pred_overall)\n",
        "OACC=accuracy_score(ytest_overall,np.round(pred_overall))\n",
        "OCACC=customized_accuracy(ytest_overall,np.round(pred_overall),1)\n",
        "\n",
        "HMAE=mean_absolute_error(ytest_helpful,pred_helpful)\n",
        "HACC=accuracy_score(ytest_helpful,np.round(pred_helpful))\n",
        "HCACC=customized_accuracy(ytest_helpful,np.round(pred_helpful),2)\n",
        "HBTS=binary_tresholding_score(ytest_helpful,np.round(pred_helpful))\n",
        "\n",
        "ACC=customized_accuracy_score_double(ytest_overall,np.round(pred_overall),ytest_helpful,np.round(pred_helpful))\n",
        "CACC=customized_accuracy_score_double(ytest_overall,np.round(pred_overall),ytest_helpful,np.round(pred_helpful),1,2)\n",
        "\n",
        "print('Note: MAE = {0:.3f}, Accuracy = {1:.3f}, Customized Accuracy = {2:.3f}'.format(OMAE,OACC,OCACC))\n",
        "print('Utilité: MAE = {0:.3f}, Accuracy = {1:.3f}, Customized Accuracy = {2:.3f}, Binary Thresholding Score = {3:.3f}'.format(HMAE,HACC,HCACC,HBTS))\n",
        "print('Les deux: Accuracy = {0:.3f}, Customized Accuracy = {1:.3f}'.format(ACC,CACC))\n",
        "display(confusion_matrix(ytest_overall,np.round(pred_overall)))\n",
        "display(confusion_matrix(ytest_helpful,np.round(pred_helpful)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "uBnlLZ7opROh",
        "outputId": "7b073245-1037-402f-e7ed-1391d72de2df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "654/654 [==============================] - 16s 25ms/step\n",
            "654/654 [==============================] - 16s 25ms/step\n",
            "Note: MAE = 0.763, Accuracy = 0.451, Customized Accuracy = 0.858\n",
            "Utilité: MAE = 2.860, Accuracy = 0.103, Customized Accuracy = 0.488, Binary Thresholding Score = 0.630\n",
            "Les deux: Accuracy = 0.045, Customized Accuracy = 0.415\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[1989, 1723,  543,  267,   64],\n",
              "       [ 343, 1525,  780,  363,   47],\n",
              "       [ 104,  803, 1197, 1044,  158],\n",
              "       [  45,  298,  857, 2001,  802],\n",
              "       [  81,  337,  664, 2163, 2718]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[   0,  185,   96,  278,  763, 1202,  975,  560,   56,   11,   20],\n",
              "       [   0,   10,   10,   16,   65,  101,   70,   47,    5,    0,    0],\n",
              "       [   0,   17,   19,   41,  133,  206,  190,  151,   10,    0,    1],\n",
              "       [   0,   10,   14,   43,  182,  284,  282,  227,   13,    1,    0],\n",
              "       [   0,    2,    5,   16,   56,  137,  168,  120,   16,    0,    0],\n",
              "       [   0,   10,   18,   83,  353,  818,  899,  694,   67,    6,    1],\n",
              "       [   0,    3,    2,   12,   73,  198,  328,  329,   27,    0,    0],\n",
              "       [   0,    5,    3,   46,  161,  404,  681,  719,   77,    5,    2],\n",
              "       [   0,    4,    3,   22,  123,  375,  748,  964,  137,    6,    4],\n",
              "       [   0,    0,    0,   17,   55,  187,  486,  846,  169,    9,    0],\n",
              "       [   0,   50,   37,  104,  403,  853, 1098, 1186,  172,   11,    9]])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### II. Meilleur modèle\n",
        "  \n",
        "Tableau récapiutulatif:  \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>Sous partie</td>\n",
        "    <td>Description</td>\n",
        "    <td>MAE (note, utilité)</td>\n",
        "    <td>Accuracy (note, utilité, les deux)</td>\n",
        "    <td>Cust. Accuracy (note, utilité, les deux)</td>\n",
        "    <td>Binary Thresholding Score (utilité)</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2.B.III+3.B.IV</td>\n",
        "    <td>LSTM(300,1),LSTM(300,1)+MLP</td>\n",
        "    <td>(0.763, 2.860)</td>\n",
        "    <td>(0.451, 0.103, 0.045)</td>\n",
        "    <td>(0.858, 0.488, 0.415)</td>\n",
        "    <td>0.630</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>4.B.III</td>\n",
        "    <td>LSTM(300,2)</td>\n",
        "    <td>(0.832, 2.898)</td>\n",
        "    <td>(0.439, 0.102, 0.047)</td>\n",
        "    <td>(0.820, 0.500, 0.410)</td>\n",
        "    <td>0.614</td>\n",
        "  </tr>\n",
        "</table>\n",
        "  \n",
        "Il y a très peu de différence entre les performances des deux modèles, nous choisissons donc le modèle de la sous partie 4.B.III (un modèle qui prédit à la fois la note et l'utilité à partir du commentaire)."
      ],
      "metadata": {
        "id": "4PQRxSCYr3Xa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### III. Sauvegarde sur disque du meilleur modèle (modèle de la sous partie 4.B.III)"
      ],
      "metadata": {
        "id": "WYzCCSmhr9fB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_line_per_category = 20000"
      ],
      "metadata": {
        "id": "_B6abwJwz8sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset utilisé pour la prédiction de l'utilité\n",
        "ho_df = pd.concat([data_sample_by_helpful_and_balanced_note(df, nb_line_per_category, i) for i in df['helpful'].unique()])"
      ],
      "metadata": {
        "id": "vyTFRrYh0Xwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Affectation et transformation\n",
        "Y = ho_df[['overall', 'helpful']]\n",
        "X = ho_df[['title', 'reviewText']]"
      ],
      "metadata": {
        "id": "TMgkp4e90ZTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,shuffle=True,random_state=1)"
      ],
      "metadata": {
        "id": "4p2WAEsX0a3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_train_tokens = X_train['reviewText'].apply(lambda line : preprocess_text(line))\n",
        "reviews_test_tokens = X_test['reviewText'].apply(lambda line : preprocess_text(line))"
      ],
      "metadata": {
        "id": "o5AqT_aa0cgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = {word: idx for idx, word in enumerate(model_google_vec_neg_300.index_to_key)}\n",
        "X_train_sequences = [to_sequence(word2idx, x) for x in reviews_train_tokens]"
      ],
      "metadata": {
        "id": "36lgM2j20eXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import pad_sequences\n",
        "\n",
        "MAX_SEQ_LENGHT=50\n",
        "N_FEATURES = len(model_google_vec_neg_300.index_to_key)\n",
        "X_train_sequences = pad_sequences(X_train_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)"
      ],
      "metadata": {
        "id": "rYtrSZ0R04b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDINGS_LEN = model_google_vec_neg_300.vector_size\n",
        "embeddings_index = np.zeros((len(model_google_vec_neg_300.index_to_key)+1, EMBEDDINGS_LEN))\n",
        "embeddings_index.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXGg6DvZ0-jY",
        "outputId": "e8110208-b178-4712-c825-5e2cca4d8e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000001, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word, idx in word2idx.items():\n",
        "    try:\n",
        "        embedding = model_google_vec_neg_300[word]\n",
        "        embeddings_index[idx] = embedding\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "nEaqBEuS1FXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense,LSTM\n",
        "\n",
        "\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(len(model_google_vec_neg_300.key_to_index)+1,\n",
        "                    EMBEDDINGS_LEN,  # Embedding size\n",
        "                    weights=[embeddings_index],\n",
        "                    trainable=False))\n",
        "\n",
        "model_lstm.add(LSTM(300, dropout=0.2))\n",
        "\n",
        "model_lstm.add(Dense(2))\n",
        "model_lstm.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "model_lstm.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkqcboN_1Hkq",
        "outputId": "a8f1cc1d-8b7c-4bb6-b600-f2f33e90efc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, None, 300)         900000300 \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 300)               721200    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 602       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 900,722,102\n",
            "Trainable params: 721,802\n",
            "Non-trainable params: 900,000,300\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_lstm.fit(X_train_sequences, y_train, epochs=10, batch_size=128, verbose=1,validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvumM2501Qmp",
        "outputId": "89ae475c-0ba8-4fc7-8fd5-b9b6a7755a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "589/589 [==============================] - 84s 141ms/step - loss: 2.0386 - mean_absolute_error: 2.0386 - val_loss: 1.9421 - val_mean_absolute_error: 1.9421\n",
            "Epoch 2/10\n",
            "589/589 [==============================] - 81s 138ms/step - loss: 1.9322 - mean_absolute_error: 1.9322 - val_loss: 1.9027 - val_mean_absolute_error: 1.9027\n",
            "Epoch 3/10\n",
            "589/589 [==============================] - 80s 136ms/step - loss: 1.8972 - mean_absolute_error: 1.8972 - val_loss: 1.8847 - val_mean_absolute_error: 1.8847\n",
            "Epoch 4/10\n",
            "589/589 [==============================] - 81s 138ms/step - loss: 1.8866 - mean_absolute_error: 1.8866 - val_loss: 1.9018 - val_mean_absolute_error: 1.9018\n",
            "Epoch 5/10\n",
            "589/589 [==============================] - 82s 140ms/step - loss: 1.8667 - mean_absolute_error: 1.8667 - val_loss: 1.8711 - val_mean_absolute_error: 1.8711\n",
            "Epoch 6/10\n",
            "589/589 [==============================] - 81s 138ms/step - loss: 1.8466 - mean_absolute_error: 1.8466 - val_loss: 1.8701 - val_mean_absolute_error: 1.8701\n",
            "Epoch 7/10\n",
            "589/589 [==============================] - 81s 137ms/step - loss: 1.8290 - mean_absolute_error: 1.8290 - val_loss: 1.8682 - val_mean_absolute_error: 1.8682\n",
            "Epoch 8/10\n",
            "589/589 [==============================] - 81s 138ms/step - loss: 1.8141 - mean_absolute_error: 1.8141 - val_loss: 1.8621 - val_mean_absolute_error: 1.8621\n",
            "Epoch 9/10\n",
            "589/589 [==============================] - 80s 136ms/step - loss: 1.7970 - mean_absolute_error: 1.7970 - val_loss: 1.8630 - val_mean_absolute_error: 1.8630\n",
            "Epoch 10/10\n",
            "589/589 [==============================] - 80s 136ms/step - loss: 1.7717 - mean_absolute_error: 1.7717 - val_loss: 1.8707 - val_mean_absolute_error: 1.8707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model to a file\n",
        "model_lstm.save('model_lstm_review_to_rate_and_utility.h5')"
      ],
      "metadata": {
        "id": "8X50ok_Y1U6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Chargement du meilleur modèle et prédiction"
      ],
      "metadata": {
        "id": "UFd5ibBksCe6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A. Chargement du modèle"
      ],
      "metadata": {
        "id": "frufUdm55pUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "import gensim\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from gensim.parsing.porter import PorterStemmer\n",
        "import gensim.models.keyedvectors as word2vec\n",
        "\n",
        "# Ignorer les warnings\n",
        "import warnings\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "warnings.warn = warn\n",
        "\n",
        "from textblob import TextBlob\n",
        "from random import sample\n",
        "\n",
        "from keras.models import load_model\n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error,accuracy_score,confusion_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQTTRoFPr7lm",
        "outputId": "d56680d5-56d1-4553-e10f-4e515ab6c596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-01-25 00:04:43.757229: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-01-25 00:04:44.854315: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2023-01-25 00:04:44.854369: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2023-01-25 00:04:44.854374: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "porter_stemmer = PorterStemmer()\n",
        "def preprocess_text(sentence):\n",
        "    sentence = remove_stopwords(sentence)\n",
        "    tokenized_sentence = gensim.utils.simple_preprocess(sentence, deacc=True) # Tokenize the sentence and delete accent\n",
        "    tokenized_sentence = [porter_stemmer.stem(word) for word in tokenized_sentence] #stemmatization (garde seulement la racine des mots)\n",
        "    # tokenized_sentence = tokenized_sentence[:300]\n",
        "    return tokenized_sentence"
      ],
      "metadata": {
        "id": "EK9YecB_6p7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_google_vec_neg_300 = word2vec.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "metadata": {
        "id": "7I_ojtNX4Bqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_sequence(index, text):\n",
        "    indexes = [index[word] for word in text if word in index]\n",
        "    return indexes"
      ],
      "metadata": {
        "id": "JFTUX4aH4DIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = {word: idx for idx, word in enumerate(model_google_vec_neg_300.index_to_key)}\n",
        "MAX_SEQ_LENGHT=50\n",
        "N_FEATURES = len(model_google_vec_neg_300.index_to_key)"
      ],
      "metadata": {
        "id": "QEMbmhPK5R_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model from a file\n",
        "model = load_model('model_lstm_review_to_rate_and_utility.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQytiPg55DtS",
        "outputId": "f3ad144a-8ea5-402f-d5c9-61c3ea11269f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-01-25 00:05:04.769250: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-25 00:05:04.922575: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
            "2023-01-25 00:05:04.922599: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2023-01-25 00:05:04.922909: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-01-25 00:05:04.975296: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 3600001200 exceeds 10% of free system memory.\n",
            "2023-01-25 00:05:05.345965: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 3600001200 exceeds 10% of free system memory.\n",
            "2023-01-25 00:05:05.710293: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 3600001200 exceeds 10% of free system memory.\n",
            "2023-01-25 00:05:11.252336: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 3600001200 exceeds 10% of free system memory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction qui charge le modèle et prédit la note et l'utilité d'une liste de commentaire\n",
        "def predict_rate_and_utility_from_review(reviews):\n",
        "  reviews_tokens = [preprocess_text(line) for line in reviews]\n",
        "  X_sequences = [to_sequence(word2idx, x) for x in reviews_tokens]\n",
        "  X_sequences = pad_sequences(X_sequences, maxlen=50, value=N_FEATURES)\n",
        "  return model.predict(X_sequences)"
      ],
      "metadata": {
        "id": "ZqWK0Fa339Dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B. Performances sur le dataset de test"
      ],
      "metadata": {
        "id": "BeuI-ZFm5htc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chargement des données\n",
        "df = pd.read_csv('./dataset/test.csv',sep=';')\n",
        "\n",
        "# Suppression des na\n",
        "df = df.dropna()\n",
        "\n",
        "# Apercu du dataset\n",
        "df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "9k9ild255g3V",
        "outputId": "e02fb214-563a-4cba-8a12-7c2328d859ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title        asin helpful  \\\n",
              "0                Acorn Women's Snowline Mule Slipper  B0076UAIMW  [0, 0]   \n",
              "1  Barilla Marinara Penne Italian Entree, 9 Ounce...  B0076YVPLG  [3, 4]   \n",
              "2  Samsung Galaxy Note Flip Cover Case - White (C...  B0076Z3EA0  [0, 0]   \n",
              "\n",
              "   overall                                         reviewText   reviewTime  \\\n",
              "0      3.0  I got these slippers because I was tired of ha...  06 27, 2014   \n",
              "1      3.0  I tried both this variety and the tomato and b...   11 1, 2012   \n",
              "2      5.0  While buying the flip case I was a bit skeptic...  05 17, 2013   \n",
              "\n",
              "       reviewerID           reviewerName                 summary  \\\n",
              "0   ARKGUYYA6SZ1D          SupermansLois  They're great at first   \n",
              "1  A3R9H6OKZHHRJD                  LH422          A bit of spice   \n",
              "2   AEJYH29XQBQUQ  Amazon Customer \"Pri\"           No complaints   \n",
              "\n",
              "   unixReviewTime  \n",
              "0      1403827200  \n",
              "1      1351728000  \n",
              "2      1368748800  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>asin</th>\n",
              "      <th>helpful</th>\n",
              "      <th>overall</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>reviewTime</th>\n",
              "      <th>reviewerID</th>\n",
              "      <th>reviewerName</th>\n",
              "      <th>summary</th>\n",
              "      <th>unixReviewTime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Acorn Women's Snowline Mule Slipper</td>\n",
              "      <td>B0076UAIMW</td>\n",
              "      <td>[0, 0]</td>\n",
              "      <td>3.0</td>\n",
              "      <td>I got these slippers because I was tired of ha...</td>\n",
              "      <td>06 27, 2014</td>\n",
              "      <td>ARKGUYYA6SZ1D</td>\n",
              "      <td>SupermansLois</td>\n",
              "      <td>They're great at first</td>\n",
              "      <td>1403827200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Barilla Marinara Penne Italian Entree, 9 Ounce...</td>\n",
              "      <td>B0076YVPLG</td>\n",
              "      <td>[3, 4]</td>\n",
              "      <td>3.0</td>\n",
              "      <td>I tried both this variety and the tomato and b...</td>\n",
              "      <td>11 1, 2012</td>\n",
              "      <td>A3R9H6OKZHHRJD</td>\n",
              "      <td>LH422</td>\n",
              "      <td>A bit of spice</td>\n",
              "      <td>1351728000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Samsung Galaxy Note Flip Cover Case - White (C...</td>\n",
              "      <td>B0076Z3EA0</td>\n",
              "      <td>[0, 0]</td>\n",
              "      <td>5.0</td>\n",
              "      <td>While buying the flip case I was a bit skeptic...</td>\n",
              "      <td>05 17, 2013</td>\n",
              "      <td>AEJYH29XQBQUQ</td>\n",
              "      <td>Amazon Customer \"Pri\"</td>\n",
              "      <td>No complaints</td>\n",
              "      <td>1368748800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction qui convertie l'utilité en nombre\n",
        "def parse_helpful(s):\n",
        "  tmp = s.split(\",\")\n",
        "  tmp = [int(i) for i in tmp]\n",
        "  return 0 if tmp[1] == 0 else round(tmp[0] / tmp[1], 1)"
      ],
      "metadata": {
        "id": "XATgSLM7Agc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['helpful'] = df['helpful'].str.replace(' ', '', regex=False).str.replace('[', '', regex=False).str.replace(']', '', regex=False)\n",
        "df['helpful'] = df['helpful'].apply(lambda line : parse_helpful(line))\n",
        "# Ramener à 1 les valeurs qui dépassent 1 et multiplier par 10\n",
        "df['helpful'] = df['helpful'].apply(lambda n : int((n if n <= 1 else 1) * 10))"
      ],
      "metadata": {
        "id": "uXFgoIQbAcEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = df['reviewText']\n",
        "Y = df[['overall', 'helpful']]"
      ],
      "metadata": {
        "id": "imHfCiJ_-IgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import mean\n",
        "\n",
        "# Notre accuracy personaliser accepte les distances de 1 entre les y\n",
        "def customized_accuracy(y_true, y_pred, d = 1):\n",
        "  return mean([(1 if abs(y_true[i] - y_pred[i]) <= d else 0) for i in range(len(y_true))])\n",
        "\n",
        "# Calcul de score basé sur le seuillage binaire de l'utilité\n",
        "def binary_tresholding_score(y_true, y_pred, t = 6):\n",
        "  return mean([(1 if (y_true[i] < t and y_pred[i] < t) or (y_true[i] >= t and y_pred[i] >= t) else 0) for i in range(len(y_true))])\n",
        "\n",
        "def customized_accuracy_score_double(y1_true, y1_pred, y2_true, y2_pred, d1=0, d2=0):\n",
        "  return mean([(1 if abs(y1_true[i] - y1_pred[i]) <= d1 and abs(y2_true[i] - y2_pred[i]) <= d2 else 0) for i in range(len(y1_true))])"
      ],
      "metadata": {
        "id": "S-NaUc8J__ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction=predict_rate_and_utility_from_review(reviews)\n",
        "\n",
        "ytest_overall = Y.iloc[:,0].to_numpy()\n",
        "pred_overall = prediction[:,0]\n",
        "pred_overall[pred_overall < 1] = 1\n",
        "pred_overall[pred_overall > 5] = 5\n",
        "        \n",
        "ytest_helpful = Y.iloc[:,1].to_numpy()\n",
        "pred_helpful = prediction[:,1]\n",
        "pred_helpful[pred_helpful < 1] = 1\n",
        "pred_helpful[pred_helpful > 10] = 10\n",
        "        \n",
        "OMAE=mean_absolute_error(ytest_overall,pred_overall)\n",
        "OACC=accuracy_score(ytest_overall,np.round(pred_overall))\n",
        "OCACC=customized_accuracy(ytest_overall,np.round(pred_overall),1)\n",
        "\n",
        "HMAE=mean_absolute_error(ytest_helpful,pred_helpful)\n",
        "HACC=accuracy_score(ytest_helpful,np.round(pred_helpful))\n",
        "HCACC=customized_accuracy(ytest_helpful,np.round(pred_helpful),2)\n",
        "HBTS=binary_tresholding_score(ytest_helpful,np.round(pred_helpful))\n",
        "\n",
        "ACC=customized_accuracy_score_double(ytest_overall,np.round(pred_overall),ytest_helpful,np.round(pred_helpful))\n",
        "CACC=customized_accuracy_score_double(ytest_overall,np.round(pred_overall),ytest_helpful,np.round(pred_helpful),1,2)\n",
        "\n",
        "print('Note: MAE = {0:.3f}, Accuracy = {1:.3f}, Customized Accuracy = {2:.3f}'.format(OMAE,OACC,OCACC))\n",
        "print('Utilité: MAE = {0:.3f}, Accuracy = {1:.3f}, Customized Accuracy = {2:.3f}, Binary Thresholding Score = {3:.3f}'.format(HMAE,HACC,HCACC,HBTS))\n",
        "print('Les deux: Accuracy = {0:.3f}, Customized Accuracy = {1:.3f}'.format(ACC,CACC))\n",
        "display(confusion_matrix(ytest_overall,np.round(pred_overall)))\n",
        "display(confusion_matrix(ytest_helpful,np.round(pred_helpful)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "9HvjdxfW-fv9",
        "outputId": "46d132a4-2033-4dea-f094-00057c82d678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "123/123 [==============================] - 3s 23ms/step\n",
            "Note: MAE = 0.760, Accuracy = 0.498, Customized Accuracy = 0.839\n",
            "Utilité: MAE = 4.343, Accuracy = 0.019, Customized Accuracy = 0.217, Binary Thresholding Score = 0.636\n",
            "Les deux: Accuracy = 0.009, Customized Accuracy = 0.169\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[ 205,  113,   44,   18,    7],\n",
              "       [  57,   80,   58,   31,    4],\n",
              "       [  31,   70,  119,  110,   21],\n",
              "       [  12,   58,  143,  322,  177],\n",
              "       [  41,  123,  241,  606, 1220]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  0, 269, 203, 262, 556, 609, 437, 278, 188,  29,   0],\n",
              "       [  0,   0,   1,   1,   0,   3,   1,   3,   0,   0,   0],\n",
              "       [  0,   1,   1,   0,   1,   3,   3,   3,   0,   0,   0],\n",
              "       [  0,   2,   2,   4,   3,   5,   5,   2,   1,   0,   0],\n",
              "       [  0,   0,   0,   0,   0,   2,   3,   0,   1,   0,   0],\n",
              "       [  0,   6,   2,   9,  12,  34,  23,  17,  11,   3,   0],\n",
              "       [  0,   0,   1,   1,   2,   3,   5,   4,   2,   0,   0],\n",
              "       [  0,   2,   0,   1,   5,   4,  13,  17,  10,   1,   0],\n",
              "       [  0,   0,   1,   3,   5,  12,   8,  19,  14,   3,   0],\n",
              "       [  0,   1,   1,   1,   2,   6,   5,   6,   9,   1,   0],\n",
              "       [  0,  31,  26,  51,  93, 165, 123, 122, 110,  23,   0]])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LbyWjBND-13z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}